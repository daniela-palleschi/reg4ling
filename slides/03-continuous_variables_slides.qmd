---
title: "Continuous variables"
subtitle: "WiSe23/24"
author: "Daniela Palleschi"
institute: Humboldt-Universit√§t zu Berlin
lang: en
date: 2023-10-10
format: 
  pdf:
    output-file: 03-continuous_variables_slides.pdf
    toc: true
    number-sections: false
    colorlinks: true
    code-overflow: wrap
  revealjs:
    code-overflow: wrap
    theme: [dark]
    width: 1600
    height: 900
    progress: true
    scrollable: true
    # smaller: true
    slide-number: c/t
    code-link: true
    # logo: logos/hu_logo.png
    # css: logo.css
    incremental: true
    # number-sections: true
    toc: false
    toc-depth: 2
    toc-title: 'Overview'
    navigation-mode: linear
    controls-layout: bottom-right
    fig-cap-location: top
    font-size: 0.6em
    slide-level: 4
    self-contained: true
    title-slide-attributes: 
      data-background-image: logos/logos.tif
      data-background-size: 15%
      data-background-position: 50% 92%
    fig-align: center
    output-location: fragment
editor_options: 
  chunk_output_type: console
bibliography: ../references.bib
csl: ../apa.csl
---

```{r setup, eval = T, echo = F}
knitr::opts_chunk$set(echo = T, # print chunks?
                      eval = T, # run chunks?
                      error = F, # print errors?
                      warning = F, # print warnings?
                      message = F, # print messages?
                      cache = F # cache?; be careful with this!
                      )
```


This lecture is based on Ch. 5 (Correlation, Linear, and Nonlinear transformations) from @winter_statistics_2019.

# Learning Objectives {.unnumbered .unlisted visibility="uncounted"}

Today we will learn...

- why and how to centre continuous predictors
- when and how to standardize continuous predictors
- why and how to log-transform continuous variables

# Set-up environment  {.unnumbered .unnumbered visibility="uncounted"}

```{r}
# suppress scientific notation
options(scipen=999)
```

```{r}
# load libraries
pacman::p_load(
               tidyverse,
               here,
               broom,
               lme4,
               janitor,
               languageR)
```

```{r}
#| echo: false

# extra packages for the lecture notes/slides
pacman::p_load(
               patchwork,
               knitr,
               kableExtra,
               gt,
               googlesheets4)

# tell googlesheets4 we don't want private
gs4_deauth()
```

## Load data {.unnumbered .unlisted visibility="uncounted"}

```{r}
df_freq <- read_csv(here("data", "ELP_frequency.csv")) |> 
  clean_names()
```

- Reminder of our variables:

```{r}
#| output-location: fragment
summary(df_freq)
```

# Linear transformations

- refer to constant changes across values that do not alter the relationship between these values
  + adding, subtracting, or multiplying by a constant value
- let's look at some common ways of linearly transforming our data, and the reasons behind doing so

## Centering

- Centering is typically applied to predictor variables
  - subtracting the mean of a variable from each value
  - results in each centered value representing the original value's deviance from the mean (i.e., a mean-deviation score)
- What would a centered value of $0$ represent in terms of the original values?

---

- let's centre our frequency values using the tidyverse

```{r}
# add centered variable with the tidyverse
df_freq <- 
  df_freq |> 
  mutate(freq_c = freq-mean(freq))
```

- and with base R (more verbose...)

```{r}
# add centered variable with base R
df_freq$freq_c <- df_freq$freq-mean(df_freq$freq)
```

- both code chunks produce the same result

```{r}
#| output-location: fragment
head(df_freq)
```


## Centred predictor

- re-fit our model with and without centred predictor

```{r}
# run our model with the original predictor
fit_rt_freq <- 
  lm(rt ~ freq, data = df_freq)
```

```{r}
# run our model with the centered predictor
fit_rt_freq_c <- 
  lm(rt ~ freq_c, data = df_freq)
```

---

- what is the difference between the two models?

```{r}
tidy(fit_rt_freq)
tidy(fit_rt_freq_c)
```

-  the intercept values: `r coef(fit_rt_freq)['(Intercept)']` (uncentered) and `r coef(fit_rt_freq_c)['(Intercept)']` (centered)
- what does this correspond to?

---

```{r}
#| output-location: fragment
mean(df_freq$rt)
```

- intercept with a single centered continuous predictor variable = the mean of a continuous response variable
- this is crucial in interpreting interaction effects, which we will discuss briefly tomorrow (for more: Chapter 8 in @winter_statistics_2019)

## Standardizing (*z*-scoring)

- standardize continuous predictors
  + dividing centered values by the standard deviation of the sample
  + when to do this? if you have multiple continuous predictors (which we don't at present)

---

- what are our mean and standard deviation?

```{r}
#| output-location: column-fragment
mean(df_freq$freq)
```

```{r}
#| output-location: column-fragment
sd(df_freq$freq)
```

- what are the first six values of `freq` in the original scale and centred scale?

```{r}
#| output-location: column-fragment
df_freq$freq[1:6]
```

```{r}
#| output-location: column-fragment
df_freq$freq_c[1:6]
```

---

- standardise z-scores for frequency by dividing these centered values by the standard deviation of `freq`
  + Again, this can be done with `mutate()` from `dplyr`, or by using base R syntax.

```{r}
# standardise using the tidyverse
df_freq <- 
  df_freq |> 
  mutate(freq_z = freq_c/sd(freq))
```


```{r}
# standardize with base R
df_freq$freq_z <- df_freq$freq_c/sd(df_freq$freq)
```

```{r}
#| output-location: fragment
df_freq |> 
  select(freq, freq_c, freq_z) |> 
  head()
```

# Non-linear transformations

- the meat and potates of dealing with continuous variables (depending on your subfield)
- in linguistic research, and especially experimental research, we often deal with continuous variables truncated/bound at 0
  + Reaction times, reading times and formant frequencies: there is (typically) no such thing as a negative reading time or fundamental frequency
- these types of data are almost never normally distributed, typically having a 'positive skew' (long tail to the right)
  + this has implications for the normality of residuals fit to a straight line
  + these very large, exceptional values will have a stronger influence on the line of best fit, leading to the coefficient estimates that are "suboptimal for the majority of data points" [@@baayen_analyzing_2008; p. 92]
  
- How do we deal with this nonnormality?
  + We use non-linear transformations, the most common of which is the log-transformation

## Log-transformation

- luckily, we can easily log-transform continuous values by passing them through the `log()` function
  + this uses the *natural* logarithm, which finds the power to which Euler's number ($e$ = `2.718281828459`) is raised to equal $x$ (don't worry about the math)
- importantly, log-transforming a continuous variable makes numbers smaller, with a larger shrinkage for larger numbers

---

- let's see how log-transformation changes values

```{r}
#| output-location: column-fragment
raw_values <- 
  tibble(
    row = 1:4,
    raw = c(50, 250, 700, 5000),
    log = log(raw))

raw_values
```


```{r}
#| code-fold: true
#| output-location: fragment
#| out-width: 100%
#| fig-asp: .35
fig_raw <-
  raw_values |> 
  ggplot() +
  aes(x = row, y = raw) +
  labs(title = "Raw values") +
  geom_point() +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)


fig_log <-
raw_values |> 
  ggplot() +
  aes(x = row, y = log) +
  labs(title = "Log values") +
  geom_point() +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)


fig_log_raw <-
  raw_values |> 
  ggplot() +
  aes(x = log, y = raw) +
  labs(title = "Raw by log values") +
  geom_point() 

library(patchwork)

fig_raw + fig_log + fig_log_raw + plot_annotation(tag_levels = "A")
```

## Log word frequency

```{r}
df_freq$freq
```


```{r}
#| echo: false
#| output-location: fragment
fig_raw_freq <-
  df_freq |> 
  arrange(freq) |> 
  mutate(row = 1:12) |> 
  ggplot() +
  aes(x = row, y = freq) +
  labs(title = "Raw frequencies") +
  geom_text(aes(label = word)) +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)

fig_log_freq <-
df_freq |> 
  arrange(freq) |> 
  mutate(row = 1:12) |> 
  ggplot() +
  aes(x = row, y = log(freq)) +
  labs(title = "Log frequencies") +
  geom_text(aes(label = word), position = position_jitter(0.5)) +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)

fig_raw_freq + fig_log_freq + plot_annotation(tag_levels = "A")
```

## Log reaction times

```{r}
#| output-location: column-fragment
df_freq |> 
  mutate(log_rt = log(rt)) |> 
  arrange(rt) |> 
  select(rt, log_rt) 
```


```{r}
#| echo: false
#| output-location: fragment
fig_raw_rt <-
  df_freq |> 
  arrange(rt) |> 
  mutate(row = 1:12) |> 
  ggplot() +
  aes(x = rt, y = row) +
  labs(title = "Raw reaction times") +
  geom_point() +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)

fig_log_rt <-
df_freq |> 
  arrange(rt) |> 
  mutate(row = 1:12) |> 
  ggplot() +
  aes(x = log(rt), y = row) +
  labs(title = "Log reaction times") +
  geom_point() +
  geom_line(colour = "grey") +
  geom_smooth(method = "lm", se = F)

fig_raw_rt + fig_log_rt + plot_annotation(tag_levels = "A")
```

### Model with log-transformed variables

```{r}
df_freq <-
  df_freq |> 
    mutate(rt_log = log(rt),
           freq_log = log(freq),
           freq_log_c = freq_log - mean(freq_log))
```

```{r}
fit_log <- lm(rt_log ~ freq_log_c, data = df_freq)
```

```{r}
# or, log-transform directly in the model syntax
fit_log <- lm(log(rt) ~ freq_log_c, data = df_freq)
```

```{r}
tidy(fit_rt_freq_c)
```

```{r}
tidy(fit_log)
```

- what has changed?

## Extracting predictions

- the inverse of the log is the exponential

```{r}
#| output-location: column-fragment
log(2)
exp(0.6931472)
```

- we can plus our equation of a line into the `exp()` function to extract predictions
  - what's our predicted reaction time for the word *door*?

$$
\begin{align}
y_i & = b_0 + b_1x_i
y_i & = b_0 + b_1*freq(door)
\end{align}
$$

```{r}
b0 <- coef(fit_log)['(Intercept)']
b1 <- coef(fit_log)['freq_log_c']
freq_door <-
  df_freq |> filter(word == "door") |> select(freq_log_c)
```

```{r}
b0 + b1*freq_door
```

```{r}
exp(b0 + b1*freq_door)
```


```{r}
predict(fit_log)
```

## `augment()`

- remember the `augment()` function appends model output to the data frame

```{r}
#| output-location: column-fragment
df_freq <-
augment(fit_log, data = df_freq) |> 
  arrange(freq) |>  
  mutate(exp_fit = exp(.fitted)) |> 
  relocate(exp_fit, .after = rt)
df_freq |> 
  select(word, rt_log, .fitted, rt, exp_fit) |> 
  head()
```

---

```{r}
#| code-fold: true
fig_fit_raw <-
  df_freq |> 
  ggplot() +
  aes(x = rt, y = exp_fit, label = word) +
  geom_text() +
  geom_smooth(method = "lm", se = F)


fig_fit_log <-
df_freq |> 
  ggplot() +
  aes(x = rt_log, y = .fitted, label = word) +
  geom_text() +
  geom_smooth(method = "lm", se = F)

fig_fit_freq <-
df_freq |> 
  ggplot() +
  aes(x = freq_log_c, y = .fitted, label = word) +
  labs(title = "log(rt) ~ word frequency") +
  geom_text() +
  geom_smooth(method = "lm", se = F)

fig_fit_raw + fig_fit_log + fig_fit_freq
```



## Log for positive values

- notice that we logged `freq` *before* centering it
  + this is because centering makes half the values negative (because half are below the mean)
  + it's not possible to log-transform zero or negative numbers (because Eulen's $e$ cannot be risen to the power of 0 or a negative number!)
  + so, always log before centering predictors!


# Reporting transformations

- data transformations are typically reported in the data analysis section

> Reaction times and word frequencies had a non-normal distribution with a positive skew. Both variables were log-transformed to achieve normality. Log word frequencies were then standardized by subtracting the variable's mean from each value, and dividing by the standard deviation of the variable's standard deviation. A linear regression model was fit to log-transformed reaction times with standardized log-transformed frequency values as fixed effect.

- it's always good practice to look at papers in a relevant field to get an idea of what to report, especially those that you think were well written and whose methodology you trust

# Learning Objectives {.unnumbered .unlisted}

Today we learned...

- why and how to centre continuous predictors
- when and how to standardize continuous predictors
- why and how to log-transform continuous variables

# Take-home messages {.unnumbered .smaller}

- linear transformations are cosmetic changes
  + do not alter the relationship between variables or values
  + changes the representation of values
  + centring should be performed on continuous predictors (and interval response variables, like ratings scales; but subtract median possible response from observed responses)
  + standardizing should be performed when there are multiple continuous predictors

- non-linear transformations attempt to normalise skewed variables
  + compress the data, squeezing larger/more extreme values to the rest of the data
  + reduces the spread in the distribution

# Task

## Assessing assumptions

1. Re-run the models `fit_rt_freq`, `fit_rt_freq_c`, and `fit_log`
2. Produce diagnostic plots for each of them (histograms, Q-Q plots, residual plots)
3. Interpret the plots

```{r}
#| echo: false
#| eval: false

df_freq_raw <- augment(fit_rt_freq, df_freq)

fig_hist_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .resid) +
  labs(title = "fit_rt_freq") +
  geom_histogram(bins = 8, fill = "grey", colour = "black") +
  theme_bw()
  

fig_qq_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(sample = .resid) +
  labs(title = "fit_rt_freq") +
  geom_qq(colour = "red") +
  geom_qq_line() 

  
fig_res_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .fitted, y = .resid) +
  labs(title = "fit_rt_freq") +
  geom_point() +
  geom_smooth(method = "lm", se = F)

fig_hist_raw + fig_qq_raw + fig_res_raw + plot_annotation(tag_levels = "A")
```

```{r}
#| echo: false
#| eval: false

df_freq_raw <- augment(fit_rt_freq_c, df_freq)

fig_hist_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .resid) +
  labs(title = "fit_rt_freq_c") +
  geom_histogram(bins = 8, fill = "grey", colour = "black") +
  theme_bw()
  

fig_qq_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(sample = .resid) +
  labs(title = "fit_rt_freq_c") +
  geom_qq(colour = "red") +
  geom_qq_line() 

  
fig_res_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .fitted, y = .resid) +
  labs(title = "fit_rt_freq_c") +
  geom_point() +
  geom_smooth(method = "lm", se = F)

fig_hist_raw + fig_qq_raw + fig_res_raw + plot_annotation(tag_levels = "A")
```

```{r}
#| echo: false
#| eval: false

df_freq_raw <- augment(fit_log, df_freq)

fig_hist_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .resid) +
  labs(title = "fit_log") +
  geom_histogram(bins = 8, fill = "grey", colour = "black") +
  theme_bw()
  

fig_qq_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(sample = .resid) +
  labs(title = "fit_log") +
  geom_qq(colour = "red") +
  geom_qq_line() 

  
fig_res_raw <-
  df_freq_raw |> 
  ggplot() +
  aes(x = .fitted, y = .resid) +
  labs(title = "fit_log") +
  geom_point() +
  geom_smooth(method = "lm", se = F)

fig_hist_raw + fig_qq_raw + fig_res_raw + plot_annotation(tag_levels = "A")
```


## Model comparison

1. Use the `glance()` function to inspect the $R^2$, AIC, and BIC of each model.
2. Which is the best fit? Why?

```{r}
#| echo: false
#| eval: false

rbind(
  glance(fit_rt_freq),
  glance(fit_rt_freq_c),
  glance(fit_log)
  )

# log is better fit, but we can't compare models with different non-linear transformations
```


# Literaturverzeichnis {.unlisted .unnumbered visibility="uncounted"}

::: {#refs custom-style="Bibliography"}
:::


