---
title: "Logistic regression"
subtitle: "WiSe23/24"
author: "Daniela Palleschi"
institute: Humboldt-Universit√§t zu Berlin
lang: en
date: 2023-10-11
format: 
  pdf:
    output-file: 06-logistic_regression.pdf
    toc: true
    number-sections: false
    colorlinks: true
    code-overflow: wrap
  revealjs:
    include-in-header: ../mathjax.html # for multiple equation hyperrefs
    code-overflow: wrap
    theme: [dark]
    width: 1600
    height: 900
    progress: true
    scrollable: true
    # smaller: true
    slide-number: c/t
    code-link: true
    # logo: logos/hu_logo.png
    # css: logo.css
    incremental: true
    # number-sections: true
    toc: false
    toc-depth: 2
    toc-title: 'Overview'
    navigation-mode: linear
    controls-layout: bottom-right
    fig-cap-location: top
    font-size: 0.6em
    slide-level: 4
    self-contained: true
    title-slide-attributes: 
      data-background-image: logos/logos.tif
      data-background-size: 15%
      data-background-position: 50% 92%
    fig-align: center
    output-location: fragment
editor_options: 
  chunk_output_type: console
bibliography: ../references.bib
csl: ../apa.csl
---

```{r setup, eval = T, echo = F}
knitr::opts_chunk$set(echo = T, # print chunks?
                      eval = T, # run chunks?
                      error = F, # print errors?
                      warning = F, # print warnings?
                      message = F, # print messages?
                      cache = F # cache?; be careful with this!
                      )
```

# Learning Objectives {.unnumbered .unlisted}

Today we will learn...

- how to model binomial data with logistic regression
- how to interpret log-odds and odds ratio
- how to report a logistic regression model

# Resources {.unnumbered .unlisted}

- this lecture covers chapter 12 \'Genearlised Lienar Models 1: Logistic Regression' [@winter_statistics_2019]
  + we're skipping a few chapters, which I encourage you to go through on your own
  + they cover topics that you presumably have covered in previous courses (namely significance testing, *t*-values and *p*-values).

# Set-up environment  {.unnumbered}

```{r}
# suppress scientific notation
options(scipen=999)
options(pillar.sigfig = 5)
```

```{r}
#| echo: false
library(broman)
# function to format p-values
format_pval <- function(x){
  if (x < .001) return(paste('<', '.001'))
  if (x < .01) return(paste('<', '.01'))
  if (x < .05) return(paste('<', '.05'))
  paste('=', myround(x, 3))  # if above .05, print p-value to 3 decimalp points
}
```

```{r}
# load libraries
pacman::p_load(
               tidyverse,
               here,
               broom,
               janitor,
               languageR)
```

```{r}
#| echo: false

# extra packages for the lecture notes/slides
pacman::p_load(
               patchwork,
               knitr,
               kableExtra)
```

```{r}
# set preferred ggplot2 theme
theme_set(theme_bw() + theme(plot.title = element_text(size = 10)))
```

# Generalised linear models

- linear regression assumes a normal distribution
  + Equation \ref{eq-normal}, where $\mu$ and $\sigma$ correspond to the mean and standard deviation 
- logistic regression assumes a binomial distribution (a.k.a., Bernoulli distribution)
  + Equation \ref{eq-binomial}, where $N$ and $p$ correspond to the number of trials and the probability of $y$ being $1$ or $0$

\begin{align}
y &\sim Normal(\mu,\sigma) \label{eq-normal} \\
y &\sim binomial(N = 1, p) \label{eq-binomial}
\end{align}

- logistic regression is a type of genearlised linear model (GLM)
  + used to model binomial response data

## Log-odds, odds ratio, and probabilities

- logistic regression describes the *probability* ($p$) of observing one outcome or another as a function of a predictor variable
  + e.g., the absence or presence of some phenomenon (word order, schwa, etc.) or button responses (yes/no, accept/reject)
- this can be described as the probability, odds, or log-odds of a particular outcome over another


```{r}
#| echo: false
df_odds <-
  tibble(
  log_odds = seq(from = -5, to = 5, by = 0.25),
  odds = exp(log_odds),
  prob = plogis(log_odds),
  q = 1 - prob
)
```

### Probability

- probability ranges from 0 (no chance) to 1 (certain)
  + 50% chance = probability of 0.5
  
### Odds (ratio)

- odds range from 0 to infinity
  + *the odds that I'll win are 2:1* ($\frac{2}{1} = 2$ in favour of my winning)
  + *the odds that you'll win are 1:2*($\frac{1}{2} = 0.5$)
  - if the odds are even (1:1), then: $\frac{1}{1} = 1$
  
- odds of 1 correspond to a probability of 0.5

### Log-odds

- log-odds are just the logarithmically-transformed odds
  + $log(2) =$ `r log(2)`
  + $log(0.5) =$ `r log(0.5)`
  + $log(1) =$ `r log(1)`
- so the log-odds of 0 correspond to a probability of 0.5 (and odds of 1)

## Calculating odds/log-odds/probability

- Equations \ref{eq1}-\ref{eq3} demonstrate the relationship between the three 

\begin{align}
p &= \frac{odds}{1 + odds} \label{eq1}\\
odds &= \frac{p}{1-p} \label{eq2}\\
log\;odds &= exp(odds) \label{eq3}
\end{align}

- TASK: Using R and Equations \ref{eq1}-\ref{eq3}, compute:
  + the probability and log odds for odds of 0.082
  + the log odds and odds for a probability of 0.924
  + the probability and odds for a log odds of -2.5

## Comparing odds/log-odds/probability

- @tbl-odds gives an example of how the three relate to each other
  + did you get the correct values?

```{r}
#| echo: false
#| label: tbl-odds
#| tbl-cap: Comparison of different values of probabilities/odds/log-odds
library(gt)
col_func <- colorRampPalette(c('lightblue', '#DE77AE'))

# Test function:

# col_func(4)

df_odds |> 
  filter(row_number() %% 5 == 1) |> 
  select(-q) |> 
  mutate(row = row_number()) %>%
  pivot_longer(
    cols = c(prob, odds, log_odds)
  ) |> 
  # pivot_wider(names_from = Cor, values_from = value)
  pivot_wider(
    # id_cols = names,
    names_from = row,
    values_from = value
  ) |> 
  knitr::kable(digits = 3, col.names = NULL) |> 
  kableExtra::kable_styling()
```

- try the task again, this time using `plogis()`, which produces a probility from a log odds

## Plotting odds/log-odds/probability

- this relationship is demonstrated in @fig-odds
- take your time to really understand these plots

```{r}
#| echo: false
#| label: fig-odds
#| fig-cap: Relationship between probability, odds, and log-odds
#| fig-asp: .35
fig_log_odds <-
  df_odds |> 
  ggplot() +
  aes(x = log_odds, y = odds) +
  geom_line() +
  scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
  labs(title = "Odds versus log-odds") +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "slategrey")

fig_prob_odds <-
  df_odds |> 
  ggplot() +
  aes(x = prob, y = odds) +
  geom_line() +
  scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
  labs(title = "Odds versus probability") +
  geom_vline(xintercept = .5, linetype = "dashed", colour = "slategrey")

fig_prob_log <-
  df_odds |> 
  ggplot() +
  aes(x = prob, y = log_odds) +
  geom_line() +
  scale_x_continuous(breaks = seq(-5, 5, by = 1)) +
  labs(title = "Odds versus probability") +
  geom_vline(xintercept = .5, linetype = "dashed", colour = "slategrey")

fig_log_odds + fig_prob_odds + fig_prob_log + plot_annotation(tag_levels = "A")
```

# Logistic regression

- let's run our first logistic regression to understand this relationship better
- Most relevant to the output of a logistic regression model is @fig-odds C
  + the model will output log-odds, and we most likely want to interpret them in terms of probabilities
  
## Load data

- load in the @biondo_yesterday_2022 dataset again
  + let's look at the binomial measure *regression in* at the *verb* region

```{r}
df_tense <-
  read_csv(here("data", "Biondo.Soilemezidi.Mancini_dataset_ET.csv"),
           locale = locale(encoding = "Latin1") # for special characters in Spanish
           ) |> 
  clean_names() |> 
  mutate(gramm = ifelse(gramm == "0", "ungramm", "gramm"))  |> 
  filter(roi == 4,
         adv_type == "Deic")
```

## EDA

- conduct a quick EDA: print head of data

```{r}
#| output-location: fragment
head(df_tense)
```

- and summary

```{r}
#| output-location: fragment
df_tense |> 
  select(roi, ri, ro) |> 
  summary()
```

### Plot

- let's plot the count of yes/no for regressions in

```{r}
#| code-fold: true

facet_labels <-
  c(
    "ri" = "Reg. In",
    "ro" = "Reg. Out",
    "Future" = "Future",
    "Past" = "Past"
  )
  
# fig_reg <-
  df_tense |> 
  filter(roi == "4") |> 
  mutate(gramm = as_factor(gramm),
         ri = ifelse(ri == 1, "yes", "no")) |> 
  drop_na(ri) |> 
  ggplot() +
  labs(title = "Count of regressions in") +
  aes(x = gramm, fill = ri) +
  geom_bar() +
  facet_grid(.~verb_t, labeller = as_labeller(facet_labels)) 
```

## Model

- run our model
  + `verb_t` and `gramm` are each two-level factors: set sum coding
  + `past` and `grammatical` = $-0.5$, and `future` and `ungrammatical` = `+0.5`
  
### Contrast coding

- for `verb_t`

```{r}
#| output-location: column-fragment
# verb_t as factor
df_tense$verb_t <- as.factor(df_tense$verb_t)
# check levels/order
levels(df_tense$verb_t)
```


```{r}
#| output-location: column-fragment
# set contrasts accordingly
contrasts(df_tense$verb_t) <- c(+0.5, -0.5)
# check contrasts
contrasts(df_tense$verb_t)
```

- for `gramm`

```{r}
#| output-location: column-fragment
# as factor
df_tense$gramm <- as.factor(df_tense$gramm)
# check
levels(df_tense$gramm)
```

```{r}
#| output-location: column-fragment
# set contrasts
contrasts(df_tense$gramm) <- c(-0.5, +0.5)
# check contrasts
contrasts(df_tense$gramm)
```

### Fit model

- we use the `glm()` function to fit a *genearlised* linear model
  + use the argument `family = "binomial"` to indicate our data are binomial

```{r}
fit_tense_ri <-
  glm(ri ~ verb_t*gramm,
    data = df_tense,
    family = "binomial")
```

### Coefficients 

```{r}
tidy(fit_tense_ri) %>%
  mutate(p.value = as.numeric(p.value)) |> 
  mutate(p.value = round(p.value,10)
         ) |> 
  knitr::kable(digits = 2) |> 
  kableExtra::kable_styling()
```

- the intercept is negative: below 0
  + verb tense is positive: more regressions in for the `future` compared to the `past`, holding grammaticality constant
  + grammaticality is positive: more regressions in for the `ungrammatical` than `grammatical` conditions

### Interpreting 0

- what does zero mean here? 
  + logistic regression gives the estimates in log-odds
  + in log-odds, a value of 0 means there is an equal probability of a regression in or out for both conditions (as in @tbl-odds)
  + i.e., the slope is flat (or not significantly different from 0)
- How can we convert our log-odds estimates to something more interpretable, like probabilities?

### Log-odds to probabilities

- we can just use the `plogis()` function
- we can also just use the `exp()` function to get the odds ratio from the log-odds

- let's look at our coefficients in probabilities:

```{r}
#| output-location: column-fragment
plogis(-1.23) # intercept in prob
```

```{r}
#| output-location: column-fragment
plogis(0.0277) # tense in prob
```

- and in odds

```{r}
#| output-location: column-fragment
exp(-1.23) # intercept in odds
```


```{r}
#| output-location: column-fragment
exp(0.0277) # tense in odds
```

#### Streamlining

- this is a bit tedious
  + we can also just feed a tibble column through the `plogis()` and `exp()` functions to print a table with the relevant probabilities and odds

```{r}
tidy(fit_tense_ri) %>%
  mutate(p.value = round(p.value*4,10),
         prob = plogis(estimate),
         odds = exp(estimate)
         ) |> 
  mutate_if(is.numeric, round, 4) |> 
  knitr::kable(digits = 2) |> 
  kableExtra::kable_styling()
```

### Interpreting our slopes

- the odds of a regression in for the future tense versus the past tense is ~1, with the corresponding probability of 0.51
  _ unsuprisingly, we see this *p*-value indicates this effect was not significant (*p* > .05), and the *z*-value (note: not *t*-value!) is also low
  + *z*-values correspond to the estimate divided by the standard error; it's interpretation is similar to that of the *t*-value: a *z*-value of ~2 or higher will likely have a *p*-value below 0.05. 
```{r}
#| echo: false
tidy(fit_tense_ri) %>%
  mutate(p.value = round(p.value*4,10),
         prob = plogis(estimate),
         odds = exp(estimate)
         ) |> 
  mutate_if(is.numeric, round, 4) |> 
  knitr::kable(digits = 2) |> 
  kableExtra::kable_styling(font_size = 26)
```
  
### Interpreting interaction

- interaction term is negative, what does this mean?
  + the effect of congruence is different in either level of tense
  + these effects are often more easily interpreted with a visualisation, e.g., using the `plot_model()` function from the `sjPlot` package (this effect is not significant, however)

```{r}
#| output-location: column-fragment
#| label: fig-inter
#| fig-cap: "Interaction plot of grammaticality and tense"
sjPlot::plot_model(fit_tense_ri, 
                   type = "eff", 
                   terms = c("gramm", "verb_t")) +
  geom_line() +
  theme(text = element_text(size = 26))
```

### Extracting predicted values

- we can use the `predict()` function to extract the predicted values for each condition
- We could just simply print the predicted values (`predict(fit_tense_ri)`), append the predicted values to the data frame

```{r}
# make sure dataset is the same length as the model data
df_tense_v <-
  df_tense |> 
  drop_na(ri)

# append model estimates
df_tense_v <-
  augment(fit_tense_ri, data = df_tense_v) |> 
  distinct(verb_t, gramm, .keep_all = T) |>
  arrange(verb_t) |>  
  select(verb_t, gramm, .fitted)
```

### Predicted values and slopes

- now if we look at the predicted log-odds values for the future and past tenses:

```{r}
#| output-location: column-fragment
df_tense_v |> 
  summarise(
    mean_tense = mean(.fitted),
    .by = verb_t) 
```

- What is the difference between these two numbers (in our model summary)?
  + it's `r round(-1.2114 - -1.2390,2)`: our slope for `verb_t`

```{r}
#| output-location: column-fragment
df_tense_v |> 
  summarise(
    mean_gramm = mean(.fitted),
    .by = gramm)
```

- What is the difference between these two numbers (in our model summary)?
  + it's `r round(-1.06377 - -1.38666,2)`: our slope for `verb_t`

- slopes for `verb_t` and `gramm` correspond to the predicted difference between their levels

# Interpreting our coefficients

- what do our estimates reflect, though?
  + let's remind ourselves of the rate of regressions in at the verb region:

```{r}
#| output-location: column-fragment
intercept <- tidy(fit_tense_ri)$estimate[1]
tense <- tidy(fit_tense_ri)$estimate[2]
gramm <- tidy(fit_tense_ri)$estimate[3]
interact <- tidy(fit_tense_ri)$estimate[4]
```

- let's remind ourselves of our contrast coding, so we can plug these into our equation of a line

```{r}
#| output-location: column-fragment
contrasts(df_tense_v$verb_t)
```

```{r}
#| output-location: column-fragment
contrasts(df_tense_v$gramm)
```

## Calculating our predictions

- what's the probability of a regression in for the past (`tense` = -0.5) grammatical (`gramm` = -0.5) condition?

```{r}
#| output-location: column-fragment
plogis(intercept + tense*-.5 + gramm*-.5)
```

- and past ungrammatical (change `gramm` to +0.5)?

```{r}
#| output-location: column-fragment
plogis(intercept + tense*-.5 + gramm*.5)
```

- And for the future condition (`verb_t` = 0.5) grammatical (`gramm` = -0.5)?

```{r}
#| output-location: column-fragment
plogis(intercept + tense*.5 + gramm*-.5)
```

- and future ungrammatical (gramm = +0.5)?

```{r}
#| output-location: column-fragment
plogis(intercept + tense*.5 + gramm*.5)
```


\begin{equation}
y_i = b_0 + b_1x_i + b_2x_1 + ... + e \label{eq-line}
\end{equation}

## Math with factors

- so, even when our dependent *and* independent variables are categorical, we can include them in our equation of a line (equation \ref{eq-line})
- we do this by assigning them numerical values
  + a probability/log odd/odds for a binomial dependent variable
  + and contrast coding for categorical predictors


# Reporting

@sonderegger_regression_nodate-1 (Section 6.9) makes a few important points regarding coefficients:

> Reporting a logistic regression model in a write-up is generally similar to reporting a linear regression model: the guidelines and rationale in section 4.6 for reporting individual coefficients and the whole model hold, with some adjustments.

> For each regression coefficient you report at a minimum the coefficient estimate, its SE, the test statistic value...and corresponding p-value.

> As for linear regression, it is useful to also give visualizations, CIs, and basic descriptive statistics, but what is appropriate will depend on context and space.

> Model prediction plots are especially important for interpreting logistic regressions, as discussed in section 6.7.3.

## Producing table summaries with `papaja`

- we can produce such a table using e.g., `papaja` package (true for any type of model; @tbl-glm-summary)

```{r}

library(papaja)

fit_tense_ri |> 
  apa_print() |>
  apa_table(label = "tbl-glm-summary",
            caption = "Model summary for regressions in at the verb region. Estimates are given in log odds.")
```

## Producing table summaries with `broom::tidy()`

- or by extracting the model summary with `tidy()`, and even adding our probabilities (@tbl-glm-summary-tidy)

```{r}

tidy(fit_tense_ri, conf.int = TRUE) |> 
  mutate(prob = plogis(estimate)) |> 
  relocate(prob, .after = std.error) |> 
  apa_table(label = "tbl-glm-summary-tidy",
            caption = "Same table with `tidy()`")
```

# Learning Objectives {.unnumbered .unlisted}

Today we learned...

- how to model binomial data with logistic regression
- how to interpret log-odds and odds ratio
- how to report a logistic regression model


# Important terms {.unnumbered .smaller}

```{r}
#| echo: false
tribble(
 ~"term", ~"description/other terms",
 
) %>% kable() %>% kable_styling()
```

# Task

## Regressions in at the adverb region

Using the same dataset, run a logistic model exploring regressions in (`ri`) at the adverb region (`roi = "2"`). Before you run the model, do you have any predictions? Try plotting the regressions in for this region first, and generate some summary tables to get an idea of the distributions of regressions in across conditions.

```{r}
#| echo: false
#| eval: false

df_tense <-
  read_csv(here("data", "Biondo.Soilemezidi.Mancini_dataset_ET.csv"),
           locale = locale(encoding = "Latin1") # for special characters in Spanish
           ) |> 
  clean_names() |> 
  mutate(gramm = ifelse(gramm == "0", "ungramm", "gramm"))  |> 
  filter(roi == 2,
         adv_type == "Deic")

head(df_tense)

# plot

facet_labels <-
  c(
    "ri" = "Reg. In",
    "ro" = "Reg. Out",
    "Future" = "Future",
    "Past" = "Past"
  )
  
# fig_reg <-
  df_tense |> 
  mutate(gramm = as_factor(gramm),
         ri = ifelse(ri == 1, "yes", "no")) |> 
  drop_na(ri) |> 
  ggplot() +
  labs(title = "Count of regressions in") +
  aes(x = gramm, fill = ri) +
  geom_bar() +
  facet_grid(.~verb_t, labeller = as_labeller(facet_labels)) 


# set contrasts: gramm
df_tense$gramm <- as.factor(df_tense$gramm)
levels(df_tense$gramm)
contrasts(df_tense$gramm)
contrasts(df_tense$gramm) <- c(-0.5,0.5)
# set contrasts: verb_t
df_tense$verb_t <- as.factor(df_tense$verb_t)
levels(df_tense$verb_t)
contrasts(df_tense$verb_t)
contrasts(df_tense$verb_t) <- c(0.5,-0.5)


# run model
fit_adverb <-
  glm(ri ~ verb_t*gramm,
      data = df_tense,
      family = "binomial")

summary(fit_adverb)

fit_adverb |> 
  papaja::apa_print() |> 
  papaja::apa_table()
 
papaja::apa_print(fit_adverb)$table |> 
  knitr::kable(digits = 2) |> 
  kableExtra::kable_styling()

tidy(fit_adverb) |> 
  mutate(prob = plogis(estimate)) |> 
  knitr::kable(digits = 2) |> 
  kableExtra::kable_styling()

tidy(fit_adverb, conf.int = T)
```


## Dutch verb regularity

Load in the `regularity` data from the `languageR` package.

```{r}
df_reg <-
  regularity |> 
  clean_names()
```

> Regular and irregular Dutch verbs and selected lexical and distributional properties.

Our relevant variables will be:

- `written_frequency`: a numeric vector of logarithmically transformed frequencies in written Dutch (as available in the CELEX lexical database).
- `regularity`: a factor with levels irregular (1) and regular (0).
- `verb`: a factor with the verbs as levels.

```{r}
names(df_reg)

df_reg |> 
  mutate(reg_n = ifelse(regularity == "regular",1,0))
```


1. Fit a logistic regression model to the data which predicts verb regularity by written frequency. Consider: What type of predictor variable do you have, and what steps should you take before fitting your model?

2. Print the model coefficients, e.g., using `tidy()`.

3. Interpret the coefficients, either in log-odds or probabilities. Report your findings.


# Literaturverzeichnis {.unlisted .unnumbered visibility="uncounted"}

::: {#refs custom-style="Bibliography"}
:::


