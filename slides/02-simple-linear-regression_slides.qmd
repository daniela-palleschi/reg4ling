---
title: "Simple linear regression"
subtitle: "WiSe23/24"
author: "Daniela Palleschi"
institute: Humboldt-Universität zu Berlin
lang: en
date: 2023-10-10
format: 
  pdf:
    output-file: 02-simple-linear-regression.pdf
    toc: true
    number-sections: false
    colorlinks: true
    code-overflow: wrap
  revealjs:
    code-overflow: wrap
    theme: [dark]
    width: 1600
    height: 900
    progress: true
    scrollable: true
    # smaller: true
    slide-number: c/t
    code-link: true
    # logo: logos/hu_logo.png
    # css: logo.css
    incremental: true
    # number-sections: true
    toc: false
    toc-depth: 2
    toc-title: 'Overview'
    navigation-mode: linear
    controls-layout: bottom-right
    fig-cap-location: top
    font-size: 0.6em
    slide-level: 4
    self-contained: true
    title-slide-attributes: 
      data-background-image: logos/logos.tif
      data-background-size: 15%
      data-background-position: 50% 92%
    fig-align: center
    output-location: fragment
editor_options: 
  chunk_output_type: console
bibliography: ../references.bib
csl: ../apa.csl
---

```{r setup, eval = T, echo = F}
knitr::opts_chunk$set(echo = T, # print chunks?
                      eval = T, # run chunks?
                      error = F, # print errors?
                      warning = F, # print warnings?
                      message = F, # print messages?
                      cache = F # cache?; be careful with this!
                      )
```

```{r}
library(broman)
# function to format p-values
format_pval <- function(pval){
	dplyr::case_when(
		pval < .001 ~ "< .001",
		pval < .01 ~ "< .01",
		pval < .05 ~ "< .05",
		TRUE ~ broman::myround(pval, 3)
	)
}

# round to nearest non-zero decimal
my_round = function(x, n=2) {
  max(abs(round(x, n)), abs(signif(x, 1))) * sign(x)
  }
```


# Learning Objectives {.unnumbered .unlisted}

Today we will learn...

- how to fit a simple linear model with the `lm()` function
- how to interpret our model output
- how to report our model

## Workflow

```{r echo = F, fig.align = "center"}
#| label: fig-Wickham-workflow
#| fig-cap: "Image source: @wickham_r_2023 (all rights reserved)"
#| fig-cap-location: bottom
library(magick)
y <- magick::image_read(here::here("media/Wickham_tidyworkflow.png"))

magick::image_negate(y)
```

## Set-up environment  {.unnumbered}

- always start with a clean R Environment (`Session > Restart R`)
  + go to the `Session` > select `Restart R`
  + or use the keyboard shortcut :`Cmd/Ctrl+Shift+0` 
  
---

- I usually run`options(scipen=999)` to supress scientific notation

```{r}
# suppress scientific notation
options(scipen=999)
```

- load in our required packages

```{r}
# load libraries
pacman::p_load(
               tidyverse,
               here,
               broom,
               lme4,
               janitor,
               languageR)
```

```{r}
#| echo: false

# extra packages for the lecture notes/slides
pacman::p_load(
               patchwork,
               knitr,
               kableExtra,
               gt,
               googlesheets4)

# tell googlesheets4 we don't want private
gs4_deauth()

theme_set(theme_minimal())
```

- and set my preferred `ggplot2` theme

```{r}
# set ggplot theme
theme_set(theme_bw())
```

# Simple linear model: RT ~ frequency {.smaller}

- $y \sim x$ can be read as "y as a function of x", or "y predicted by x"
- following @winter_statistics_2019, we will first model some word frequency data. 
- our first model will be:

$$
RT \sim frequency
$$



## Load data

- load our data using the `read_csv()` function from `readr`
- the `clean_names()` function from the `janitor` package tidies up variable names (e.g., no spaces, all lower case).

```{r}
# load ELP_frequency.csv
df_freq <- read_csv(here("data", "ELP_frequency.csv")) |> 
  clean_names()
```


## Mini-EDA

- Exploratory Data Analysis is usually first step once collecting data
  + involves plotting and summarising variables of interest
- let's explore the data a little bit, which is what we would normally do before fitting any models

### `head()`

- let's use `head()` from base R to see the first 6 rows of our data

```{r}
# print head of df_freq
head(df_freq)
```

- 3 columns: `word`, `freq`, and `rt`
- we can assume that they correspond to the word, its frequency, and the reaction time, respectively
- We can also see in our global environment that there are 12 observations, meaning 12 rows

### `summary()`

- `summary()` provides summaries of each variable in a dataframe
- numeric variables: descriptive statistics for the centre and spread of the data (mean, median, quartiles)
- categorical data: count per category
- character variables: number of observations

```{r}
summary(df_freq)
```

### Plotting

- what does the relationship between `freq` and `rt` look like?

```{r}
plot(df_freq$freq, df_freq$rt)
```

- a lot of frequency values below roughly 400
  + these seem to have higher reaction times than those with a higher frequency value
- let's fit these data to our first linear model to explore this effect of frequency on reaction times

## `lm()`

- `lm()` function fits simple linear models
- as arguments it takes a formula ($y ~ x$) and a dataset, at minimum
- for now, we will use $1$ for our predictor, which is a placeholder for the intercept
  
$$
lm(outcome \sim 1 + predictor,\;data\;=\;df\_name)
$$

- the intercept is included by default
  + so if you omit the `1` the intercept is still included in the formula
- if you wanted to remove the intercept (which you often don't), you could replace `1` with `0`

### Running a model

- to run such a model with our `df_freq` data:

```{r}
#| eval: false
lm(rt ~ 1, data = df_freq) 
```

- or, to save the model as an object so that we can call on it later, assign it a name (`name <- value`)

```{r}
fit_rt_1 <- lm(rt ~ 1, data = df_freq) 
```

::: {.callout-tip}

### Object naming

- the letters `df` in `df_freq` stand for 'data frame'
  + this serves as a reminder of what exactly that object in our environment is
- we are saving our model as `fit_rt_1`, using 'fit' to signal that this object is a model fit. You could also save it as `mod_freq_1`, `lm_freq_1`, or whatever you see fit (there are no rules)
- if we plot the frequency data, we could call save the plot as `fig_freq` or `plot_freq`
- this simply helps keep our environment structured, which will become useful when you begin having more objects in your environment at a time

:::

### Model ouput

- print our model

```{r}
# print model
fit_rt_1
```

- `intercept` and `slope` are called `coefficients`
  + Why do we only see `Intercept`? 
  + because we didn't include any predictors in our model. 

--- 

- We typically use the `summary()` function to print full model outputs.

```{r}
summary(fit_rt_1)
```

---

::: {.callout-tip}
### `broom` package

The `broom` package has some useful functions for printing model outputs

  + `tidy()` produces a `tibble` (type of dataframe) of the `coefficients`
  + `glance()` produces goodness of fit measures (which we won't discuss)

The outputs from `tidy()` and `glance()` can be fed into `kable` and/or `kable_styling()` to create formatted tables

```{r}
#| output-location: column-fragment
tidy(fit_rt_1)
```

```{r}
#| output-location: column-fragment
glance(fit_rt_1)
``` 

`augment()` adds model values as columns to your dataframe (e.g., useful for plotting observed vs. fitted values).

```{r}
#| eval: false
augment(fit_rt_1, data = df_freq) %>% summary()
```

:::

## Interpreting model output

- let's take a closer look at our model summary

```{r}
#| eval: false
summary(fit_rt_1)
```

```{r, eval = F}
Call:
lm(formula = rt ~ 1, data = df_freq) #<1>

Residuals:
     Min       1Q   Median       3Q      Max
-172.537  -74.677   -9.137   91.296  197.613   #<2>

Coefficients:
            Estimate Std. Error t value       Pr(>|t|)    #<3>
(Intercept)   679.92      34.02   19.99 0.000000000538 *** #<4>
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #<5>

Residual standard error: 117.8 on 11 degrees of freedom #<6>
```

1) formula repetition
2) residuals: differences between observed values and those predicted by the model
3) names for columns `Estimates`, `standard error`, `t-value`, `p-value` (`Pr(>|t|)`)
4) Intercept ($b_0$)
5) Significance codes
6) Standard deviation of residuals/error in our model (lower = better)

#### Intercept

- intercept is roughly `r round(mean(df_freq$rt),1)` milliseconds; what does this number represent?

```{r}
#| output-location: column-fragment

# print model intercept
coef(fit_rt_1)['(Intercept)']
```

```{r}
#| output-location: column-fragment

# print data mean
mean(df_freq$rt)
```

- intercept corresponds to the mean reaction time value
  + why is this?

#### Plotting `rt ~ 1`

```{r}
#| echo: false
df_freq <-
  df_freq %>%
  mutate(jitter = rnorm(nrow(.), sd = .1))
  
fig_rt_1 <-
  df_freq %>% 
  ggplot(aes(x = 0, y = rt)) +
  labs(title = "rt ~ 1",
       y = "First-pass RTs (ms)") + 
  geom_point(alpha = .2) +
  theme(legend.position = "none") +
  geom_point(aes(x = 0, y=mean(rt)), colour = "red", size = 3) +
  geom_hline(aes(yintercept=mean(rt)), colour = "red") +
  theme_bw() +
  scale_x_continuous(limits = c(-1,1),breaks = c(-1,-.5,.5,0,1)) +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        text = element_text(size=9))

fig_rt_1_pj <-
  df_freq %>% 
  ggplot(aes(x = jitter, y = rt)) +
  labs(title = "with position_jitter(.2)",
       y = "First-pass RTs (ms)") + 
  geom_point(alpha = .2) +
    theme(legend.position = "none") +
  geom_point(aes(x = 0, y=mean(rt)), colour = "red", size = 3) +
  geom_hline(aes(yintercept=mean(rt)), colour = "red") +
  theme_bw() +
  scale_x_continuous(limits = c(-1,1),breaks = c(-1,-.5,.5,0,1)) +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        text = element_text(size=9))

fig_rt_1_pj_res <-
df_freq %>% 
  ggplot(aes(x = jitter, y = rt)) +
  labs(title = "residuals",
       y = "First-pass RTs (ms)") +
  geom_linerange(aes(x = jitter, ymin = mean(rt), ymax = rt),
                 colour = "pink") +
  geom_point(alpha = .2) +
  theme(legend.position = "none") +
  geom_point(aes(x = 0, y = mean(rt)), colour = "red", size = 3) +
  geom_hline(aes(yintercept = mean(rt)), colour = "red") +
  theme_bw() +
  scale_x_continuous(limits = c(-1, 1), breaks = c(-1, -.5, .5, 0, 1)) +
  theme(axis.title.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        text = element_text(size=9))
```

- @fig-rt1 shows the intercept (red dot) amongst the observed data (black dots)
  + along the x-axis we have abstract numerical units (the values don't mean anything)
  + what would the values of the intercept be?

```{r}
#| label: fig-rt1
#| echo: false
#| fig-cap: "Visualisation of 'rt ~ 1': observed values (black) and mean (intercept; red). Residuals would be the distance from each black dot to the y-value of the read dot"
#| out-width: "100%"
#| fig-asp: .5
fig_rt_1 + fig_rt_1_pj + fig_rt_1_pj_res + plot_annotation(tag_levels = "A")
```


## Adding a fixed effect (slope)

- let's include a predictor, which will give us a *slope*
- the slope represents the change in $y$ (DV: `rt`) when we move 1-unit along $y$ (IV: `freq`)
  + it tells us the *effect* our IV has on the DV (although be weary of making causal inferences)

---

- let's first plot the data again, but with a line:

```{r}
df_freq |> 
  ggplot() +
  aes(x = freq, y = rt) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

- what does this tell us?

### Fit model (treatment contrasts)

```{r}
# fit simple linear model
fit_rt_freq <- lm(rt ~ freq, data = df_freq)
```

#### Model summary

```{r}
summary(fit_rt_freq)
```

#### Intercept

- our intercept is no longer the grand mean of first-pass reading times...what is it?

```{r}
#| output-location: column-fragment

# print model intercept
coef(fit_rt_freq)['(Intercept)']
```

```{r}
#| output-location: column-fragment

# print data mean
mean(df_freq$rt)
```

#### Slope

- the slope is `r coef(fit_rt_freq)['freq']`
  + what does this correspond to?

```{r}
#| output-location: fragment

# print slope
coef(fit_rt_freq)['freq']
```

- change in $y$ (our DV `rt`) for a 1-unit change in $x$ (our IV: `freq`)
  + how we interpret this value depends on the measurement unit your variables are
  

# Exploring the model

- we can extract information from our model and compare it to our observed data

```{r}
#| output-location: column-fragment
# how many observed values did we enter into the model?
df_freq |> 
  nrow()
```

```{r}
#| output-location: column-fragment
# how many fitted values does our model have?
length(fitted(fit_rt_freq))
```

## Exploring the model: residuals

```{r}
#| output-location: fragment
# what do our FITTED values look like?
head(fitted(fit_rt_freq))
```

```{r}
#| output-location: fragment
# what do our OBSERVED values look like?
head(df_freq$rt)
```

```{r}
#| output-location: fragment
# what is the difference between the FITTED and OBSERVED values?
head(df_freq$rt) - head(fitted(fit_rt_freq))
```

```{r}
#| output-location: fragment
# what are our RESIDUALS?
head(residuals(fit_rt_freq))
```

## Exploring the model: predicted values {-}

- what were our coefficients?

```{r}
#| output-location: fragment
coef(fit_rt_freq)
```

- what would be our predicted reaction time for a word with frequency of 0?

```{r}
#| output-location: fragment
coef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 0
```

- ignore the `(Intercept)` label here, `R` just takes the first label when performing an operation on 2 vectors

- what is the predicted reaction time for a word with frequency score of `5000`?

```{r}
#| output-location: fragment
coef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 5000
```

# Model assumptions

- is our model a good fit for our data?
- linear regression makes assumptions about our data
  - these assumptions relate to the *residuals* of our model, not the raw data points themselves
- we'll focus on two assumptions for now:
  + assumptions of *normality* of the residuals
  + the constant *variance* of the residuals
- both assumptions are often diagnosed visually, so it takes some practice to learn what looks right

## Normality

- a model's *residuals* (i.e., the difference between the *fitted* and *observed* values) should be approximately normally distributed
- Normality is typically visualised using a histogram (@fig-Winter-assumptions A) and/or a quantile-quantile (Q-Q) plot  (@fig-Winter-assumptions B).


```{r echo = F, fig.align = "center"}
#| label: fig-Winter-assumptions
#| fig-cap: "Image source: @winter_statistics_2019 (all rights reserved)"
#| fig-cap-location: bottom
library(magick)
y <- magick::image_read(here::here("media/Winter_2019_assumptions_plots.png"))

magick::image_negate(y)
```

## Constant variance

- if a model satisfies the constant variance assumption (also called *homoscedasticity*, or the absence of *heteroscedasticity*), the spread of residuals will be equal across the regression line
- typically visualised using a residual plot, which should look like a blob (@fig-Winter-assumptions C).

## Visualising model assumptions

- let's plot our residuals to assess whether our model satisfies the assumptions of normality and constant variance

### Histogram

- let's use use the `augment()` function from `broom` to append model values to our original data frame, and then feed this into `ggplot()` from `ggplot2` (or even feed it into `hist()`).

```{r}
df_freq <- broom::augment(fit_rt_freq, df_freq)
```

```{r}
# and create ggplot
df_freq |> 
  ggplot() +
  aes(x = .resid) +
  geom_histogram(bins = 8, fill = "grey", colour = "black") +
  theme_bw()
```

### Q-Q plot

```{r}
df_freq |> 
  ggplot() +
  aes(sample = .resid) +
  geom_qq(colour = "red") +
  geom_qq_line() 
```

### Residual plot

```{r}
df_freq |> 
  ggplot() +
  aes(x = .fitted, y = .resid) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

# Reporting our model

- following @sonderegger_regression_nodate-1 (Section 4.6.1), we should report
  + our individual coefficients (coefficient estimate, standard error,    test statistic (e.g., *t*-value) and corresponding *p*-value)
  + measures of model fit

## Coefficients

- this can be written

> Higher-frequency words had longer reaction times, but this effect was not significant ($\hat{\beta}$ = `r round(coef(fit_rt_freq)['freq'],3)`; t = `r round(tidy(fit_rt_freq)[2,'statistic'],2)`).

- and/or presented in a table

```{r}
#| code-fold: true

# issue with mathmode in quarto; use unicode and fmt_markdown (https://kazuyanagimoto.com/blog/2022/12/27/quarto_tips/)
tidy(fit_rt_freq) |> 
  mutate(p.value = format_pval(p.value),
         estimate = round(estimate,3),
         std.error = round(std.error,3),
         statistic = round(statistic,2)) |> 
  # rename(`ˆβ` = estimate) |>
  gt() |> 
  cols_label(
   term = "Coefficient",
   estimate = "ˆβ",
   std.error = "SE",
   statistic = md("*t*"),
   p.value = md("*p*")
  ) |> 
  fmt_markdown(columns = everything()) |> 
  fmt_number(decimals = 3,
             drop_trailing_zeros = T) |> 
  tab_options(table.font.size = 36)
```

## Model fit

- $R^2$ is a measure of goodness of fit, representing the proportion of variance in the data that is described by our model
- AIC and BIC are also measures of goodness of fit (Akaike/Bayesian information criteria)
  + used to compare models
  + *lower* AIC/BIC values are better (when comparing models); think about temperatures: the colder the better
  + penalise for number of parameters in the model
  
  
```{r}
#| code-fold: true
#
glance(fit_rt_freq) |> 
  select(r.squared, sigma, df, AIC, BIC) |> 
   gt() |> 
  cols_label(
   r.squared = "R^2",
   sigma = "sigma"
  ) |> 
  fmt_markdown(columns = everything()) |> 
  fmt_number(decimals = 3,
             drop_trailing_zeros = T) |> 
  tab_options(table.font.size = 36)
```


---

- in addition, we should/could provide
    + visualisations (of model predictions or of the raw data)
    + confidence intervals for the coefficients
    + descriptive statistics where relevant (e.g., factorial designs)

```{r}
#| code-fold: true

# plot model predictions
fig_fit <-
  sjPlot::plot_model(fit_rt_freq, type = "emm", terms = "freq") +
  scale_y_continuous(limits = c(300,900))

fig_raw <-
df_freq |> 
  ggplot() +
  aes(x = freq, y = rt) +
  labs(title = "Observed values") +
  geom_point() +
  geom_smooth(method = "lm", colour = "red", fill = "pink") +
  scale_y_continuous(limits = c(300,900))

library(patchwork)
fig_fit + fig_raw + plot_annotation(tag_levels = "A")
```

# Learning Objectives {.unnumbered .unlisted}

Today we learned...


# Task

Recycling the code above:

  + run the model with `freq` as predictor
  + extract the intercept
  + extract the slope
  + calculate the predicted reaction time for a word frequency of 462
  + run assumption diagnostics
  + assess model fit


# Literaturverzeichnis {.unlisted .unnumbered visibility="uncounted"}

::: {#refs custom-style="Bibliography"}
:::


