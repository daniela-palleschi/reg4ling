[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression for Linguists",
    "section": "",
    "text": "Course overview"
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "Regression for Linguists",
    "section": "Moodle",
    "text": "Moodle\n\nlecture materials"
  },
  {
    "objectID": "index.html#course-aims",
    "href": "index.html#course-aims",
    "title": "Regression for Linguists",
    "section": "Course aims",
    "text": "Course aims\nBy the end of this course, you will\n\nblah blah\n\n\nWhat will you learn?\n\nlinear regression\nmultiple regression\nlogistic regression\nmixed models\nusing the lme4 package\nhow to apply these models appropriately to a variety of data types\n\n\n\nWhat will you not learn?\n\nstuff"
  },
  {
    "objectID": "index.html#style-guide",
    "href": "index.html#style-guide",
    "title": "Regression for Linguists",
    "section": "Style guide",
    "text": "Style guide\nAs a self-respecting Canadian, I bounce between what is typically considered ‘American’ and ‘British’ spelling. This will be most notable in my use of -ise and -ize, but will never result in a dropped ‘u’ from words like colour, which is used often in plots. I say this so that non-native speakers don’t start to think they’ve been spelling words wrong."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Meeting\n      Lecture\n      Topic\n      Vorbereitung\n    \n  \n  \n    2023-10-10\n1\nEquation of a line\n\n📚 Winter (2019): Ch. 1-3\n\n    2023-10-11\n2\nLinear regression\n\n📚 Winter (2019): Ch. 4\n📚 Winter (2013)\n\n    2023-10-12\n3\nContinuous predictors\n\n📚 Winter (2019): Ch. 5\n📚 Winter (2013)\n\n    2023-10-10\n4\nMultiple linear regression\n\n📚 Winter (2019): Ch. 6\n📚 Winter (2013)\n\n    2023-10-11\n5\nCategorical predictors\n\n📚 Winter (2019): Ch. 7\n📚 Winter (2013)\n\n    2023-10-12\n6\nModel assumptions\n\n\n    2023-10-10\n7\nLogistic regression\n\n📚 Winter (2019): Ch. 12\n\n    2023-10-11\n8\nLog odds, logits, and odds ratio\n\n\n    2023-10-12\n9\nFoundational Ideas\n\nVasishth & Nicenboim (2016)\n\n    2024-01-12\n10\nLinear mixed models\n\n📚 Winter (2019): Ch. 14\nWinter & Grice (2021); until Section 3\n\n    2024-01-12\n11\nLinear mixed models\n\n\n    2024-01-26\n12\n\n\n    2024-01-26\n13\n\n\n    2024-02-09\n14\n\n\n    2024-02-09\n15"
  },
  {
    "objectID": "00-course_overview.html",
    "href": "00-course_overview.html",
    "title": "Resources and Set-up",
    "section": "",
    "text": "Resources\nThis course is mainly based on Winter (2019), which is an excellent introduction into regression for linguists. For even more introductory tutorials, I recommend going through Winter (2013) and Winter (2014) For a more intermediate textbook, I’d recommend Sonderegger (o. J.).\nIf you’re interested in the foundational writings on the topic of (frequentist) linear mixed models in (psycho)linguistic research, I’d recommend reading Baayen (2008); Baayen et al. (2008);Barr et al. (2013); Bates et al. (2015); Jaeger (2008); Matuschek et al. (2017); Vasishth (2022); Vasishth & Nicenboim (2016).\nFor this course, I assume that you are familiar with more classical statistical tests, such as the t-test, Chi-square test, etc. I also assume you are familiar with measures of central tendency (mean, median, mode) measures dispersion/spread (standard deviation), and with the concept of a normal distribution. Lacking this knowledge will not impeded your progress in the course, but is an important foundation on which we’ll be building. We can review these concepts in-class as needed."
  },
  {
    "objectID": "00-course_overview.html#install-r",
    "href": "00-course_overview.html#install-r",
    "title": "Resources and Set-up",
    "section": "Install R",
    "text": "Install R\n\nwe need the free and open source statistical software R to analyze our data\ndownload and install R: https://www.r-project.org"
  },
  {
    "objectID": "00-course_overview.html#install-rstudio",
    "href": "00-course_overview.html#install-rstudio",
    "title": "Resources and Set-up",
    "section": "Install RStudio",
    "text": "Install RStudio\n\nwe need RStudio to work with R more easily\nDownload and install RStudio: https://rstudio.com\nit can be helpful to keep English as language in RStudio\n\nwe will find more helpful information if we search error messages in English on the internet\n\nIf you have problems installing R or RStudio, check out this help page (in German): http://methods-berlin.com/wp-content/uploads/Installation.html"
  },
  {
    "objectID": "00-course_overview.html#install-latex",
    "href": "00-course_overview.html#install-latex",
    "title": "Resources and Set-up",
    "section": "Install LaTeX",
    "text": "Install LaTeX\n\nwe will not work with LaTeX directly, but it is needed in the background\nDownload and install LaTeX: https://www.latex-project.org/get/"
  },
  {
    "objectID": "00-course_overview.html#troubleshooting-en-troubleshooting",
    "href": "00-course_overview.html#troubleshooting-en-troubleshooting",
    "title": "Resources and Set-up",
    "section": "Troubleshooting (EN: Troubleshooting)",
    "text": "Troubleshooting (EN: Troubleshooting)\n\nError messages are very common in programming, at all levels.\nHow to find solutions for these error messages is an art in itself\nGoogle is your friend! If possible, google in English to get more information"
  },
  {
    "objectID": "01-equation_of_a_line.html#learning-objectives",
    "href": "01-equation_of_a_line.html#learning-objectives",
    "title": "1  Understanding straight lines",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nthe equation of a line\nabout intercepts, slopes, and residuals"
  },
  {
    "objectID": "01-equation_of_a_line.html#resources",
    "href": "01-equation_of_a_line.html#resources",
    "title": "1  Understanding straight lines",
    "section": "Resources",
    "text": "Resources\nThis lecture is based on the readings for today’s session: Winter (2013) and Winter (2019) (Ch. 3), and to a lesser extent (debruine_understanding_2021?); Winter (2014)."
  },
  {
    "objectID": "01-equation_of_a_line.html#when-to-model-your-data",
    "href": "01-equation_of_a_line.html#when-to-model-your-data",
    "title": "1  Understanding straight lines",
    "section": "1.1 When to model your data",
    "text": "1.1 When to model your data\nBy the time we get to the point of wanting to model our data, we should have a pretty good idea of how our data look. We achieve this through running an exploratory data analysis (EDA), which consists of visualising your data and determining outliers (a question for another day: what is an outlier?), generating summary (i.e., descriptive) statistics, and just overall getting to know your data, without making any claims beyond your data.\nHowever, an understanding of the data design and collection procedure is incredibly important and is necessary in order to appropriately fit a model to our data. In fact, planning out your analyses when designing your experiment is highly recommended in order to ensure your data will have the appropriate structure and that the assumptions made by your chosen analyses are taken into consideration before data collection.\nThe next step after conducting an EDA is to model your data, i.e., run inferential statistics, this is where we try to generalise beyond our data.\n\n1.1.1 Statistical tests versus models\nMany statistical courses and textbooks still put undue emphasis on classical statistical tests. However, these common statistical tests are simplified linear models, without the added benefits of linear models. In essence, statistical tests tell us something about our data, whereas statistical models can be used to make predictions about hypothetical future observations."
  },
  {
    "objectID": "01-equation_of_a_line.html#linear-regression",
    "href": "01-equation_of_a_line.html#linear-regression",
    "title": "1  Understanding straight lines",
    "section": "1.2 (Linear) Regression",
    "text": "1.2 (Linear) Regression\nData exploration gives us an idea about what our data look like, but if we want to be able to make predictions about hypothetical observations, i.e., to predict values of our DV based on one (or more) IV(s), we need to fit a model to our data. This model can then predict values of our DV based on one (or more) IV(s), i.e., predicting an outcome variable (dependent variable, DV) from one or more predictors (independent variable, IV). Because we’re making predictions, we need to take into account the variability (i.e., error) in our data.\n\n1.2.1 Types of regression\n\n\n\n\n\nregression type\npredictor\noutcome\n\n\n\n\nsimple regression\nSingle predictor\ncontinuous (numerical)\n\n\nmultiple regression\nmultiple predictor\ncontinuous (numerical)\n\n\nhierarchical/linear mixed models/linear mixed effect models\ninclude random effect\ncontinuous (numerical)\n\n\ngeneralised linear (mixed) models: logistic regression\nas above\nbinary/binomial data\n\n\ngeneralised linear (mixed) models: poisson regression\nas above\ncount data"
  },
  {
    "objectID": "01-equation_of_a_line.html#straight-lines",
    "href": "01-equation_of_a_line.html#straight-lines",
    "title": "1  Understanding straight lines",
    "section": "1.3 Straight lines",
    "text": "1.3 Straight lines\n\nlinear regression summarises the data with a straight line\n\nwe model our data as/fit our data to a straight line\n\nstraight lines can be defined by\n\nIntercept (\\(b_0\\))\n\nvalue of \\(Y\\) when \\(X = 0\\)\n\nSlope (\\(b_1\\))\n\ngradient (slope) of the regression line\ndirection/strength of relationship between \\(x\\) and \\(y\\)\nregression coefficient for the predictor\n\n\nso we need to define an intercept and a slope\n\n\n1.3.1 A line = intercept and slope\n\na line is defined by its intercept and slope\n\nin a regression model, these two are called coefficients\n\n\n\n\n\n\n\nFigure 1.1: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nEquation of a line\n\n\n\n\\[\n\\begin{align}\ny & = mx + c\\\\\nY_i &= (b_0 + b_1X_i) \\\\\noutcome_i & = (model) \\\\\ny_i & = (intercept + slope*x_i)\n\\end{align}\n\\]\n\n\n\n\n1.3.2 Intercept (\\(b_0\\))\n\nthe value of \\(y\\) when \\(x = 0\\)\n\n\n\n\n\n\n\n\n1.3.3 Slopes (\\(b_1\\))\nA slope describes a change in \\(x\\) (\\(\\Delta x\\)) over a change in \\(y\\) (\\(\\Delta y\\)), where \\(\\Delta\\) (the Greek letter delta) can be read as ‘difference’ (because it also starts with a ‘d’…). So a slope’s value equals the difference in \\(x\\) for a difference of 1 unit in \\(y\\). Positive slopes indicate that as \\(x\\) increases, \\(y\\) increases. A negative slope value indicates that as \\(x\\) increases, \\(y\\) decreases (or vice versa). A slope of 0 indicates there is no change in \\(y\\) as a function of \\(x\\), or: there is no change in \\(y\\) when the value of \\(x\\) changes.\n\\[\nslope = \\frac{\\Delta x}{\\Delta y}\n\\]\nThis relationship between \\(x\\) and \\(y\\) is sometimes referred to as “rise over run”: how do you ‘rise’ in \\(y\\) for a given ‘run’ in \\(x\\)? For example, if we were to measure children’s heights and ages, we would expect to find an increase in height for every increase in age. Or, for a linguistic example, we would expect to find longer whole-sentence reading times (a measure variable) for longer texts: if a sentence has 9 words (I find straight lines to be really interesting and fun.), we would expect longer reading times than a sentence with 3 words (I love lines.).\n\n\nwhat is the intercept of this line?\nwhat is the slope of this line?"
  },
  {
    "objectID": "01-equation_of_a_line.html#error-and-residuals",
    "href": "01-equation_of_a_line.html#error-and-residuals",
    "title": "1  Understanding straight lines",
    "section": "1.4 Error and residuals",
    "text": "1.4 Error and residuals\n\nfixed effects (IV/predictors): things we can understand/measure\nerror (random effects): things we cannot understand/measure\n\nin biology, social sciences (and linguistic research), there will always sources of random error that we cannot account for\nrandom error is less an issue in e.g., physics (e.g., measuring gravitational pull)\n\nresiduals: the difference (vertical difference) between observed data and the fitted values (predicted values)\n\n\n\n\n\n\n\nEquation of a line\n\n\n\n\\[\n\\begin{align}\ny & = mx + c\\\\\nY_i &= (b_0 + b_1X_i) + \\epsilon_i\\\\\noutcome_i & = (model) + error_i\\\\\ny_i & = (intercept + slope*x_i) + error_i\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 Method of least squares\n\nso how is any given line chosen to fit any given data?\nthe method of least squares\n\ntake a given line, and square all the residuals (i.e., \\(residual^2\\))\nthe line with the lowest sum of squares is the line with the best fit to the given data\nwhy do we square the residuals before summing them up?\n\nso all values are positive (i.e., so that negative values don’t cancel out positive values)\n\n\nthis is how we find the line of best fit\n\nR fits many lines to find the one with the best fit\n\n\n\n\n\n\n\n\nFigure 1.2: Observed values (A), Residuals for line of best fit (B), A line of worse fit with larger residuals (C)"
  },
  {
    "objectID": "01-equation_of_a_line.html#learning-objectives-1",
    "href": "01-equation_of_a_line.html#learning-objectives-1",
    "title": "1  Understanding straight lines",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…"
  },
  {
    "objectID": "01-equation_of_a_line.html#important-terms",
    "href": "01-equation_of_a_line.html#important-terms",
    "title": "1  Understanding straight lines",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Intercept\nNA\nNA"
  },
  {
    "objectID": "01-equation_of_a_line.html#tasks",
    "href": "01-equation_of_a_line.html#tasks",
    "title": "1  Understanding straight lines",
    "section": "1.5 Tasks",
    "text": "1.5 Tasks\n\n1.5.1 Task 1: pen-and-paper\nYou will receive a piece of paper with several grids on it. Follow the instructions, which include drawing some lines. If you aren’t in-class, this is the paper we are using:\n\n\n1.5.2 Task 2: simulating data\nAll of the figures we just saw (except Figure 1.1, which is from Winter (2019)) were generated in R. Simulating data and plotting is a great way to understand concepts, or even to map out our hypotheses. Let’s use R for the first time to try to simulate some data in order to plot lines. Our goal will be to produce a line that has the following:\n\nintercept = 4.5\nslope = 3\n\n\nPlanning\nFirst, think about what steps will be required to create such plots. Can you come up with a workflow plan (without peaking at the next tasks)?\n\n\nProducing our line"
  },
  {
    "objectID": "01-equation_of_a_line.html#literaturverzeichnis",
    "href": "01-equation_of_a_line.html#literaturverzeichnis",
    "title": "1  Understanding straight lines",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nWinter, B. (2013). Linear models and linear mixed effects models in R: Tutorial 1.\n\n\nWinter, B. (2014). A very basic tutorial for performing linear mixed effects analyses (Tutorial 2).\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "02-simple_linear_regression.html#learning-objectives",
    "href": "02-simple_linear_regression.html#learning-objectives",
    "title": "2  Simple linear regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nhow to fit a simple linear model with the lm() function\nhow to interpret our model output"
  },
  {
    "objectID": "02-simple_linear_regression.html#set-up-environment",
    "href": "02-simple_linear_regression.html#set-up-environment",
    "title": "2  Simple linear regression",
    "section": "Set-up environment",
    "text": "Set-up environment\nMake sure you always start with a clean R Environment (Session &gt; Restart R). This means you should have no objects stored in your Environment, and no packages loaded. To ensure this, you can go to the Session tab (up where you’ll find File, Help, etc.), and select Restart R. You can also use the keyboard shortcut Cmd/Ctrl+Shift+0 (that’s a zero, not an ‘oh’).\nIn addition, I often prefer to run options(scipen=999) in order to supress scientific notation, which writes very large or very small numbers in an unintuitive way. For example, 0.000005 is written 5e-06 in scientific notation.\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad in data\nNow that we’ve loaded our packages, we can take a look at our dataset. Fitting our data to a model typically follows an Exploratory Data Analysis (EDA), which consists of plotting and summarising a data set. We won’t get into the EDA process, but there are many great resources on how to go about it in R (e.g., https://r4ds.hadley.nz/).\nLet’s load our dataset using the read_csv() function from the tidyverse package readr. Alternatively, we could use base R to load in the data with read.csv(). Some additional functions in the code below:\n\nmutate_if(is.character, as.factor) force character variables to factors\nfilter for the verb region from critical items only, remove participant 3, and remove values of first-fixtation that are 0\n\n\n# load in dataset\n# df_freq &lt;-\n#   readr::read_csv(\n#     here::here(\"data/tidy_data_lifetime_pilot.csv\"),\n#     # for special characters\n#     locale = readr::locale(encoding = \"latin1\")\n#   ) |&gt;\n#   mutate_if(is.character, as.factor) |&gt; # all character variables as factor\n#   # mutate(lifetime = fct_relevel(lifetime, \"living\", \"dead\"),\n#   #        tense = fct_relevel(tense, \"PP\", \"SF\")) |&gt;\n#   filter(type == \"critical\", # only critical trials\n#          px != \"px3\", # px3 had a lot of missing values\n#          rt &gt; 0, # only values of rt above 0\n#          region == \"verb\") %&gt;% # critical region only\n#   droplevels() # remove any factor levels with no observations"
  },
  {
    "objectID": "02-simple_linear_regression.html#simple-linear-model-rt-frequency",
    "href": "02-simple_linear_regression.html#simple-linear-model-rt-frequency",
    "title": "2  Simple linear regression",
    "section": "2.1 Simple linear model: RT ~ frequency",
    "text": "2.1 Simple linear model: RT ~ frequency\nRecall that \\(y \\sim x\\) can be read as “y as a function of x”, or “y predicted by x”. Following Winter (2019), we will first model some word frequency data. In this experiment, Our first model will be:\n\\[\nRT \\sim frequency\n\\]\nLet’s load our data using the read_csv() function from readr. I also use the clean_names() function from the janitor package, which tidies up variable names (e.g., no spaces, all lower case).\n\n# load ELP_frequency.csv\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\n\n\n2.1.1 Mini-EDA\nLet’s explore the data a little bit, which is what we would normally do before fitting any models. First, let’s see how the data is structured.\n\n# print head of df_freq\nhead(df_freq)\n\n# A tibble: 6 × 3\n  word      freq    rt\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 thing    55522  622.\n2 life     40629  520.\n3 door     14895  507.\n4 angel     3992  637.\n5 beer      3850  587.\n6 disgrace   409  705 \n\n\nLooks like there are only 3 columns: word, freq, and rt. We can assume that they correspond to the word, its frequency, and the reaction time, respectively. We can also see in our global environment that there are 12 observations, meaning 12 rows.\nThe summary() function provides summaries of each variable in a dataframe. For numeric variables, it will provide descriptive statistics for the centre and spread of the data (mean, median, quartiles). For categorical data, it will provide the count per category. For character variables, simply lists the number of observations.\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5  \n\n\nWe see freq has a pretty big range, from 4 to 55522. rt has a range of 507.38 to 877.53, with an average reaction time of 679.9. Let’s now get an overview of the relationship between freq and rt.\n\nplot(df_freq$freq, df_freq$rt)\n\n\n\n\nWe see there are a lot of frequency values below roughly 400, and these seem to have higher reaction times than those with a higher frequency value. Let’s fit these data to our first linear model to explore this effect of frequency on reaction times.\n\n\n2.1.2 lm()\nThe the lm() function fits simple linear models. As arguments it takes a formula and a dataset, at minimum.\n\\[\nlm(outcome \\sim 1 + predictor,\\;data\\;=\\;df\\_name)\n\\]\nThe lm() function formula syntax can be read as: outcome predicted by the intercept (1 is a placeholder for the intercept) and predictor. The intercept is included by default, so if you omit the 1 the intercept is still included in the formula. If you wanted to remove the intercept (which you often don’t), you could replace 1 with 0.\n\nRunning a model\nBefore we add our predictor freq, let’s see what our model looks like without it. We can write it as:\n\nlm(rt ~ 1, data = df_freq) \n\nBut it’s useful to save the model as an object so that we can call on it later. It’s often a good idea to have informative prefixes to your objects\n\nfit_rt_1 &lt;- lm(rt ~ 1, data = df_freq) \n\n\n\n\n\n\n\nObject naming\n\n\n\nYou may have wondered what the letters df are for when loading in our data set as df_freq. These letters stand for ‘data frame’, and serve as a reminder of what exactly that object in our environment is. We might also have wanted to plot the frequency data, in which case we could call save the plot as fig_freq or plot_freq. Here we are saving our model as fit_rt_1, using ‘fit’ to signal that this object is a model fit. You could also save it as mod_freq_1, lm_freq_1, or whatever you see fit. This simply helps keep our environment structured, which will become useful when you begin working with multiple datasets at a time.\n\n\n\n\nModel ouput\nNow that we’ve saved our model in our Enrivonement, we can call it by name. Printing just the model gives us the formula and the coefficients.\n\n# print model\nfit_rt_1\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nCoefficients:\n(Intercept)  \n      679.9  \n\n\nRecall that the intercept and slope are called coefficients. Why do we only see Intercept? Because we didn’t include any predictors in our model. This output isn’t very dense, however. We typically use the summary() function to print full model outputs.\n\nsummary(fit_rt_1)\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-172.537  -74.677   -9.137   91.296  197.613 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 117.8 on 11 degrees of freedom\n\n\nWe see a lot more information here.\n\n\n\n\n\n\nbroom package\n\n\n\nThe broom package has some useful functions for printing model outputs\n\ntidy() produces a tibble (type of dataframe) of the coefficients\nglance() produces goodness of fit measures (which we won’t discuss)\n\nThe outputs from tidy() and glance() can be fed into kable and/or kable_styling() to create formatted tables\n\ntidy(fit_rt_1)\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     680.      34.0      20.0 5.38e-10\n\n\n\nglance(fit_rt_1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0             0  118.        NA      NA    NA  -73.7  151.  152.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\naugment() adds model values as columns to your dataframe (e.g., useful for plotting observed vs. fitted values).\n\naugment(fit_rt_1, data = df_freq) %&gt;% summary()"
  },
  {
    "objectID": "02-simple_linear_regression.html#interpreting-model-output",
    "href": "02-simple_linear_regression.html#interpreting-model-output",
    "title": "2  Simple linear regression",
    "section": "2.2 Interpreting model output",
    "text": "2.2 Interpreting model output\n\nlet’s take a closer look at our model summary\n\n\nsummary(fit_rt_1)\n\n\nCall:\n1lm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n2-172.537  -74.677   -9.137   91.296  197.613\n\nCoefficients:\n3            Estimate Std. Error t value       Pr(&gt;|t|)\n4(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\n5Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n6Residual standard error: 117.8 on 11 degrees of freedom\n\n\n1\n\nformula repetition\n\n2\n\nresiduals: differences between observed values and those predicted by the model\n\n3\n\nnames for columns Estimates, standard error, t-value, p-value (Pr(&gt;|t|))\n\n4\n\nIntercept (\\(b_0\\))\n\n5\n\nSignificance codes\n\n6\n\nR\\(^2\\), a measure of model fit (squared residuals); percentage of variance in the data shared with the predictor (higher numbers are better…this is pretty low)\n\n\n\n\n\nIntercept\nOur intercept is roughly 679.9 milliseconds; what does this number represent?\n\n# print model intercept?\ncoef(fit_rt_1)['(Intercept)']\n\n(Intercept) \n   679.9167 \n\n\n\n# print data mean\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nThe intercept corresponds to the mean reaction time value. Let’s explore this.\n\nIntercept significance\nIn the model output, the intercept seems to be significant (indicated with a low p-value, and ***). What does this mean? Significance pretty much tells us if a number is equal to (or not statistically significantly different from) 0. So this tells us that the intercept (i.e., the mean reaction time) is different from 0. How do we interpret this? In most cases we don’t. Whether or not the intercept is significantly different from 0 this isn’t interesting or even theoretically relevant, as reaction times shouldn’t be near 0, so neither should their mean. This is also true for formant frequencies, reading times, and other types of continuous linguistic data.\n\n\n\nStandard Error\nStandard error takes both the variability in our data and the sample size into account. The equation for standard error is:\n\\[\nSE = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\n\\tag{2.1}\\]\nwhere \\(\\sigma\\) is the standard deviation, and \\(n\\) is the sample size. As a refresher, the equation for standard deviation (Equation 2.2) is the square root of the sum of all squared deviances from the mean (\\(\\sum^n_{i=1}(x_i - \\hat{\\mu})^2\\)) divided by the sample size -1. Don’t stress about the math for now, but it’s helpful to try to understand where there values come from and what they represent.\n\\[\n\\hat{\\sigma} = \\sqrt{\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu})^2}{n-1}}\n\\tag{2.2}\\]\n\n\nt-values\nSimple linear regression is equivalent to a t-test. The one-sample t-test corresponds to an intercept-only.\n\ndf_freq %&gt;% \nt.test(rt ~ 1, data = .)\n\n\n    One Sample t-test\n\ndata:  rt\nt = 19.988, df = 11, p-value = 0.000000000538\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 605.0461 754.7872\nsample estimates:\nmean of x \n 679.9167 \n\n\n\ndf_freq %&gt;% \nlm(rt ~ 1, data = .) %&gt;% \n  tidy() %&gt;%\n  mutate_if(is.numeric, round, 10)\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)     680.      34.0      20.0 0.0000000005\n\n\nThe real power of linear regression is coming tomorrow and in January…multiple regression and mixed models. But for now, it’s important to remember that the larger the t-value, the smaller the p-value. But more important is to not rely too heavily on p-values, as such black-and-white classifications have proven a poor substitute for understanding our data and our models.\n\n\np-values\n\n\n\n\n\n\nA word on t-values and p-values\n\n\n\nt-values quantify the difference between population means.\np-values quantify the probability of obtaining a result equal to or greater than what was observed, given the assumption of no effect (the null hypothesis).\nIf the null hypothesis were true, we would expect no effect (a flat line). If we have a lot of evidence/are confidence that there is an effect (the line (slope) is in fact not flat), then it would be unlikely that we would find such a result under the assumption that there is no effect (the line actually is flat) i.e., the null hypothesis. This is reflected in a small p-value.\n\n\n\n\nPlotting rt ~ 1\n\nFigure 2.1 shows the intercept (red dot) amongst the observed data (black dots)\n\nalong the x-axis we have abstract numerical units (the values don’t mean anything)\nwhat would the values of the intercept be?\n\n\n\n\n\n\n\nFigure 2.1: Visualisation of ‘rt ~ 1’: observed values (black) and mean (intercept; red). Residuals would be the distance from each black dot to the y-value of the read dot"
  },
  {
    "objectID": "02-simple_linear_regression.html#adding-a-fixed-effect-slope",
    "href": "02-simple_linear_regression.html#adding-a-fixed-effect-slope",
    "title": "2  Simple linear regression",
    "section": "2.3 Adding a fixed effect (slope)",
    "text": "2.3 Adding a fixed effect (slope)\nNow let’s include a predictor, which will give us a slope. The slope represents the change in \\(y\\) (DV: rt) when we move 1-unit along \\(y\\) (IV: freq). In other words, it tells us the effect our IV has on the DV. Let’s first plot the data:\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = freq, y = rt) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n2.3.1 Fit model (treatment contrasts)\n\n# fit simple linear model\nfit_rt_freq &lt;- lm(rt ~ freq, data = df_freq)\n\n\nModel summary\n\nsummary(fit_rt_freq)\n\n\nCall:\nlm(formula = rt ~ freq, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-155.947  -73.141    2.117   85.050  163.837 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 713.706298  34.639105   20.60 0.0000000016 ***\nfreq         -0.003382   0.001699   -1.99       0.0746 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 104.6 on 10 degrees of freedom\nMultiple R-squared:  0.2838,    Adjusted R-squared:  0.2121 \nF-statistic: 3.962 on 1 and 10 DF,  p-value: 0.07457\n\n\n\n\nIntercept\nThe intercept in our last model was the mean reaction time. But now it’s a different value.\n\n# print model intercept\ncoef(fit_rt_freq)['(Intercept)']\n\n(Intercept) \n   713.7063 \n\n\n\n# print data mean\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nOur intercept is no longer the grand mean of first-pass reading times…what is it?\n\n\nSlope\nOur slope was our slope -0.0033823. What does this correspond to?\n\n# print slope\ncoef(fit_rt_freq)['freq']\n\n        freq \n-0.003382289 \n\n\nThis is the change in \\(y\\) (our DV rt) for a 1-unit change in \\(x\\) (our IV: freq). So when we move up 1 unit in frequency, reaction times decrease by -0.0033823. Whether or not it makes sense to consider this number depends on the measurement unit your data is in, e.g., a unit change from one millimeter or one meter will have a drastically different slope value (say, for age), but the actual slope will be the exact same.\n\nheights_m &lt;- c(1.71, 1.56, .9, 2.06, 1.63)\nheights_cm &lt;- c(171, 156, 90, 206, 163)\nheights_mm &lt;- c(1710, 1560, 900, 2060, 1630)\nyear &lt;- c(22,15,10,26,18)\nmonths &lt;- c(22,15,10,26,18)*12\ndays &lt;- c(22,15,10,26,18)*365\n\ndf_heights_age &lt;- cbind(year, months, days, heights_mm, heights_cm, heights_m) |&gt; as.data.frame() |&gt; \n  pivot_longer(\n    cols = c(heights_mm, heights_cm, heights_m),\n    names_to = \"unit\",\n    values_to = \"height\"\n  ) |&gt; \n  pivot_longer(\n    cols = c(year, months, days),\n    names_to = \"unit_age\",\n    values_to = \"age\"\n  )\n\n\n\n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ days)\n\n\nCall:\nlm(formula = heights_cm ~ days)\n\nCoefficients:\n(Intercept)         days  \n   39.66230      0.01769  \n\nlm(heights_m ~ months)\n\n\nCall:\nlm(formula = heights_m ~ months)\n\nCoefficients:\n(Intercept)       months  \n   0.396623     0.005382  \n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ year)\n\n\nCall:\nlm(formula = heights_cm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     39.662        6.458  \n\nlm(heights_m ~ year)\n\n\nCall:\nlm(formula = heights_m ~ year)\n\nCoefficients:\n(Intercept)         year  \n    0.39662      0.06458  \n\n\n\nggplot(data = df_heights_age) +\n  aes(x = height, y = age) +\n  facet_wrap(unit ~ unit_age, scales = \"free\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw()"
  },
  {
    "objectID": "02-simple_linear_regression.html#model-assumptions",
    "href": "02-simple_linear_regression.html#model-assumptions",
    "title": "2  Simple linear regression",
    "section": "2.4 Model assumptions",
    "text": "2.4 Model assumptions\nNow that we’ve fit a model and understand the output, it’s time to think about whether this model is a good fit for our data. We first have to understand some assumptions that need to met in regression modelling. Importantly, these assumptions reate to the residuals of our model, not the raw data points themselves. The two assumptions we’ll focus on for now are the assumptions of normality of the residuals, and the constant variance of the residuals. Both assumptions are often diagnosed visually, so it takes some practice to learn what looks right.\n\n2.4.1 Normality\nWhen a model satisfies the normalit assumption, its residuals (i.e., the difference between the fitted and observed values) will be approximately normally distributed. Normality is typically visualised using a histogram (Figure 2.2 A) and/or a quantile-quantile (Q-Q) plot (Figure 2.2 B).\n\n\n\n\n\nFigure 2.2: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWinter (2019)’s description of how QQ plots are generated (p. 110):\nTo create this plot, every residual is transformed into a percentile (or quantile) […] The question the Q-Q plot answers is: what is the corresponding numerical value of the 13.8th percentile on the normal distribution? If the values are the same, they will fit on a straight line, which indicates that the two distributions (the distribution of the residuals and the theoretical normal distribution) are very similar.\n\n\n\n\n2.4.2 Constant variance\nWhen a model satisfies the constant variance assumption (also called homoscedasticity, or the absence of heteroscedasticity), the spread of residuals will be equal across the regression line. This is typically visualised using a residual plot, which should look like a blob (Figure 2.2 C).\n\n\n2.4.3 Visualising model assumptions\nLet’s plot our residuals to assess whether our model satisfies the assumptions of normality and constant variance.\n\nHistogram\nWe can do this how it’s done in Winter (2019) (in Ch. 6, p. 110-111), by first extracting the residuals from the model and then fitting them them using the base R function hist().\n\n# extract residuals\nres &lt;- residuals(fit_rt_freq)\n\n\n# plot histogram\nhist(res)\n\n\n\n\nOr, we can use the augment() function from broom to append model values to our original data frame, and then feed this into ggplot() from ggplot2 (or even feed it into hist()).\n\n# or, add to df\ndf_freq &lt;- broom::augment(fit_rt_freq, df_freq)\n\n\n# and create ggplot\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 8, fill = \"grey\", colour = \"black\") +\n  theme_bw()\n\n\n\n\n\n\nQ-Q plot\nAgain, we can do it Bodo’s way:\n\nqqnorm(res)\nqqline(res)\n\n\n\n\nOr using augment() and ggplot().\n\ndf_freq |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() \n\n\n\n\n\n\nResidual plot\nBodo’s way:\n\nplot(fitted(fit_rt_freq), res)\n\n\n\n\nOr with ggplot:\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n\n2.4.4 performance package\nI like to use the performance package to visualise model fit (Lüdecke et al., 2021).\n\nperformance::check_normality(fit_rt_freq)\n\nOK: residuals appear as normally distributed (p = 0.702).\n\n\n\nperformance::check_heteroscedasticity(fit_rt_freq)\n\nOK: Error variance appears to be homoscedastic (p = 0.980).\n\n\n\nperformance::check_model(fit_rt_freq)\n\n\n\n\n\nCoefficients table with summary()\n\n\n&gt; summary(fit_rt_freq)\n\nCall:\n1lm(formula = rt ~ lifetime, data = df_freq, subset = rt &gt; 0)\n\n2Residuals:\n    Min      1Q  Median      3Q     Max \n-228.99 -109.29  -26.99   58.86  777.71 \n\nCoefficients:\n3             Estimate Std. Error t value Pr(&gt;|t|)\n4(Intercept)  309.142      6.259  49.394 &lt;0.0000000000000002 ***\n5lifetime1     31.701     12.517   2.533              0.0116 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 57.46 on 541 degrees of freedom\n6Multiple R-squared:  0.01172,   Adjusted R-squared:  0.00989\nF-statistic: 6.414 on 1 and 541 DF,  p-value: 0.0116\n\n\n1\n\nformula\n\n2\n\nResiduals: differences between observed values and those predicted by the model\n\n3\n\nNames for columns Estimates, SE, t-value, p-value\n\n4\n\nIntercept (\\(b_0\\)), i.e., value of \\(y\\) (first-pass) with a move of one unit of \\(x\\) (lifetime)\n\n5\n\nSlope (\\(b_1\\)), i.e., change in first fixation going from dead to living\n\n6\n\nOutput from an ANOVA\n\n\n\n\n\n\n\nwhat is the intercept?\nis the slope positive or negative?\n\nwhat is it’s value?\n\nthis is what the slope would look like:\n\n\n\n\n\nExploring the model\n\n# how many observed values did we enter into the model?\ndf_freq |&gt; \n  nrow()\n\n[1] 12\n\n\n\n# how many observed values did we enter into the model?\nlength(fitted(fit_rt_freq))\n\n[1] 12\n\n\n\n\nExploring the model: residuals\n\n# what do our FITTED values look like?\nhead(fitted(fit_rt_freq))\n\n       1        2        3        4        5        6 \n525.9148 576.2873 663.3271 700.2042 700.6845 712.3229 \n\n\n\n# what do our OBSERVED values look like?\nhead(df_freq$rt)\n\n[1] 621.77 519.56 507.38 636.56 587.18 705.00\n\n\n\n# what is the difference between the FITTED and OBSERVED values?\nhead(df_freq$rt) - head(fitted(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942 \n\n\n\n# what are our RESIDUALS?\nhead(residuals(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942 \n\n\n\n\nExploring the model\n\nwhat were our coefficients?\n\n\ncoef(fit_rt_freq)\n\n  (Intercept)          freq \n713.706297951  -0.003382289 \n\n\n\nwhat would be our predicted reaction time for a word with frequency of 0?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 0\n\n(Intercept) \n   713.7063 \n\n\n\nignore the (Intercept) label here, R just takes the first label when performing an operation on 2 vectors\nwhat is the mean of our predictor coded as +0.5?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 5000\n\n(Intercept) \n   696.7949 \n\n\n\n\n\n\n\n\nImage source: Winter (2019) (all rights reserved)"
  },
  {
    "objectID": "02-simple_linear_regression.html#summary",
    "href": "02-simple_linear_regression.html#summary",
    "title": "2  Simple linear regression",
    "section": "2.5 Summary",
    "text": "2.5 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor"
  },
  {
    "objectID": "02-simple_linear_regression.html#important-terms",
    "href": "02-simple_linear_regression.html#important-terms",
    "title": "2  Simple linear regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Simple linear regression\nNA\n`lm(response ~ predictor, data = data)`"
  },
  {
    "objectID": "02-simple_linear_regression.html#learning-objectives-1",
    "href": "02-simple_linear_regression.html#learning-objectives-1",
    "title": "2  Simple linear regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…"
  },
  {
    "objectID": "02-simple_linear_regression.html#task",
    "href": "02-simple_linear_regression.html#task",
    "title": "2  Simple linear regression",
    "section": "2.6 Task",
    "text": "2.6 Task\nNow it’s your turn. Try to run the following lm() models:\n\ntotal reading time at the verb region\ntotal reading time at the verb+1 region."
  },
  {
    "objectID": "02-simple_linear_regression.html#literaturverzeichnis",
    "href": "02-simple_linear_regression.html#literaturverzeichnis",
    "title": "2  Simple linear regression",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nLüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "03-continuous_predictors.html#learning-objectives",
    "href": "03-continuous_predictors.html#learning-objectives",
    "title": "3  Continuous predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nwhy and how to centre continuous predictors\nwhen and how to standardize continuous predictors\nwhy and how to log-transform continuous variables"
  },
  {
    "objectID": "03-continuous_predictors.html#set-up-environment",
    "href": "03-continuous_predictors.html#set-up-environment",
    "title": "3  Continuous predictors",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad data\n\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\n\nReminder of our variables:\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5"
  },
  {
    "objectID": "03-continuous_predictors.html#summary",
    "href": "03-continuous_predictors.html#summary",
    "title": "3  Continuous predictors",
    "section": "Summary",
    "text": "Summary\nIn the last lectures we saw that the equation for a straight line boils down to its intercept and slope, and that linear regression fits a line to our data. This line results in predicted/fitted values, which fall along the line, and residuals, which are the difference between our observed values and the fitted values.\nWe also learned about two model assumptions: normality of residuals, and constant variance of residuals. We learned that we can plot these with histograms or Q-Q plots (normality), and residual plots (constant variance).\nNow that we understand what a simple linear does, we can take a step back and focus on what we put into the model. So far we’ve looked at reaction times (milliseconds) as a function of word frequency. However, we don’t typically feed raw continuous data into a model, because most continuous linguistic variables are not normally distributed, and so a straight line will not fit it very well (because there will be some large variance for higher values)."
  },
  {
    "objectID": "03-continuous_predictors.html#linear-transformations",
    "href": "03-continuous_predictors.html#linear-transformations",
    "title": "3  Continuous predictors",
    "section": "3.1 Linear transformations",
    "text": "3.1 Linear transformations\nLinear transformations refer to constant changes across values that do not alter the relationship between these values. For example, adding, subtracting, or multiplying by a constant value will not alter the difference between values. Think of the example in the last lecture on the relationship between heights and ages as a function of the measurement unit: the relationship between all the values did not alter, because the difference between heights millimeters, centimeters, and meters is constant, as is the difference between ages in days, months, or years. We’ll now look at some common ways of linearly transforming our data, and the reasons behind doing so.\n\n3.1.1 Centering\nCentering is typically applied to predictor variables. Centering refers to subtracting the mean of a variable from each value, resulting in each centered value representing the original value’s deviance from the mean (i.e., a mean-deviation score). What would a centered value of \\(0\\) represent in terms of the original values?\nLet’s try centering our frequency values. To create a new variable (or alter an existing variable), we can use the mutate() function from dplyr.\n\n# add centered variable\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_c = freq-mean(freq))\n\nThis can also be done with base R, but it’s a lot more verbose.\n\n# add centered variable with base R\ndf_freq$freq_c &lt;- df_freq$freq-mean(df_freq$freq)\n\nNow let’s fit our models.\n\n# run our model with the original predictor\nfit_rt_freq &lt;- \n  lm(rt ~ freq, data = df_freq)\n\n\n# run our model with the centered predictor\nfit_rt_freq_c &lt;- \n  lm(rt ~ freq_c, data = df_freq)\n\nIf we compare the coefficients from fit_rt_freq and fit_rt_freq_c, what do we see? The only difference is the intercept values: 713.706298 (uncentered) and 679.9166667 (centered).\n\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nThe intercept for a centered continuous predictor variable corresponds to the mean of a continuous response variable. This is crucial in interpreting interaction effects, which we will discuss tomorrow. For more detail on interpreting interactions, see Chapter 8 in Winter (2019) (we won’t be discussing this chapter as a whole).\n\n\n\n\n\n\nCentering interval data\n\n\n\nIf you have interval data with a specific upper and lower bound, you could alternatively subtract the median value. In linguistic research, this is most typically rating scale data. For example, if you have a dataset consisting of ratings from 1-7, you can centre these ratings by subtracting 4 from all responses. A centred response of -3 would correspond to the lowest rating (1), and of +3 to the highest rating (7), which 0 would correspond to a medial rating (4). This can also be helpful in plotting, as there is no question as to whether 1 or 7 was high or low, because all ratings are now centred around 0 (and negative numbers correspond to our intuition of low-ratings).\n\n\n\n\n3.1.2 Standardizing (z-scoring)\nWe can also standardize continuous predictors by dividing centered values by the standard deviation of the sample. Let’s look at our frequency/reaction time data again.\nFirst, what are our mean and standard deviation? This will help us understand the changes to our variables as we center and stardardize them.\n\nmean(df_freq$freq)\n\n[1] 9990.167\n\n\n\nsd(df_freq$freq)\n\n[1] 18558.69\n\n\nWhat are the first six values of freq in the original scale?\n\ndf_freq$freq[1:6]\n\n[1] 55522 40629 14895  3992  3850   409\n\n\nWhat are the first six values of freq_c in the centered scale? These should be the values of freq minus the mean of freq (which we saw above is 9990.1666667).\n\ndf_freq$freq_c[1:6]\n\n[1] 45531.833 30638.833  4904.833 -5998.167 -6140.167 -9581.167\n\n\nNow, let’s create our standardised z-scores for frequency by dividing these centered values by the standard deviation of freq (which will be the same as the standard deviation of freq_c), and which we saw is 18558.6881679. Again, this can be done with mutate() from dplyr, or by using base R syntax.\n\n# standardise using the tidyverse\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_z = freq_c/sd(freq))\n\n\n# standardize with base R\ndf_freq$freq_z &lt;- df_freq$freq_c/sd(df_freq$freq)\n\n\nhead(df_freq)\n\n# A tibble: 6 × 5\n  word      freq    rt freq_c freq_z\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 thing    55522  622. 45532.  2.45 \n2 life     40629  520. 30639.  1.65 \n3 door     14895  507.  4905.  0.264\n4 angel     3992  637. -5998. -0.323\n5 beer      3850  587. -6140. -0.331\n6 disgrace   409  705  -9581. -0.516\n\n\n\n\n\n\n\n\nCorrelation"
  },
  {
    "objectID": "03-continuous_predictors.html#non-linear-transformations",
    "href": "03-continuous_predictors.html#non-linear-transformations",
    "title": "3  Continuous predictors",
    "section": "3.2 Non-linear transformations",
    "text": "3.2 Non-linear transformations\nThis is really the meat and potates of dealing with continuous variables (depending on your subfield). In linguistic research, and especially experimental research, we often deal with continuous variables truncated/bound at 0. Reaction times, reading times and formant frequencies are all examples of such types of data: there is no such thing as a negative reading time or fundamental frequency. The problem with these types of data is that they are almost never normally distributed, which has implications for the normality of residuals for any line that tries to fit to these data. Very often, this type of data will have a ‘positive skew’, or a long tail off to the right (assuming larger values are plotting to the right). This shape is not symmetrical, meaning that the residuals tend to be much larger for larger values. It is also often the case that these very large, exceptional values will have a stronger influence on the line of best fit, leading to the coefficient estimates that are “suboptimal for the majority of data points” [@Baayen (2008); p. 92]. How do we deal with this nonnormality? We use non-linear transformations, the most common of which is the log-transformation.\n\n3.2.1 Log-transformation\nLet’s look at our reaction time data again. We’ll log-transform our reaction time data and frequency data. Note that in Winter (2019), frequency is transformed using log to the base 10 for interpretability, but we’ll stick to the natural logarithm.\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = log(freq)) +\n  geom_density()\n\n\n\n\n\ndf_freq &lt;-\n  df_freq |&gt; \n    mutate(rt_log = log(rt),\n           freq_log = log(freq))\n\n\nlm(rt_log ~ freq_log, data = df_freq) |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 freq_log     -0.0453   0.00871     -5.20 4.03e- 4\n\n\n\n# or, log-transform directly in the model syntax\nlm(log(rt) ~ log(freq), data = df_freq) |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 log(freq)    -0.0453   0.00871     -5.20 4.03e- 4"
  },
  {
    "objectID": "03-continuous_predictors.html#learning-objectives-1",
    "href": "03-continuous_predictors.html#learning-objectives-1",
    "title": "3  Continuous predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…\n\nwhy and how to centre continuous predictors\nwhen and how to standardize continuous predictors\nwhy and how to log-transform continuous variables"
  },
  {
    "objectID": "03-continuous_predictors.html#important-terms",
    "href": "03-continuous_predictors.html#important-terms",
    "title": "3  Continuous predictors",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Centering\ntype of linear transformation\n`dplyr::mutate(variable = variable - mean(variable))`"
  },
  {
    "objectID": "03-continuous_predictors.html#take-home-messages",
    "href": "03-continuous_predictors.html#take-home-messages",
    "title": "3  Continuous predictors",
    "section": "Take-home messages",
    "text": "Take-home messages\nContinuous data are often transformed before fitting a model to this data. Linear transformations, like adding or multiplying all values by a single value, are often performed on continuous predictors by means of centring and standardizing (when there are multiple continuous predictors). Non-linear transformations are often performed on continuous data with a positive skew (a few values much larger than the majority) in order to satisfy the normality assumption. Although the normality assumption refers to the normality of residuals, the distribution of the data will have implications for the distribution of the residuals. The most common non-linear transformation is the log-transformation (the inverse of the exponential), which shrinks values, especially making big numbers smaller. This has the result of squeezing big numbers towards smaller numbers, reducing the spread in the distribution (e.g., the log of 3 is 1.0986123, the log of 30 is 3.4011974, and the log of 30 is 5.7037825).\nWhat to do with this information? If you have continuous data truncated at 0 (with no upperbound, e.g., reaction time data or fundamental frequency), visualise the data (histogram and Q-Q plot) in order to check its distribution. If it is not normally distributed, you will likely want to log-transform it. Is this data your response variable? Then that is all you will likely want to do. Is this data a predictor variable? Then you will want to centre it (subtract the mean of this variable from all values). Do you have more than one continuous predictor variable? Then standardizing these variables will facilitate the interpretation of interaction effects (we’ll talk about these soon)."
  },
  {
    "objectID": "03-continuous_predictors.html#task",
    "href": "03-continuous_predictors.html#task",
    "title": "3  Continuous predictors",
    "section": "3.3 Task",
    "text": "3.3 Task"
  },
  {
    "objectID": "03-continuous_predictors.html#literaturverzeichnis",
    "href": "03-continuous_predictors.html#literaturverzeichnis",
    "title": "3  Continuous predictors",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nBaayen, R. H. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics using R.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "04-multiple_regression.html#summary",
    "href": "04-multiple_regression.html#summary",
    "title": "4  Multiple Regression",
    "section": "Summary",
    "text": "Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor"
  },
  {
    "objectID": "04-multiple_regression.html#learning-objectives",
    "href": "04-multiple_regression.html#learning-objectives",
    "title": "4  Multiple Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nwhat multiple regression is\nhow to include multiple predictor variables\nhow to interpret slopes in multiple regression\nhow to interpret interaction effects\nabout the assumption of the absence of collinearity"
  },
  {
    "objectID": "04-multiple_regression.html#set-up-environment",
    "href": "04-multiple_regression.html#set-up-environment",
    "title": "4  Multiple Regression",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad data\nWe’ll use the full dataset of the frequency data.\n\ndf_freq_full &lt;-\n  read_csv(here(\"data\", \"ELP_full_length_frequency.csv\")) |&gt; \n  clean_names() |&gt; \n  mutate(freq = 10^(log10freq), # inverse log10\n         freq_log = log(freq)) |&gt;  # use natural logarithm\n  relocate(word, rt, length, freq, freq_log)\n\nWe have 4 variables:\n\nword\nlength\nrt\nfreq\nfreq_log\nlog10freq"
  },
  {
    "objectID": "04-multiple_regression.html#multiple-regression",
    "href": "04-multiple_regression.html#multiple-regression",
    "title": "4  Multiple Regression",
    "section": "4.1 Multiple regression",
    "text": "4.1 Multiple regression\nSo far we’ve worked with simple linear models, which fit a model to a predictor and response variable. These models do not differ so greatly from a one- or two-sample t-test (for a categorical predictor) or Pearson’s r (for a standardised continuous predictor). You might be wondering then we would bother with linear regression. One reason is that it allows us to include multiple predictors in our models, which still boils down to modeling the mean, but while condintioning the mean on multiple variables at once.\nRecall the equation of a line (Equation 4.1), which states that any value of \\(y\\) equals the intercept (\\(b_0\\)) plus the corresponding value of \\(x\\) multiplied by the slope (\\(b_1x\\)), plus the error, which are our residuals (\\(e\\)). In multiple regression, we can include more than one slope (Equation 4.2).\n\\[\ny = b_0 + b_1x + e\n\\tag{4.1}\\]\n\\[\ny = b_0 + b_1x + b_2x + ... + e\n\\tag{4.2}\\]\n\n\n\n\n\nflowchart LR\n  A[Continuous variable] \n  A --&gt; F[Zero-truncated with positive skew \\n e.g., reaction times]\n  A --&gt; H[Interval i.e., lower and upperbound \\n e.g., rating scale]\n  H --&gt; I[Centre on median value]\n  I --&gt; E(Response)\n  E --&gt; Z[Done]\n  F --&gt; G[Non-linear transformation \\n e.g., log-transform]\n  G --&gt; B(Predictor)\n  I --&gt; B(Predictor)\n  B --&gt; C{One predictor}\n  C --&gt; X[Centre]\n  D --&gt; Y[Centre and standardise]\n  B --&gt; D{Two predictor}\n  G --&gt; E(Response)\n  \n  \n\n\nFigure 4.1: Flowchart of common steps for linear and non-linear transformations of continuous variables. Such decision trees are not a one-size-fits-all solution and cannot replace critical thinking and understanding of your data.\n\n\n\n\n\n4.1.1 One predictor\nLet’s re-run our simple model with this dataset. Let’s keep reaction times in the raw milliseconds for now for interpretability.\n\nfit_freq_full &lt;-\n  lm(rt ~ log(freq), data = df_freq_full)\n\n\ntidy(fit_freq_full)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    907.      1.09       828.       0\n2 log(freq)      -37.5     0.262     -143.       0\n\n\nWe see there is a decrease in reaction times (-37.5 milliseconds) for a 1-unit increase in log frequency. Let’s look at the model fit using glance().\n\nglance(fit_freq_full)$r.squared\n\n[1] 0.3834186\n\n\nWe see that the R-squared is 0.383, meaning our model describes 38% of the variance in response times. We can’t be sure that this described variance is due solely to frequency, however. Our models only know what we tell them! Other effects that are correlated with frequency might be conflating the frequency effect, e.g., more frequent words tend to be shorter (zipf_1949?). Let’s expand our model to include word length Equation 4.3.\n\\[\ny = b_0 + b_1*log frequency + b_2*word length\n\\tag{4.3}\\]\n\n\n4.1.2 Adding a predictor\nLet’s add length as a predictor to our model.\n\nfit_freq_mult &lt;-\n  lm(rt ~ log(freq) + length, data = df_freq_full)\n\n\ntidy(fit_freq_mult) |&gt; select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    748. \n2 log(freq)      -29.5\n3 length          19.5\n\n\nWe see that length is also a significant predictor of reaction times, with an increase in word length (+1 letter) corresponds to a 20ms increase in reaction times. Our intercept is also now 748ms, instead of 907ms. The 907ms intercept corresponds to the prediction for reaction times to a word with 0 log frequency and 0 word length, but this is not very interpretable. If we were to center both prdictors, the intercept would be the reaction time for a wrd with average frequency and average length.\nThe slope for log frequency has also changed: from -37.5 to -29.5. This change tells us that some of the effect in our first model was confounded with length, as controlling for length weakens the effect of frequency.\n\nglance(fit_freq_mult)$r.squared\n\n[1] 0.4872977\n\n\nWe also see that including length increases the variance described by our model, reflected in the R-squared values (0.4872977 instead of 0.3834186."
  },
  {
    "objectID": "04-multiple_regression.html#standardising-our-predictors",
    "href": "04-multiple_regression.html#standardising-our-predictors",
    "title": "4  Multiple Regression",
    "section": "4.2 Standardising our predictors",
    "text": "4.2 Standardising our predictors\nRecall that, when we have multiple continuous predictors, standardising them can help their interpretation, as their slopes are comparable. We could achieve this by centering each variable and then dividing by the standard deviation, or we could use the scale() function, which does just this.\n\n# centre and then standardize\ndf_freq_full |&gt; \n  mutate(\n         freq_z1 = (freq-mean(freq))/sd(freq),\n         freq_z2 = scale(freq)) |&gt; \n  select(freq_z1, freq_z2) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  freq_z1 freq_z2[,1]\n    &lt;dbl&gt;       &lt;dbl&gt;\n1 -0.0902     -0.0902\n2 -0.0864     -0.0864\n3 -0.0905     -0.0905\n4 -0.0864     -0.0864\n5 -0.0885     -0.0885\n6 -0.0901     -0.0901\n\n\nLet’s use scale() for freq and length.\n\ndf_freq_full &lt;-\n  df_freq_full |&gt; \n  mutate(freq_z = scale(freq_log),\n         length_z = scale(length))\n\n\nfit_freq_z &lt;-\n  lm(rt ~ freq_z + length_z, data = df_freq_full)\n\nFirst, let’s check the \\(R^2\\):\n\nglance(fit_freq_z)$r.squared\n\n[1] 0.4872977\n\n\nWe see that our \\(R^2\\) value is 0.4872977, just like above. This serves as a reminder that the predictors still represent the same variance in the underlying model, their units and scales have simply changed. What about our coefficients:\n\ntidy(fit_freq_z) |&gt; select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    770. \n2 freq_z         -60.6\n3 length_z        43.3\n\n\nHere, a 1-unit change always corresponds to a change of 1 standard deviation. Now we see that frequency has a larger magnitude than the effect of length. So, for each instease in frequency by 1 standard deviation (holiding length constant), reaction times decrease by 29.5 ms.\n\n4.2.1 Adding an interaction term\nWe won’t spent much time talking about interactions, but please check out Ch. 8 (Interations and nonlinear effects) in Winter (2019) for a more in-depth treatment. For now, what’s important to know is that interactions describe how effects of one predictor may be influenced by changes in another predictor. We can add interactin terms of two predictors by connecting them with a colon (:).\n\nlm(rt ~ freq_z + length_z + freq_z:length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nOr, we can simply connect the two predictors with an asterisk (*) to indicate that we want to look at both predictors and their interaction.\n\nlm(rt ~ freq_z*length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nThe model estimates are the same for both models. The intercept is the predicted reaction time for a word with the mean length and mean frequency. Notice that the interaction slope is negative, meaning when both freq and length increase, reaction times will decrease."
  },
  {
    "objectID": "04-multiple_regression.html#model-assumptions",
    "href": "04-multiple_regression.html#model-assumptions",
    "title": "4  Multiple Regression",
    "section": "4.3 Model assumptions",
    "text": "4.3 Model assumptions\nWe’ve already discussed the assumptions of normality and homoscedasticity (constant variance), which both refer to the residuals of a model. We typically assess these assumptions visually, with histogram and Q-Q plots.\n\n4.3.1 Normality and Homoscedasticity\nFor our model\n\nfig_hist &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nThe histogram looks approximately normally distributed, with a bit of a positive skew. The Q-Q plot suggests a less-normal distribution, with the model estimates fitting larger reaction times more poorly. The residual plot also shows that the variance of the residuals is not constant, with much larger residual variance for larger fitted values. This tells us we should probably log reaction times. Let’s try it all again, with log-transformed reaction times.\n\n\n4.3.2 Log-transformed response variable\n\nfit_freq_log_z &lt;-\n  lm(log(rt) ~ freq_z*length_z,\n     data = df_freq_full)\n\n\nglance(fit_freq_log_z)$r.squared\n\n[1] 0.5176913\n\n\n\ntidy(fit_freq_log_z) |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.63   \n2 freq_z          -0.0826 \n3 length_z         0.0524 \n4 freq_z:length_z -0.00779\n\n\nWe see now that our values are much smaller, because they’re on the log-scale.\n\nexp(6.63 + -0.0826*5 + 0.0524*2)\n\n[1] 556.5739\n\nexp(6.63 + -0.0826*4 + 0.0524*2)\n\n[1] 604.499\n\nexp(6.63 + -0.0826*1 + 0.0524*6)\n\n[1] 955.0847\n\ntidy(fit_freq_log_z)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      6.63     0.000636   10428.  0       \n2 freq_z          -0.0826   0.000666    -124.  0       \n3 length_z         0.0524   0.000649      80.7 0       \n4 freq_z:length_z -0.00779  0.000581     -13.4 8.51e-41\n\n\n\nfig_hist &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nLooks much better.\n\n\n4.3.3 Collinearity\nCollinearity refers to when continuous predictor variables are correlated, which can make the interpretation of their coefficients difficult, and the results spurious. Regression assumes there is an absence of collinearity, i.e., our predictor variables are not correlatded.\nTo assess collinearity, you can use the vif() function from the car package to compare variance inflation factors. VIF values close to 1 indicates there is not a high degree of collinearity between your variables.\n\ncar::vif(fit_freq_log_z)\n\n         freq_z        length_z freq_z:length_z \n       1.246509        1.184641        1.068283 \n\n\nCollinearity is a conceptual problem, and is something that you need to consider in the planning stage. Typically, we want to include predictors that we have specific predictions or research questions about. Shoving a bunch of predictors in a model to see what comes out significant is bad practice. Rather, we should have a principled approach to model building and variable selection. This is not to say that exploratory analyses should be avoided, but that this comes with caveats.\n\n\n4.3.4 Adjusted \\(R^2\\)\nAlthough we should avoid throwing any old predictor into our model, adjusted \\(R^2\\) is a more conservative version of \\(R^2\\) that takes into account the number of predictors in a model. For each additional predictor, adjusted \\(R^2\\) includes the number of predictors (\\(k\\)) in its denominator (bottom half of a division), which means that the more predictors there are, the smaller \\(R^2\\) will be, unless each additional predictor explains sufficient variance to counteract this penalisation.\n\nglance(fit_freq_log_z)$adj.r.squared\n\n[1] 0.5176475\n\n\nIf we were to look at the (adjusted) \\(R^2\\) of our simple linear regression model, where log reaction times are predicted by standardised log frequency, we see that there is a large increase in our model which includes length and its interaction. This suggests that our model is not overfit, and that length contributes to the variance explained by the model.\n\nglance(lm(log(rt) ~ freq_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.4148675\n\n\nIf we likewise compare to the same model without an interaction term (log reaction times ~ frequency * length), we see that the adjusted \\(R^2\\) is not very different. If the adjusted \\(R^2\\) were much lower, this would indicate that including the interaction term leads to overfitting.\n\nglance(lm(log(rt) ~ freq_z + length_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.5150461\n\n\n\n\nImportant terms\n\n\n\n\n\n\n  \n    \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    NA\nNA\nNA"
  },
  {
    "objectID": "04-multiple_regression.html#learning-objectives-1",
    "href": "04-multiple_regression.html#learning-objectives-1",
    "title": "4  Multiple Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…\nToday we will learn…\n\nwhat multiple regression is\nhow to include multiple predictor variables\nhow to interpret slopes in multiple regression\nhow to interpret interaction effects\nabout the assumption of the absence of collinearity"
  },
  {
    "objectID": "04-multiple_regression.html#task",
    "href": "04-multiple_regression.html#task",
    "title": "4  Multiple Regression",
    "section": "4.4 Task",
    "text": "4.4 Task\nLoad in the english dataset from the languageR package (Baayen & Shafaei-Bajestan, 2019) (code below). You don’t need to load in any CSV file, because this dataset is available if you have the package loaded. From the manual:\n\nThis data set gives mean visual lexical decision latencies and word naming latencies to 2284 monomorphemic English nouns and verbs, averaged for old and young subjects, with various predictor variables. (languageR manual, p. 29)\n\n\n# load in 'english' dataset from languageR\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  relocate(word)\n\nWe’re keeping five variables:\n\nword: a factor with 2284 words\nrt_lexdec: numeric vector of log RT in visual lexical decision\nrt_naming: numeric vector of log RT in word naming\nlength_in_letters: numeric vector with length of the word in letters\nAgeSubject: a factor with as levels the age group of the subject: young versus old.\nfreq_written: numeric vector with log frequency in the CELEX lexical database\n\nTake the following steps:\n\nPerform an exploratory data analysis to understand the data.\n\n\n4.4.1 Exploratory data analysis\nLet’s start with a summary of the data.\n\n\n      word        rt_lexdec       rt_naming     length_in_letters age_subject \n arm    :   4   Min.   :6.205   Min.   :6.022   Min.   :2.000     old  :2284  \n barge  :   4   1st Qu.:6.426   1st Qu.:6.149   1st Qu.:4.000     young:2284  \n bark   :   4   Median :6.550   Median :6.342   Median :4.000                 \n bear   :   4   Mean   :6.550   Mean   :6.323   Mean   :4.342                 \n beef   :   4   3rd Qu.:6.653   3rd Qu.:6.490   3rd Qu.:5.000                 \n bind   :   4   Max.   :7.188   Max.   :6.696   Max.   :7.000                 \n (Other):4544                                                                 \n  freq_written   \n Min.   : 0.000  \n 1st Qu.: 3.761  \n Median : 4.832  \n Mean   : 5.021  \n 3rd Qu.: 6.247  \n Max.   :11.357  \n                 \n\n\nRemember that word is a factor, i.e., categorical variable. The numbers beside the words indicate how many observations (i.e., rows) there are per word. To see how many words have how many observations, we can use the count() function from the dplyr package, and then pipe to count(n).\n\ndf_freq_eng |&gt; \n  count(word) |&gt; \n  count(n)\n\n  n   nn\n1 2 2110\n2 4   87\n\n\nThis tells us that there are 2110 words that have 2 observations, and 87 that have 4 observations.\nNow let’s look at the distribution of our raw reaction time data (which is already logged, so we feed it into exp()). We’ll produce histograms and density plots of the raw and log reaction times.\n\n\n\n\n\nThis looks like a bimodal distribution, i.e., there are two modes (most frequent value, i.e., peak in a histogram). What might be driving this? We know that there were two subject groups: old and young. How does the distribution of these two groups look?\n\n\n\n\n\n\n\n\n\n\n\ndf_freq_eng |&gt; \n  ggplot() +\n  aes(x = freq_written, y = rt_lexdec, colour = age_subject, shape = age_subject) +\n  geom_point(alpha = .5)\n\n\n\n\n\n\n4.4.2 Model\nNow let’s run our model, ignoring age_subject for now.\n\nfit_freq_eng &lt;-\n  lm(rt_lexdec ~ freq_written, data = df_freq_eng)\n\n\nsummary(fit_freq_eng)\n\n\nCall:\nlm(formula = rt_lexdec ~ freq_written, data = df_freq_eng)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45708 -0.11657 -0.00109  0.10403  0.56085 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   6.735931   0.006067 1110.19 &lt;0.0000000000000002 ***\nfreq_written -0.037010   0.001134  -32.63 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1413 on 4566 degrees of freedom\nMultiple R-squared:  0.1891,    Adjusted R-squared:  0.1889 \nF-statistic:  1065 on 1 and 4566 DF,  p-value: &lt; 0.00000000000000022\n\n\nLet’s check our model fit.\n\nperformance::check_model(fit_freq_eng)\n\n\n\n\n\ntidy(fit_freq_eng)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    6.74     0.00607    1110.  0        \n2 freq_written  -0.0370   0.00113     -32.6 4.46e-210\n\n\n\nglance(fit_freq_eng)$r.squared\n\n[1] 0.1890641"
  },
  {
    "objectID": "04-multiple_regression.html#literaturverzeichnis",
    "href": "04-multiple_regression.html#literaturverzeichnis",
    "title": "4  Multiple Regression",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nBaayen, R. H., & Shafaei-Bajestan, E. (2019). languageR: Analyzing linguistic data: A practical introduction to statistics. https://CRAN.R-project.org/package=languageR\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "05-categorical_predictors.html#learning-objectives",
    "href": "05-categorical_predictors.html#learning-objectives",
    "title": "5  Categorical predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nabout cateogorical predictors\nhow to interpret different contrast coding"
  },
  {
    "objectID": "05-categorical_predictors.html#set-up-environment",
    "href": "05-categorical_predictors.html#set-up-environment",
    "title": "5  Categorical predictors",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe’ll also need to load in our required packages.\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad data\nLet’s continue working with the english dataset from the languageR package. Let’s just call it df_freq_eng.\n\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  # standardize continuous predictors\n  mutate(\n    freq_z = scale(freq_written),\n    length_z = scale(length_in_letters)\n  ) |&gt; \n  relocate(word) |&gt; \n  arrange(word)\n\nIn your exploratory data analysis, you might’ve noticed a bimodal distribution.\n\n\n\n\n\nThis looks like a bimodal distribution, i.e., there are two modes (most frequent value, i.e., peak in a histogram). What might be driving this? We know that there were two subject groups: old and young. How does the distribution of these two groups look?\nRunning our model of the log reaction times as predicted by frequency and length, we see:\n\nfit_freq_length &lt;-\n  lm(rt_lexdec ~ freq_z*length_z,\n     data = df_freq_eng)\n\n\nglance(fit_freq_length)$r.squared\n\n[1] 0.1896649\n\nglance(fit_freq_length)$adj.r.squared\n\n[1] 0.1891323\n\n\nSeems like we don’t have any overfitting in our model (\\(R^2\\) and adjusted \\(R^2\\) are comparable). Let’s look at our coeffiecients.\n\ntidy(fit_freq_length) |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.55   \n2 freq_z          -0.0682 \n3 length_z         0.00328\n4 freq_z:length_z -0.00196\n\n\nThere is a negative slope for frequency, indicating shorter reaction times for words with higher frequency (when holding length constant). There is a positive slope for length, indicating longer reaction times for longer words (holding frequency constant). There is also a negative interaction estimate, indicating that when both length and frequency increase, reaction times decrease. This seems similar to the dataset we explored in the previous sections. But, this bimodal distribution is suggesting we should include age group as a predictor, since the two groups seem to pattern differently in their reading times. Could it be that the effect of frequency and length also differ as a function of age group?"
  },
  {
    "objectID": "05-categorical_predictors.html#categorical-predictors",
    "href": "05-categorical_predictors.html#categorical-predictors",
    "title": "5  Categorical predictors",
    "section": "5.1 Categorical predictors",
    "text": "5.1 Categorical predictors\nIn linguistic research we often want to compare the effect of groups or categories, such as native or non-native speakers, or grammatical or ungrammatical stimuli. We might expect longer reading times for non-native (compared to native) speakers of a language, or for ungrammatical (versus grammatical) sentences. With our current dataset, we’d predict longer reading times for older participants than younger participants (although we should hypothesise before collecting and visualising our data!). How might these age effects interact with effects of word frequency and length?\n\n5.1.1 Including a categorical predictor\nWhat would happen if we just include age_subject in our model?\n\nfit_age &lt;-\n  lm(rt_lexdec ~ freq_z*length_z + age_subject,\n     data = df_freq_eng)\n\nFirst, we see that adding age to our model results in a large increase in variance explained, and that the \\(R^2\\) and adjusted \\(R^2\\) values are comparable. In addition, the VIF values for all coefficients are near 1. This indicates that our predictors all contribute to the variance explained by the model and are not correlated.\n\nglance(fit_age)$r.squared\n\n[1] 0.6888949\n\nglance(fit_age)$adj.r.squared\n\n[1] 0.6886222\n\n\n\ncar::vif(fit_age)\n\n         freq_z        length_z     age_subject freq_z:length_z \n       1.012553        1.004461        1.000000        1.008108 \n\n\nNow that we see that our model is not overfit and that our predictors are not correlatd, let’s take a look at our model estimates.\n\ntidy(fit_age) |&gt; select(term,estimate)\n\n# A tibble: 5 × 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)       6.66   \n2 freq_z           -0.0682 \n3 length_z          0.00328\n4 age_subjectyoung -0.222  \n5 freq_z:length_z  -0.00196\n\n\nIn addition to the effects we observed in our earlier model, we see that there is a negative slope for age_subjectyoung, indicating that reaction times decrease when…what? How do we interpret a slope for a categorical variable? Regression works with numerical values, so how does a categorical variable get fit to a line? If we feed a categorical variable into the lm() function, the factor levels (i.e., the categories in a categorical variable) are given numerical values. We need to know what these values are in order to know how to interpret our model estimates. We call these numerical values mapped onto factor levels contrast coding, and we can check the contrasts of a given factor using the function contrasts().\n\ncontrasts(df_freq_eng$age_subject)\n\n      young\nold       0\nyoung     1\n\n\nWe see that old was coded at \\(0\\) and young as \\(1\\). This means that our slope for age_subjectyoung represents the change in reaction times when we move from old to young, which corresponds to a 1-unit change in our predictor (because the difference between 0 and 1 is 1). This is called treatment coding, or dummy coding, where one factor level is coded as 0 and the other as 1. Let’s remove the continuous variable for now and focus on age_subject. Let’s also look at raw reaction times, to more easily interpret the results.\n\nfit_age &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\n\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\n\n\nOur \\(R^2\\) value is lower than when we included frequency and length, but higher still than our model with frequeny and length but no age.\n\ntidy(fit_age) |&gt; select(term, estimate)\n\n# A tibble: 2 × 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)          787.\n2 age_subjectyoung    -157.\n\n\nWe see that there is an estimated decrease in reaction times of 157ms for the young group compared to the old group. But what does the intercept represent here? Let’s look at our data again.\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summary()\n\n   rt_lexdec      age_subject \n Min.   : 495.4   old  :2284  \n 1st Qu.: 617.4   young:2284  \n Median : 699.6               \n Mean   : 708.1               \n 3rd Qu.: 775.3               \n Max.   :1323.2               \n\n\nAnd how does rt_lexdec differ between the groups?\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summarise(mean = mean(rt_lexdec),\n            min = min(rt_lexdec),\n            max = max(rt_lexdec),\n    .by = \"age_subject\"\n  )\n\n  age_subject     mean    min    max\n1       young 629.5473 495.38  971.8\n2         old 786.7200 603.77 1323.2\n\n\nWe see here that the intercept for our model actually corresponds to the mean reaction time for the old group. Why is this? Recall that the intercept corresponds to the \\(y\\) value (reaction time) when \\(x\\) is \\(0\\). In treatment/dummy coding, one factor level is coded as \\(0\\). In our case this was old, and so the intercept corresponds to the mean reaction time for participants in the old group. How does R choose which variable to code as \\(0\\)? It simply takes the first level name alphabetically: old comes before young, so old was automatically taken as the ‘baseline’ to which young was compared.\nAnd if we were to add the slope to the intercept, we would get the mean for the \\(young\\) group. Why is this?\n\ncoef(fit_age)['(Intercept)'] + coef(fit_age)['age_subjectyoung']\n\n(Intercept) \n   629.5473 \n\n\nWhy are the means for the two groups used? The mean is the value closest to all values in a univariate dataset, and regression aims to inimise residuals (recall the line of best fit). So, a line is fit between the means of these two factor levels to achieve minimal residuals. This actually is the same thing as a t-test:\n\nt.test(exp(rt_lexdec) ~ age_subject, data = df_freq_eng)\n\n\n    Welch Two Sample t-test\n\ndata:  exp(rt_lexdec) by age_subject\nt = 63.406, df = 4144.6, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group old and group young is not equal to 0\n95 percent confidence interval:\n 152.3128 162.0325\nsample estimates:\n  mean in group old mean in group young \n           786.7200            629.5473 \n\n\nIf we compare this to our model, we see that the t- and p-values are identical (more on these later).\n\ntidy(fit_age)\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          787.      1.75     449.        0\n2 age_subjectyoung    -157.      2.48     -63.4       0\n\n\n\nfig_nocontrasts &lt;-\ndf_freq_eng |&gt; \n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"No contrasts\") +\n  # geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +  \n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",1,0)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Treatment contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_nocontrasts + fig_treatment"
  },
  {
    "objectID": "05-categorical_predictors.html#sum-contrasts",
    "href": "05-categorical_predictors.html#sum-contrasts",
    "title": "5  Categorical predictors",
    "section": "5.2 Sum contrasts",
    "text": "5.2 Sum contrasts\nTreatment/dummy coding is the default contrast coding scheme. Sum coding is another frequently used coding scheme, which is essentially centring categorical variables. Just as with continuous variables, the motivation for sum contrast coding mainly lies in the interpretation of interaction effects. How can we tell R we want to use sum contrast coding, and not dummy coding? There are different ways to do this:\n\n# first, make sure your variable is a factor\ndf_freq_eng$age_subject &lt;- as.factor(df_freq_eng$age_subject)\n# check\nclass(df_freq_eng$age_subject)\n\n[1] \"factor\"\n\n\n\n# next, you could use the contr.sum() function\ncontrasts(df_freq_eng$age_subject) &lt;- contr.sum(2) # where 2 means we have 2 levels\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold      1\nyoung   -1\n\n\nHere we see that old is coded as \\(-1\\) and young as \\(+1\\). I prefer to use +/-0.5 for reasons we don’t need to go into here. I would also prefer to have young coded in the negative value, and old in the positive value. This aids in the way I interpret the slope: a change in reaction times for the older group compared to the younger group.\n\n#or, you could manually control the sum contrasts\n## check the order of the levels\nlevels(df_freq_eng$age_subject)\n\n[1] \"old\"   \"young\"\n\n## code 'old' as +.5 and 'young' as -.5\ncontrasts(df_freq_eng$age_subject) &lt;- c(+0.5, -0.5)\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold    0.5\nyoung -0.5\n\n\nYou could also choose to store the contrast values in their own variable.\n\ndf_freq_eng &lt;- \n  df_freq_eng |&gt; \n  mutate(age_numeric = ifelse(age_subject == \"young\", -0.5, +0.5))\n\n\ndf_freq_eng |&gt; \n  select(age_subject, age_numeric) |&gt; \n  head()\n\n     age_subject age_numeric\n338        young        -0.5\n1790         old         0.5\n3125       young        -0.5\n3957         old         0.5\n3313       young        -0.5\n4145         old         0.5\n\n\nNow, we can run our model using either age_subject or age_numeric.\n\nfit_age_sum &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\n\n\nglance(fit_age_sum)$r.squared\n\n[1] 0.4682224\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\n\n\nNo difference in variance account for by our model.\n\ntidy(fit_age_sum) |&gt; select(term,estimate)\n\n# A tibble: 2 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 (Intercept)      708.\n2 age_subject1     157.\n\n\nBut there is a difference in the intercept, and a change in sign in our slope. Why is this?\n\nfig_sum1 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-1,1)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_sum5 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-.5,.5)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment + fig_sum5 + plot_annotation(tag_levels = \"A\")\n\n\n\n\nFigure 5.1: The difference in slope corresponds to which level is coded as 0 (dummy coding) or -5/-1 (sum coding)\n\n\n\n\nAs we see in Figure 5.1, the sign of the slope depends on how we’ve contrast coded our factor levels. In Figure 5.1 A, the old group is coded as \\(0\\) and young as \\(1\\). In Figure 5.1 B, the young group is coded as \\(-.5\\) and the old group as \\(+.5\\).\nThe intercept value is also now the overall mean of all observed reaction times, because now the \\(y\\) value when \\(x\\) equals zero lies in the middle of the two groups. The slope magnitude (i.e., size of the value) hasn’t changed, because the difference betwen the two group means has not changed.\n\nmean(exp(df_freq_eng$rt_lexdec))\n\n[1] 708.1336\n\n\n\n5.2.1 Exploring predicted values\nLet’s also explore the predicted values of our model with a categorical variable.\n\nhead(fitted(fit_age), n = 10)\n\n     338     1790     3125     3957     3313     4145      337     1789 \n629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 \n    3513     4345 \n629.5473 786.7200 \n\n\nWe see that there are only 2 values, 630 and 787. These correspond to the means for each group that we saw above. They also seem to be in a pattern: young-mean, old-mean, young-mean, old-mean, etc. How does this correspond to the age group of the participant for the first ten observations?\n\nhead(df_freq_eng$age_subject, n = 10)\n\n [1] young old   young old   young old   young old   young old  \nattr(,\"contrasts\")\n      [,1]\nold    0.5\nyoung -0.5\nLevels: old young\n\n\nThe first ten observations in our data are in young-old pairs. What are the first values in the raw data?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10)\n\n [1] 623.61 775.67 617.10 715.52 575.70 742.19 592.42 748.37 541.67 824.76\n\n\nAnd what is the difference between these reaction times and the fitted values?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10) - head(fitted(fit_age), n = 10)\n\n       338       1790       3125       3957       3313       4145        337 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991 -37.127299 \n      1789       3513       4345 \n-38.349991 -87.877299  38.040009 \n\n\n\nhead(residuals(fit_age))\n\n       338       1790       3125       3957       3313       4145 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991"
  },
  {
    "objectID": "05-categorical_predictors.html#summary",
    "href": "05-categorical_predictors.html#summary",
    "title": "5  Categorical predictors",
    "section": "5.3 Summary",
    "text": "5.3 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor\n\n\nImportant terms\n\n\n\n\n\nterm\ndescription/other terms"
  },
  {
    "objectID": "05-categorical_predictors.html#learning-objectives-1",
    "href": "05-categorical_predictors.html#learning-objectives-1",
    "title": "5  Categorical predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…"
  },
  {
    "objectID": "05-categorical_predictors.html#task",
    "href": "05-categorical_predictors.html#task",
    "title": "5  Categorical predictors",
    "section": "5.4 Task",
    "text": "5.4 Task"
  },
  {
    "objectID": "05-categorical_predictors.html#literaturverzeichnis",
    "href": "05-categorical_predictors.html#literaturverzeichnis",
    "title": "5  Categorical predictors",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis"
  },
  {
    "objectID": "06-logistic_regression.html#learning-objectives",
    "href": "06-logistic_regression.html#learning-objectives",
    "title": "6  Logistic regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn…\n\nhow to model binomial data with logistic regression\nhow to interpret log-odds and odds ratio"
  },
  {
    "objectID": "06-logistic_regression.html#set-up-environment",
    "href": "06-logistic_regression.html#set-up-environment",
    "title": "6  Logistic regression",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\noptions(pillar.sigfig = 5)\n\n\nlibrary(broman)\n# function to format p-values\nformat_pval &lt;- function(x){\n  if (x &lt; .001) return(paste('&lt;', '.001'))\n  if (x &lt; .01) return(paste('&lt;', '.01'))\n  if (x &lt; .05) return(paste('&lt;', '.05'))\n  paste('=', myround(x, 3))  # if above .05, print p-value to 3 decimalp points\n}\n\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\n# set preferred ggplot2 theme\ntheme_set(theme_bw() + theme(plot.title = element_text(size = 10)))"
  },
  {
    "objectID": "06-logistic_regression.html#generalised-linear-models",
    "href": "06-logistic_regression.html#generalised-linear-models",
    "title": "6  Logistic regression",
    "section": "6.1 Generalised linear models",
    "text": "6.1 Generalised linear models\nLogistic regression is a type of genearlised linear model (GLM), and is used to model binomial response data. Whereas continuous response variables, such as reaction times, assume a normal distribution (a.k.a., a Gaussian distribution), logistic regression assumes a binomial distribution (a.k.a., Bernoulli distribution). These are formalised in equations Equation 6.1, where \\(\\mu\\) and \\(\\sigma\\) correspond to the mean and standard deviation, and Equation 6.2, where \\(N\\) and \\(p\\) refer to the number of trials and the probability of \\(y\\) being \\(1\\) or \\(0\\).\n\\[\ny \\sim Normal(\\mu,\\sigma)\n\\tag{6.1}\\]\n\\[\ny \\sim binomial(N = 1, p)\n\\tag{6.2}\\]\nDon’t stress about this for now, I find the math behind everything will start to make more sense the more often you see it. However, some math is necessary in order to understand the output of our models, namely the relation between probabilities, odds, and log odds.\n\n6.1.1 Log-odds, odds ratio, and probabilities\nIn logistic regression, we the probability (\\(p\\)) of observing one outcome or another as a function of a predictor variable. In linguistic research, these outcomes could be the absence or presence of some phenomenon (pause, schwa, etc.) or button responses (yes/no, accept/reject). In logistic regression, we describe the probability, odds, or log-odds of a particular outcome over another.\nProbability is quite intuitive, and ranges from 0 (no chance) to 1 (certain). A 50% chance corresponds to a probability of 0.5. You’re also likely familiar with odds, which can range from 0 to infinity. Odds are often used in betting, such as the odds that I’ll win are 2:1, which corresponds to \\(\\frac{2}{1} = 2\\) in favour of my winning. Conversely, the odds that you’ll win are 1:2, corresponding to \\(\\frac{1}{2} = 0.5\\), meaning it’s less likely that you’ll win compared to you losing. If the odds are even, then: \\(\\frac{1}{1} = 1\\). So, odds of 1 correspond to a probability of 0.5. Log-odds are just the logarithmically-transformed odds: \\(log(2) =\\) 0.6931472; \\(log(0.5) =\\) -0.6931472; \\(log(1) =\\) 0. Probability can also be computed using the odds, as shown in Equation 6.3: \\(\\frac{2}{1+2} =\\) 0.6666667; \\(\\frac{1}{1+1} =\\) 0.5; \\(\\frac{0.5}{1+0.5} =\\) 0.3333333.\nWe can get the probability from a log odds value using plogis(), which performs the following calculation:\n\\[\np = \\frac{exp(log\\;odds)}{1 + exp(log\\;odds)} = \\frac{odds}{1 + odds}\n\\tag{6.3}\\]\nTable 6.1 gives an example of how the three relate to each other. The grey cells are all where chances re 50/50, with increasingly more likely (green) or less likely (red) values/\n\n\n\n\n\n\nTable 6.1:  Comparison of different values of probabilities/odds/log-odds \n  \n  \n    prob\n0.00669\n0.02298\n0.07586\n0.2227\n0.5\n0.7773\n0.92414\n0.97702\n0.99331\n    odds\n0.00674\n0.02352\n0.08208\n0.2865\n1\n3.49034\n12.18249\n42.52108\n148.41316\n    log_odds\n−5\n−3.75\n−2.5\n−1.25\n0\n1.25\n2.5\n3.75\n5\n  \n  \n  \n\n\n\n\n\nThis relationship is demonstrated in Figure 6.1. Take your time to really understand these plots, as it will help understand the output of our models.\n\n\n\n\n\nFigure 6.1: Relationship between probability, odds, and log-odds"
  },
  {
    "objectID": "06-logistic_regression.html#logistic-regression",
    "href": "06-logistic_regression.html#logistic-regression",
    "title": "6  Logistic regression",
    "section": "6.2 Logistic regression",
    "text": "6.2 Logistic regression\nI find the more we talk about the math behind the models before even running a model, the more overwhelmed we become. So, let’s run our first logistic regression and then dissect it to understand it. Most relevant to the output of a logistic regression model is Figure 6.1 C, as the model will output log-odds, and we most likely want to interpret them in terms of probabilities.\nWe’ll use a dataset from (biondo_yesterday_2022?), an eye-tracking reading study exploring the processing of adverb-tense concord in Spanish past and future tenses. Participants read sentences that began with a temporal adverb (e.g., yesterday/tomorrow), and had a verb marked with the congruent or incongruent tense (past/future). We will look at the measure regression in at the verb region.\nLet’s start by loading in the data:\n\ndf_tense &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") # for special characters in Spanish\n           ) |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  clean_names()\n\n\n6.2.1 EDA\nAnd conducting a quick EDA: print summaries and plot the response variables.\n\nhead(df_tense)\n\n# A tibble: 6 × 13\n  sj     item adv_type adv_t verb_t gramm   roi label       fp    gp    tt    ri\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        54 Deic     Past  Past   gramm     1 En la c…  1173  1173  1173     0\n2 1        54 Deic     Past  Past   gramm     2 ayer te…   474   474   474     0\n3 1        54 Deic     Past  Past   gramm     3 los car…   910   910   910     0\n4 1        54 Deic     Past  Past   gramm     4 encarga…  1027  1027  1027     0\n5 1        54 Deic     Past  Past   gramm     5 muchas …   521   521   521     0\n6 1        54 Deic     Past  Past   gramm     6 al prov…  1029  1029  1029     0\n# ℹ 1 more variable: ro &lt;dbl&gt;\n\n\nLet’s look at only the verb region, which is roi == \"4\" (region of interest 4).\n\ndf_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  select(roi, ri, ro) |&gt; \n  summary()\n\n      roi          ri               ro         \n Min.   :4   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:4   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :4   Median :0.0000   Median :0.00000  \n Mean   :4   Mean   :0.2189   Mean   :0.08527  \n 3rd Qu.:4   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :4   Max.   :1.0000   Max.   :1.00000  \n             NA's   :72       NA's   :72       \n\n\nAnd plot some of the continuous eye-tracking measures (fp: first-pass reading time; gp: go-past time/regression path duration; tt: total reading time), just to get an idea of what was is going on in the data. Let’s look at both the raw and log reading times.\n\n\n\n\n\nIt looks like, at least in total reading time (tt), the mode (peak) for ungrammatical conditions was higher than for grammatical conditions. Hard to tell about an effect of tense here. But we’re interested in binomial outcome variables: regressions in and out at the verb region.\n\ndf_tense |&gt; \n  mutate(gramm = as_factor(gramm)) \n\n# A tibble: 34,680 × 13\n   sj     item adv_type adv_t  verb_t gramm   roi label     fp    gp    tt    ri\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        54 Deic     Past   Past   gramm     1 En la…  1173  1173  1173     0\n 2 1        54 Deic     Past   Past   gramm     2 ayer …   474   474   474     0\n 3 1        54 Deic     Past   Past   gramm     3 los c…   910   910   910     0\n 4 1        54 Deic     Past   Past   gramm     4 encar…  1027  1027  1027     0\n 5 1        54 Deic     Past   Past   gramm     5 mucha…   521   521   521     0\n 6 1        54 Deic     Past   Past   gramm     6 al pr…  1029  1029  1029     0\n 7 1         4 Deic     Future Future gramm     1 Debid…   961   961  2059     1\n 8 1         4 Deic     Future Future gramm     2 el pr…   593   593  1356     1\n 9 1         4 Deic     Future Future gramm     3 los a…  1242  1242  1865     0\n10 1         4 Deic     Future Future gramm     4 colec…   562   562  1337     1\n# ℹ 34,670 more rows\n# ℹ 1 more variable: ro &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n6.2.2 Model\nNow let’s run our model. Verb tense and grammaticality are each two-level factors, so we’ll want to set sum coding for each of them. Let’s set past and grammatical to \\(-0.5\\), and future and ungrammatical to +0.5.\n\ndf_tense$verb_t &lt;- as.factor(df_tense$verb_t)\nlevels(df_tense$verb_t)\n\n[1] \"Future\" \"Past\"  \n\ncontrasts(df_tense$verb_t) &lt;- c(+0.5, -0.5)\ncontrasts(df_tense$verb_t)\n\n       [,1]\nFuture  0.5\nPast   -0.5\n\n\n\ndf_tense$gramm &lt;- as.factor(df_tense$gramm)\nlevels(df_tense$gramm)\n\n[1] \"gramm\"   \"ungramm\"\n\ncontrasts(df_tense$gramm) &lt;- c(-0.5, +0.5)\ncontrasts(df_tense$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5\n\n\nNow that we’ve set our contrasts (if you have continuous predictors, you would centre and potentially standardise them instead), we can fit our model. We use the glm() function to fit a genearlised linear model, and use the argument family = \"binomial\" to indicate our data are binomial.\n\nfit_tense_ri &lt;-\n  glm(ri ~ verb_t*gramm,\n    data = df_tense,\n    subset = roi == \"4\",\n    family = \"binomial\")\n\nWhat do our coefficients look like?\n\ntidy(fit_tense_ri) %&gt;%\n  mutate(p.value = as.numeric(p.value)) |&gt; \n  mutate(p.value = round(p.value*4,10)\n         ) |&gt; \n  gt() |&gt; \n  fmt_number(drop_trailing_zeros = T)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−1.23\n0.03\n−36.87\n0\n    verb_t1\n0.03\n0.07\n0.42\n2.71\n    gramm1\n0.32\n0.07\n4.86\n0\n    verb_t1:gramm1\n−0.13\n0.13\n−1\n1.27\n  \n  \n  \n\n\n\n\nLet’s first consider the estimates. The intercept is negative, meaning it is below 0. Verb tense is positive, meaning that there are more regressions in for the future compared to the past, holding grammaticality constant. Grammaticality is positive, meaning that there were more regressions in for the ungrammatical than grammatical conditions. But what does zero mean here? Logistic regression gives the estimates in log-odds. This means that a value of 0 means there is an equal probability of a regression in or out for both conditions (as in (tab-odds?)), i.e., the slope is flat (or not significantly different from 0). How can we convert our log-odds estimates to something more interpretable, like probabilities? Recall the equation in Equation 6.3, which would require a lot of typing. Luckily, we can just use the plogis() function, which takes a log-odds value and spits out the corresponding probability. We can also just use the exp() function to get the odds ratio from the log-odds.\n\nplogis(-1.23) # intercept prob\n\n[1] 0.2261814\n\nplogis(0.0277) # tense prob\n\n[1] 0.5069246\n\nexp(-1.23) # intercept odds\n\n[1] 0.2922926\n\nexp(0.0277) # tense odds\n\n[1] 1.028087\n\n\nThis is great, but a bit tedious. We can also just feed a tibble column through the plogis() and exp() functions to print a table with the relevant probabilities and odds.\n\ntidy(fit_tense_ri) %&gt;%\n  mutate(p.value = round(p.value*4,10),\n         prob = plogis(estimate),\n         odds = exp(estimate)\n         ) |&gt; \n  mutate_if(is.numeric, round, 4) |&gt; \n  gt() |&gt; \n  fmt_number(drop_trailing_zeros = T)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      prob\n      odds\n    \n  \n  \n    (Intercept)\n−1.23\n0.03\n−36.87\n0\n0.23\n0.29\n    verb_t1\n0.03\n0.07\n0.42\n2.71\n0.51\n1.03\n    gramm1\n0.32\n0.07\n4.86\n0\n0.58\n1.38\n    verb_t1:gramm1\n−0.13\n0.13\n−1\n1.27\n0.47\n0.88\n  \n  \n  \n\n\n\n\nWe see that the odds of the future tense have a regression in versus the past tense is ~1, with the corresponding probability of 0.51. Unsuprisingly, we see this p-value indicates this effect was not significant (p &gt; .05), and the z-value (note: not t-value!) is also low.\n\n\n\n\n\n\nz-values\n\n\n\nz-values correspond to the estimate divided by the standard error. It’s interpretation is similar to that of the t-value: a z-value of ~2 or higher will likely have a p-value below 0.05.\n\n\nThe interaction term is negative, what does this mean? We can interpret this as indicating that the effect of congruence is different in either level of tense. These effects are often more easily interpreted with a visualisation, e.g., using the plot_model() function from the sjPlot package. This effect is not significant, however.\n\nsjPlot::plot_model(fit_tense_ri, type = \"eff\", terms = c(\"gramm\", \"verb_t\")) + geom_line()\n\n\n\n\nFigure 6.2: Interaction plot of\n\n\n\n\nWe can also use the predict() function to extract the predicted values for each condition. We could just simply print the predicted values (predict(fit_tense_ri)), append the predicted values to the data frame\n\n# make sure dataset is the same length as the model data\ndf_tense_v &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri)\n\n# append model estimates\naugment(fit_tense_ri, data = df_tense_v) |&gt; \n  distinct(verb_t, gramm, .keep_all = T) |&gt;\n  arrange(verb_t) |&gt;  \n  select(verb_t, gramm, .fitted)\n\n# A tibble: 4 × 3\n  verb_t gramm   .fitted\n  &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt;\n1 Future gramm   -1.3395\n2 Future ungramm -1.0832\n3 Past   gramm   -1.4338\n4 Past   ungramm -1.0443\n\n\nOr we could create a list of the unique conditions.\n\ndf_sim &lt;-\n    tibble(\n    verb_t = rep(c('Past', 'Future'), each = 2),\n    gramm = rep(c('0', '1'), times = 2))\n\n# alternatively, just extract the relevant factor levels from your datafram\ndf_sim &lt;-\n  df_tense |&gt; \n  arrange(verb_t) |&gt; \n  distinct(verb_t, gramm) \n\n# and add predicted values\ndf_sim$fit &lt;- num(predict(fit_tense_ri, df_sim), digits = 5)\n\ndf_sim\n\n# A tibble: 4 × 3\n  verb_t gramm         fit\n  &lt;fct&gt;  &lt;fct&gt;   &lt;num:.5!&gt;\n1 Future gramm    -1.33953\n2 Future ungramm  -1.08322\n3 Past   gramm    -1.43378\n4 Past   ungramm  -1.04432\n\n\nAnd now if we look at the predicted log-odds values for the future and past tenses:\n\ndf_sim |&gt; \n  summarise(\n    mean_tense = mean(fit),\n    .by = verb_t)\n\n# A tibble: 2 × 2\n  verb_t mean_tense\n  &lt;fct&gt;   &lt;num:.5!&gt;\n1 Future   -1.21137\n2 Past     -1.23905\n\n\nWhat is the difference between these two numbers (in our model summary)?\n\ndf_sim |&gt; \n  summarise(\n    mean_tense = mean(fit),\n    .by = gramm)\n\n# A tibble: 2 × 2\n  gramm   mean_tense\n  &lt;fct&gt;    &lt;num:.5!&gt;\n1 gramm     -1.38666\n2 ungramm   -1.06377\n\n\nWhat is the difference between these two numbers (in our model summary)?\nSo, our slopes for verb_t and gramm correspond to the predicted difference between the two levels of each factor."
  },
  {
    "objectID": "06-logistic_regression.html#interpreting-our-coefficients",
    "href": "06-logistic_regression.html#interpreting-our-coefficients",
    "title": "6  Logistic regression",
    "section": "6.3 Interpreting our coefficients",
    "text": "6.3 Interpreting our coefficients\nWhat do our coefficient estimates reflect, though? Let’s remind ourselves of the rate of regressions in at the verb region:\n\ndf_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  summary()\n\n      sj                 item          adv_type            adv_t          \n Length:5688        Min.   :  1.00   Length:5688        Length:5688       \n Class :character   1st Qu.: 24.75   Class :character   Class :character  \n Mode  :character   Median : 52.00   Mode  :character   Mode  :character  \n                    Mean   : 50.87                                        \n                    3rd Qu.: 77.00                                        \n                    Max.   :101.00                                        \n    verb_t         gramm           roi       label                 fp        \n Future:2847   gramm  :3794   Min.   :4   Length:5688        Min.   :  81.0  \n Past  :2841   ungramm:1894   1st Qu.:4   Class :character   1st Qu.: 264.0  \n                              Median :4   Mode  :character   Median : 367.0  \n                              Mean   :4                      Mean   : 435.6  \n                              3rd Qu.:4                      3rd Qu.: 528.0  \n                              Max.   :4                      Max.   :2833.0  \n       gp               tt               ri               ro         \n Min.   :  87.0   Min.   :  90.0   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 283.0   1st Qu.: 324.0   1st Qu.:0.0000   1st Qu.:0.00000  \n Median : 404.0   Median : 489.5   Median :0.0000   Median :0.00000  \n Mean   : 508.5   Mean   : 600.6   Mean   :0.2189   Mean   :0.08527  \n 3rd Qu.: 603.0   3rd Qu.: 736.0   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :5461.0   Max.   :3936.0   Max.   :1.0000   Max.   :1.00000  \n\nptab_gramm &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  select(gramm, ri) |&gt; \n  table() |&gt; \n  prop.table()\n\nptab_tense &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  select(verb_t, ri) |&gt; \n  table() |&gt; \n  prop.table()\n\ndf_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  tabyl(gramm, ri, verb_t) |&gt; \n  adorn_percentages() |&gt; \n  adorn_totals()\n\n$Future\n   gramm         0         1\n   gramm 0.7924131 0.2075869\n ungramm 0.7471022 0.2528978\n   Total 1.5395153 0.4604847\n\n$Past\n   gramm         0         1\n   gramm 0.8074895 0.1925105\n ungramm 0.7396825 0.2603175\n   Total 1.5471720 0.4528280\n\n\nWe want to measure how much more likely a regression in (y = 1) is for ungrammatical conditions (x = 1) than in grammatical conditions (x = 0). Si we want to calculate the odds of a regression in for each case, and take their ratio:\n\n# odds(y = 1 | x = 0)\nodds_ri1_gramm0 &lt;- \n  ptab_gramm[1,2] / ptab_gramm[1,1] # in gramm conditions: ri 0/1\nodds_ri1_gramm1 &lt;- \n  ptab_gramm[2,2] / ptab_gramm[2,1] # in ungramm condiitons: ri 0/1\n\n## odds ratio\nodds_ri1_gramm1 / odds_ri1_gramm0\n\n[1] 1.380227\n\n## log odds\nlog(odds_ri1_gramm1) - log(odds_ri1_gramm0)\n\n[1] 0.322248\n\n# or\nlog(odds_ri1_gramm1 / odds_ri1_gramm0)\n\n[1] 0.322248\n\n## probability\nplogis(log(odds_ri1_gramm1 / odds_ri1_gramm0))\n\n[1] 0.579872\n\n\nSo the odds of a regression into the verb region is 1.4 times more likely in ungrammatical versus grammatical conditions.\n\nintercept &lt;- tidy(fit_tense_ri)$estimate[1]\ntense &lt;- tidy(fit_tense_ri)$estimate[2]\ngramm &lt;- tidy(fit_tense_ri)$estimate[3]\ninteract &lt;- tidy(fit_tense_ri)$estimate[4]\n\nWhat are the log odds for the past (tense = -0.5) grammatical (gramm = -0.5)?\n\nplogis(intercept)\n\n[1] 0.2270206\n\nplogis(tense)\n\n[1] 0.5069182\n\nplogis(gramm)\n\n[1] 0.5800279\n\nplogis(interact)\n\n[1] 0.4667631\n\n\n\ntidy(fit_tense_ri) |&gt; \n  mutate(prob = plogis(estimate))\n\n# A tibble: 4 × 6\n  term            estimate std.error statistic     p.value    prob\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -1.2252    0.033231 -36.870   1.4116e-297 0.22702\n2 verb_t1         0.027675  0.066462   0.41640 6.7712e-  1 0.50692\n3 gramm1          0.32289   0.066462   4.8583  1.1842e-  6 0.58003\n4 verb_t1:gramm1 -0.13314   0.13292   -1.0017  3.1651e-  1 0.46676\n\n\n\nplogis(intercept + tense*-.5 + gramm*-.5)\n\n[1] 0.1977379\n\n\nAnd past ungrammatical (gramm = +0.5)?\n\nplogis(intercept + tense*-.5 + gramm*.5)\n\n[1] 0.2539595\n\n\nAnd for the future conditions?\n\nplogis(intercept + tense*.5 + gramm*-.5)\n\n[1] 0.2021648\n\n\nAnd past ungrammatical (gramm = +0.5)?\n\nplogis(intercept + tense*.5 + gramm*.5)\n\n[1] 0.2592384\n\n\n\nplogis(-1.22521)\n\n[1] 0.2270209\n\n\n\nplogis(-1.22521)\n\n[1] 0.2270209\n\n\n\\[\nlog odds = log(\\frac{p}{1-p})\n\\] \\[\nodds = \\frac{p}{1-p}\n\\]"
  },
  {
    "objectID": "06-logistic_regression.html#visualising-model-predictions",
    "href": "06-logistic_regression.html#visualising-model-predictions",
    "title": "6  Logistic regression",
    "section": "6.4 Visualising model predictions",
    "text": "6.4 Visualising model predictions\nSomething we haven’t really covered is how to visualise our model predictions. So far we’ve only visualised the raw data, but when interpreting model results it helps to see the predictions. This is especially true for logistic regression, because our estimates are given in log odds, which are not very intuitive.\nWe can use the sjPlot package, which is very handy:\n\nlibrary(sjPlot)\n\nplot_model(fit_tense_ri)\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"eff\",\n           terms = \"verb_t\")\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"eff\",\n           terms = \"gramm\")\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"int\")\n\n\n\n\nOr we can use the ggeffects package to extract summaries of effects, and then feed them into ggplot2.\n\nlibrary(ggeffects)\n\n\nggeffect(fit_tense_ri)\n\n$verb_t\n# Predicted probabilities of ri\n\nverb_t | Predicted |       95% CI\n---------------------------------\nFuture |      0.22 | [0.21, 0.24]\nPast   |      0.21 | [0.20, 0.23]\n\n$gramm\n# Predicted probabilities of ri\n\ngramm   | Predicted |       95% CI\n----------------------------------\ngramm   |      0.20 | [0.19, 0.21]\nungramm |      0.26 | [0.24, 0.28]\n\nattr(,\"class\")\n[1] \"ggalleffects\" \"list\"        \nattr(,\"model.name\")\n[1] \"fit_tense_ri\"\n\nggeffect(fit_tense_ri,\n         terms = c(\"gramm\", \"verb_t\"))\n\n# Predicted probabilities of ri\n\n# verb_t = Future\n\ngramm   | Predicted |       95% CI\n----------------------------------\ngramm   |      0.21 | [0.19, 0.23]\nungramm |      0.25 | [0.23, 0.28]\n\n# verb_t = Past\n\ngramm   | Predicted |       95% CI\n----------------------------------\ngramm   |      0.19 | [0.18, 0.21]\nungramm |      0.26 | [0.23, 0.29]"
  },
  {
    "objectID": "06-logistic_regression.html#reporting",
    "href": "06-logistic_regression.html#reporting",
    "title": "6  Logistic regression",
    "section": "6.5 Reporting",
    "text": "6.5 Reporting\nSonderegger (2023) (Section 6.9) says the following:\n\nReporting a logistic regression model in a write-up is generally similar to reporting a linear regression model…Reporting a logistic regression model in a write-up is generally similar to reporting a linear regression model: the guidelines and rationale in section 4.6 for reporting individual coefficients and the whole model hold, with some adjustments.\n\nWe can produce such a table using the papaja package:\n\nlibrary(papaja)\n\nfit_tense_ri |&gt; \n  apa_print() |&gt;\n  apa_table()\n\n\n\nTable 6.2: Model summary for regressions in at the verb region. Estimates are given in log odds.\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(z\\)\n\\(p\\)\n\n\n\n\nIntercept\n-1.23\n[-1.29, -1.16]\n-36.87\n&lt; .001\n\n\nVerb t1\n0.03\n[-0.10, 0.16]\n0.42\n.677\n\n\nGramm1\n0.32\n[0.19, 0.45]\n4.86\n&lt; .001\n\n\nVerb t1 \\(\\times\\) Gramm1\n-0.13\n[-0.39, 0.13]\n-1.00\n.317\n\n\n\n\n\n\nOr by extracting the model summary with tidy(), and even adding our probabilities:\n\ntidy(fit_tense_ri) |&gt; \n  mutate(prob = plogis(estimate)) |&gt; \n  relocate(prob, .after = std.error) |&gt; \n  apa_table()\n\n\n(#tab:unnamed-chunk-44)\n\n\nterm\nestimate\nstd.error\nprob\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.23\n0.03\n0.23\n-36.87\n0.00\n\n\nverb_t1\n0.03\n0.07\n0.51\n0.42\n0.68\n\n\ngramm1\n0.32\n0.07\n0.58\n4.86\n0.00\n\n\nverb_t1:gramm1\n-0.13\n0.13\n0.47\n-1.00\n0.32"
  },
  {
    "objectID": "06-logistic_regression.html#summary",
    "href": "06-logistic_regression.html#summary",
    "title": "6  Logistic regression",
    "section": "6.6 Summary",
    "text": "6.6 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor\n\n\nImportant terms\n\n\n\n\n\nterm\ndescription/other terms"
  },
  {
    "objectID": "06-logistic_regression.html#learning-objectives-1",
    "href": "06-logistic_regression.html#learning-objectives-1",
    "title": "6  Logistic regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned…\n\nhow to model binomial data with logistic regression\nhow to interpret log-odds and odds ratio"
  },
  {
    "objectID": "06-logistic_regression.html#task",
    "href": "06-logistic_regression.html#task",
    "title": "6  Logistic regression",
    "section": "6.7 Task",
    "text": "6.7 Task\n\n6.7.1 Regressions out\nUsing the same dataset, run a logistic model exploring regressions in (ri) at the adverb region (roi = \"2\"). Before you run the model, do you have any predictions? Try plotting the regressions in for this region first, and generate some summary tables to get an idea of the distributions of regressions in across conditions.\n\n\n6.7.2 Dutch verb regularity\nLoad in the regularity data from the languageR package.\n\ndf_reg &lt;-\n  regularity |&gt; \n  clean_names()\n\n\nRegular and irregular Dutch verbs and selected lexical and distributional properties.\n\nOur relevant variables will be:\n\nwritten_frequency: a numeric vector of logarithmically transformed frequencies in written Dutch (as available in the CELEX lexical database).\nregularity: a factor with levels irregular (1) and regular (0).\nverb: a factor with the verbs as levels.\n\n\nFit a logistic regression model to the data which predicts verb regularity by written frequency. Consider: What type of predictor variable do you have, and what steps should you take before fitting your model?\nPrint the model coefficients, e.g., using tidy().\nInterpret the coefficients, either in log-odds or probabilities. Report your findings."
  },
  {
    "objectID": "06-logistic_regression.html#literaturverzeichnis",
    "href": "06-logistic_regression.html#literaturverzeichnis",
    "title": "6  Logistic regression",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  }
]