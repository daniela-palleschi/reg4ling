[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression for Linguists",
    "section": "",
    "text": "Course overview"
  },
  {
    "objectID": "index.html#moodle",
    "href": "index.html#moodle",
    "title": "Regression for Linguists",
    "section": "Moodle",
    "text": "Moodle\n\nlecture materials"
  },
  {
    "objectID": "index.html#course-aims",
    "href": "index.html#course-aims",
    "title": "Regression for Linguists",
    "section": "Course aims",
    "text": "Course aims\nBy the end of this course, you will\n\nblah blah\n\n\nWhat will you learn?\n\nlinear regression\nmultiple regression\nlogistic regression\nmixed models\nusing the lme4 package\nhow to apply these models appropriately to a variety of data types\n\n\n\nWhat will you not learn?\n\nstuff"
  },
  {
    "objectID": "index.html#style-guide",
    "href": "index.html#style-guide",
    "title": "Regression for Linguists",
    "section": "Style guide",
    "text": "Style guide\nAs a self-respecting Canadian, I bounce between what is typically considered ‘American’ and ‘British’ spelling. This will be most notable in my use of -ise and -ize, but will never result in a dropped ‘u’ from words like colour, which is used often in plots. I say this so that non-native speakers don’t start to think they’ve been spelling words wrong."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Meeting\n      Lecture\n      Topic\n      Vorbereitung\n    \n  \n  \n    2023-10-10\n1\nEquation of a line\n\n📚 Winter (2019): Ch. 1-3\n\n    2023-10-11\n2\nLinear regression\n\n📚 Winter (2019): Ch. 4\n📚 Winter (2013)\n\n    2023-10-12\n3\nContinuous predictors\n\n📚 Winter (2019): Ch. 5\n📚 Winter (2013)\n\n    2023-10-10\n4\nMultiple linear regression\n\n📚 Winter (2019): Ch. 6\n📚 Winter (2013)\n\n    2023-10-11\n5\nCategorical predictors\n\n📚 Winter (2019): Ch. 7\n📚 Winter (2013)\n\n    2023-10-12\n6\nModel assumptions\n\n\n    2023-10-10\n7\nLogistic regression\n\n📚 Winter (2019): Ch. 12\n\n    2023-10-11\n8\nLog odds, logits, and odds ratio\n\n\n    2023-10-12\n9\nFoundational Ideas\n\nVasishth & Nicenboim (2016)\n\n    2024-01-12\n10\nLinear mixed models\n\n📚 Winter (2019): Ch. 14\nWinter & Grice (2021); until Section 3\n\n    2024-01-12\n11\nLinear mixed models\n\n\n    2024-01-26\n12\n\n\n    2024-01-26\n13\n\n\n    2024-02-09\n14\n\n\n    2024-02-09\n15"
  },
  {
    "objectID": "kursuebersicht_blatt.html",
    "href": "kursuebersicht_blatt.html",
    "title": "Resources and Set-up",
    "section": "",
    "text": "Resources\nThis course is mainly based on Winter (2019), which is an excellent introduction into regression for linguists. For even more introductory tutorials, I recommend going through (Winter_2013?) and (Winter_2014?). For a more intermediate textbook, I’d recommend (sondregger_regression_2023?).\nIf you’re interested in the foundational writings on the topic of linear mixed models in (psycho)linguistic research, I’d recommend reading (baayen_2008?); (jaeger_2008?); (barr_2013?); (matschucek_2017?).\nFor this course, I assume that you are familiar with more classical statistical tests, such as the t-test, Chi-square test, etc. I also assume you are familiar with measures of central tendency (mean, median, mode) measures dispersion/spread (standard deviation), and with the concept of a normal distribution. Lacking this knowledge will not impeded your progress in the course, but is an important foundation on which we’ll be building. We can review these concepts in-class as needed."
  },
  {
    "objectID": "kursuebersicht_blatt.html#install-r",
    "href": "kursuebersicht_blatt.html#install-r",
    "title": "Resources and Set-up",
    "section": "Install R",
    "text": "Install R\n\nwe need the free and open source statistical software R to analyze our data\ndownload and install R: https://www.r-project.org"
  },
  {
    "objectID": "kursuebersicht_blatt.html#install-rstudio",
    "href": "kursuebersicht_blatt.html#install-rstudio",
    "title": "Resources and Set-up",
    "section": "Install RStudio",
    "text": "Install RStudio\n\nwe need RStudio to work with R more easily\nDownload and install RStudio: https://rstudio.com\nit can be helpful to keep English as language in RStudio\n\nwe will find more helpful information if we search error messages in English on the internet\n\nIf you have problems installing R or RStudio, check out this help page (in German): http://methods-berlin.com/wp-content/uploads/Installation.html"
  },
  {
    "objectID": "kursuebersicht_blatt.html#install-latex",
    "href": "kursuebersicht_blatt.html#install-latex",
    "title": "Resources and Set-up",
    "section": "Install LaTeX",
    "text": "Install LaTeX\n\nwe will not work with LaTeX directly, but it is needed in the background\nDownload and install LaTeX: https://www.latex-project.org/get/"
  },
  {
    "objectID": "kursuebersicht_blatt.html#troubleshooting-en-troubleshooting",
    "href": "kursuebersicht_blatt.html#troubleshooting-en-troubleshooting",
    "title": "Resources and Set-up",
    "section": "Troubleshooting (EN: Troubleshooting)",
    "text": "Troubleshooting (EN: Troubleshooting)\n\nError messages are very common in programming, at all levels.\nHow to find solutions for these error messages is an art in itself\nGoogle is your friend! If possible, google in English to get more information"
  },
  {
    "objectID": "01-equation_of_a_line.html",
    "href": "01-equation_of_a_line.html",
    "title": "1  Understanding straight lines",
    "section": "",
    "text": "Learning Objectives\nToday we will learn…\nThis lecture is based on the readings for today’s session: Winter (2013) and Winter (2019) (Ch. 3), and to a lesser extent (debruine_understanding_2021?); Winter (2014).\nBy the time we get to the point of wanting to model our data, we should have a pretty good idea of how our data look. We achieve this through running an exploratory data analysis (EDA), which consists of visualising your data and determining outliers (a question for another day: what is an outlier?), generating summary (i.e., descriptive) statistics, and just overall getting to know your data, without making any claims beyond your data.\nHowever, an understanding of the data design and collection procedure is incredibly important and is necessary in order to appropriately fit a model to our data. In fact, planning out your analyses when designing your experiment is highly recommended in order to ensure your data will have the appropriate structure and that the assumptions made by your chosen analyses are taken into consideration before data collection.\nThe next step after conducting an EDA is to model your data, i.e., run inferential statistics, this is where we try to generalise beyond our data.\nData exploration gives us an idea about what our data look like, but if we want to be able to make predictions about hypothetical observations, i.e., to predict values of our DV based on one (or more) IV(s), we need to fit a model to our data. This model can then predict values of our DV based on one (or more) IV(s), i.e., predicting an outcome variable (dependent variable, DV) from one or more predictors (independent variable, IV). Because we’re making predictions, we need to take into account the variability (i.e., error) in our data.\nToday we learned…\nTerm\n      Definition\n      Equation/Code\n    \n  \n  \n    Intercept\nNA\nNA"
  },
  {
    "objectID": "01-equation_of_a_line.html#statistical-tests-versus-models",
    "href": "01-equation_of_a_line.html#statistical-tests-versus-models",
    "title": "1  Understanding straight lines",
    "section": "2.1 Statistical tests versus models",
    "text": "2.1 Statistical tests versus models\nMany statistical courses and textbooks still put undue emphasis on classical statistical tests. However, these common statistical tests are simplified linear models, without the added benefits of linear models. In essence, statistical tests tell us something about our data, whereas statistical models can be used to make predictions about hypothetical future observations."
  },
  {
    "objectID": "01-equation_of_a_line.html#types-of-regression",
    "href": "01-equation_of_a_line.html#types-of-regression",
    "title": "1  Understanding straight lines",
    "section": "3.1 Types of regression",
    "text": "3.1 Types of regression\n\n\n\n\n\nregression type\npredictor\noutcome\n\n\n\n\nsimple regression\nSingle predictor\ncontinuous (numerical)\n\n\nmultiple regression\nmultiple predictor\ncontinuous (numerical)\n\n\nhierarchical/linear mixed models/linear mixed effect models\ninclude random effect\ncontinuous (numerical)\n\n\ngeneralised linear (mixed) models: logistic regression\nas above\nbinary/binomial data\n\n\ngeneralised linear (mixed) models: poisson regression\nas above\ncount data"
  },
  {
    "objectID": "01-equation_of_a_line.html#a-line-intercept-and-slope",
    "href": "01-equation_of_a_line.html#a-line-intercept-and-slope",
    "title": "1  Understanding straight lines",
    "section": "4.1 A line = intercept and slope",
    "text": "4.1 A line = intercept and slope\n\na line is defined by its intercept and slope\n\nin a regression model, these two are called coefficients\n\n\n\n\n\n\n\nFigure 4.1: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nEquation of a line\n\n\n\n\\[\n\\begin{align}\ny & = mx + c\\\\\nY_i &= (b_0 + b_1X_i) \\\\\noutcome_i & = (model) \\\\\ny_i & = (intercept + slope*x_i)\n\\end{align}\n\\]"
  },
  {
    "objectID": "01-equation_of_a_line.html#intercept-b_0",
    "href": "01-equation_of_a_line.html#intercept-b_0",
    "title": "1  Understanding straight lines",
    "section": "4.2 Intercept (\\(b_0\\))",
    "text": "4.2 Intercept (\\(b_0\\))\n\nthe value of \\(y\\) when \\(x = 0\\)"
  },
  {
    "objectID": "01-equation_of_a_line.html#slopes-b_1",
    "href": "01-equation_of_a_line.html#slopes-b_1",
    "title": "1  Understanding straight lines",
    "section": "4.3 Slopes (\\(b_1\\))",
    "text": "4.3 Slopes (\\(b_1\\))\nA slope describes a change in \\(x\\) (\\(\\Delta x\\)) over a change in \\(y\\) (\\(\\Delta y\\)), where \\(\\Delta\\) (the Greek letter delta) can be read as ‘difference’ (because it also starts with a ‘d’…). So a slope’s value equals the difference in \\(x\\) for a difference of 1 unit in \\(y\\). Positive slopes indicate that as \\(x\\) increases, \\(y\\) increases. A negative slope value indicates that as \\(x\\) increases, \\(y\\) decreases (or vice versa). A slope of 0 indicates there is no change in \\(y\\) as a function of \\(x\\), or: there is no change in \\(y\\) when the value of \\(x\\) changes.\n\\[\nslope = \\frac{\\Delta x}{\\Delta y}\n\\]\nThis relationship between \\(x\\) and \\(y\\) is sometimes referred to as “rise over run”: how do you ‘rise’ in \\(y\\) for a given ‘run’ in \\(x\\)? For example, if we were to measure children’s heights and ages, we would expect to find an increase in height for every increase in age. Or, for a linguistic example, we would expect to find longer whole-sentence reading times (a measure variable) for longer texts: if a sentence has 9 words (I find straight lines to be really interesting and fun.), we would expect longer reading times than a sentence with 3 words (I love lines.).\n\n\nwhat is the intercept of this line?\nwhat is the slope of this line?"
  },
  {
    "objectID": "01-equation_of_a_line.html#method-of-least-squares",
    "href": "01-equation_of_a_line.html#method-of-least-squares",
    "title": "1  Understanding straight lines",
    "section": "5.1 Method of least squares",
    "text": "5.1 Method of least squares\n\nso how is any given line chosen to fit any given data?\nthe method of least squares\n\ntake a given line, and square all the residuals (i.e., \\(residual^2\\))\nthe line with the lowest sum of squares is the line with the best fit to the given data\nwhy do we square the residuals before summing them up?\n\nso all values are positive (i.e., so that negative values don’t cancel out positive values)\n\n\nthis is how we find the line of best fit\n\nR fits many lines to find the one with the best fit\n\n\n\n\n\n\n\n\nFigure 5.1: Observed values (A), Residuals for line of best fit (B), A line of worse fit with larger residuals (C)"
  },
  {
    "objectID": "01-equation_of_a_line.html#task-1-pen-and-paper",
    "href": "01-equation_of_a_line.html#task-1-pen-and-paper",
    "title": "1  Understanding straight lines",
    "section": "6.1 Task 1: pen-and-paper",
    "text": "6.1 Task 1: pen-and-paper\nYou will receive a piece of paper with several grids on it. Follow the instructions, which include drawing some lines. If you aren’t in-class, this is the paper we are using:"
  },
  {
    "objectID": "01-equation_of_a_line.html#task-2-simulating-data",
    "href": "01-equation_of_a_line.html#task-2-simulating-data",
    "title": "1  Understanding straight lines",
    "section": "6.2 Task 2: simulating data",
    "text": "6.2 Task 2: simulating data\nAll of the figures we just saw (except Figure 4.1, which is from Winter (2019)) were generated in R. Simulating data and plotting is a great way to understand concepts, or even to map out our hypotheses. Let’s use R for the first time to try to simulate some data in order to plot lines. Our goal will be to produce a line that has the following:\n\nintercept = 4.5\nslope = 3\n\n\n6.2.1 Planning\nFirst, think about what steps will be required to create such plots. Can you come up with a workflow plan (without peaking at the next tasks)?\n\n\n6.2.2 Producing our line"
  },
  {
    "objectID": "02-simple_linear_regression.html",
    "href": "02-simple_linear_regression.html",
    "title": "2  Simple linear regression",
    "section": "",
    "text": "Learning Objectives\nToday we will learn…\nMake sure you always start with a clean R Environment (Session &gt; Restart R). This means you should have no objects stored in your Environment, and no packages loaded. To ensure this, you can go to the Session tab (up where you’ll find File, Help, etc.), and select Restart R. You can also use the keyboard shortcut Cmd/Ctrl+Shift+0 (that’s a zero, not an ‘oh’).\nIn addition, I often prefer to run options(scipen=999) in order to supress scientific notation, which writes very large or very small numbers in an unintuitive way. For example, 0.000005 is written 5e-06 in scientific notation.\n# suppress scientific notation\noptions(scipen=999)\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\nRecall that \\(y \\sim x\\) can be read as “y as a function of x”, or “y predicted by x”. Following Winter (2019), we will first model some word frequency data. In this experiment, Our first model will be:\n\\[\nRT \\sim frequency\n\\]\nLet’s load our data using the read_csv() function from readr. I also use the clean_names() function from the janitor package, which tidies up variable names (e.g., no spaces, all lower case).\n# load ELP_frequency.csv\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\nsummary(fit_rt_1)\nCall:\n1lm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n2-172.537  -74.677   -9.137   91.296  197.613\n\nCoefficients:\n3            Estimate Std. Error t value       Pr(&gt;|t|)\n4(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\n5Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n6Residual standard error: 117.8 on 11 degrees of freedom\n\n\n1\n\nformula repetition\n\n2\n\nresiduals: differences between observed values and those predicted by the model\n\n3\n\nnames for columns Estimates, standard error, t-value, p-value (Pr(&gt;|t|))\n\n4\n\nIntercept (\\(b_0\\))\n\n5\n\nSignificance codes\n\n6\n\nR\\(^2\\), a measure of model fit (squared residuals); percentage of variance in the data shared with the predictor (higher numbers are better…this is pretty low)\nNow let’s include a predictor, which will give us a slope. The slope represents the change in \\(y\\) (DV: rt) when we move 1-unit along \\(y\\) (IV: freq). In other words, it tells us the effect our IV has on the DV. Let’s first plot the data:\ndf_freq |&gt; \n  ggplot() +\n  aes(x = freq, y = rt) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\nNow that we’ve fit a model and understand the output, it’s time to think about whether this model is a good fit for our data. We first have to understand some assumptions that need to met in regression modelling. Importantly, these assumptions reate to the residuals of our model, not the raw data points themselves. The two assumptions we’ll focus on for now are the assumptions of normality of the residuals, and the constant variance of the residuals. Both assumptions are often diagnosed visually, so it takes some practice to learn what looks right.\nTerm\n      Definition\n      Equation/Code\n    \n  \n  \n    Simple linear regression\nNA\n`lm(response ~ predictor, data = data)`\nToday we learned…\nNow it’s your turn. Try to run the following lm() models:"
  },
  {
    "objectID": "02-simple_linear_regression.html#load-in-data",
    "href": "02-simple_linear_regression.html#load-in-data",
    "title": "2  Simple linear regression",
    "section": "Load in data",
    "text": "Load in data\nNow that we’ve loaded our packages, we can take a look at our dataset. Fitting our data to a model typically follows an Exploratory Data Analysis (EDA), which consists of plotting and summarising a data set. We won’t get into the EDA process, but there are many great resources on how to go about it in R (e.g., https://r4ds.hadley.nz/).\nLet’s load our dataset using the read_csv() function from the tidyverse package readr. Alternatively, we could use base R to load in the data with read.csv(). Some additional functions in the code below:\n\nmutate_if(is.character, as.factor) force character variables to factors\nfilter for the verb region from critical items only, remove participant 3, and remove values of first-fixtation that are 0\n\n\n# load in dataset\n# df_freq &lt;-\n#   readr::read_csv(\n#     here::here(\"data/tidy_data_lifetime_pilot.csv\"),\n#     # for special characters\n#     locale = readr::locale(encoding = \"latin1\")\n#   ) |&gt;\n#   mutate_if(is.character, as.factor) |&gt; # all character variables as factor\n#   # mutate(lifetime = fct_relevel(lifetime, \"living\", \"dead\"),\n#   #        tense = fct_relevel(tense, \"PP\", \"SF\")) |&gt;\n#   filter(type == \"critical\", # only critical trials\n#          px != \"px3\", # px3 had a lot of missing values\n#          rt &gt; 0, # only values of rt above 0\n#          region == \"verb\") %&gt;% # critical region only\n#   droplevels() # remove any factor levels with no observations"
  },
  {
    "objectID": "02-simple_linear_regression.html#mini-eda",
    "href": "02-simple_linear_regression.html#mini-eda",
    "title": "2  Simple linear regression",
    "section": "3.1 Mini-EDA",
    "text": "3.1 Mini-EDA\nLet’s explore the data a little bit, which is what we would normally do before fitting any models. First, let’s see how the data is structured.\n\n# print head of df_freq\nhead(df_freq)\n\n# A tibble: 6 × 3\n  word      freq    rt\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 thing    55522  622.\n2 life     40629  520.\n3 door     14895  507.\n4 angel     3992  637.\n5 beer      3850  587.\n6 disgrace   409  705 \n\n\nLooks like there are only 3 columns: word, freq, and rt. We can assume that they correspond to the word, its frequency, and the reaction time, respectively. We can also see in our global environment that there are 12 observations, meaning 12 rows.\nThe summary() function provides summaries of each variable in a dataframe. For numeric variables, it will provide descriptive statistics for the centre and spread of the data (mean, median, quartiles). For categorical data, it will provide the count per category. For character variables, simply lists the number of observations.\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5  \n\n\nWe see freq has a pretty big range, from 4 to 55522. rt has a range of 507.38 to 877.53, with an average reaction time of 679.9. Let’s now get an overview of the relationship between freq and rt.\n\nplot(df_freq$freq, df_freq$rt)\n\n\n\n\nWe see there are a lot of frequency values below roughly 400, and these seem to have higher reaction times than those with a higher frequency value. Let’s fit these data to our first linear model to explore this effect of frequency on reaction times."
  },
  {
    "objectID": "02-simple_linear_regression.html#lm",
    "href": "02-simple_linear_regression.html#lm",
    "title": "2  Simple linear regression",
    "section": "3.2 lm()",
    "text": "3.2 lm()\nThe the lm() function fits simple linear models. As arguments it takes a formula and a dataset, at minimum.\n\\[\nlm(outcome \\sim 1 + predictor,\\;data\\;=\\;df\\_name)\n\\]\nThe lm() function formula syntax can be read as: outcome predicted by the intercept (1 is a placeholder for the intercept) and predictor. The intercept is included by default, so if you omit the 1 the intercept is still included in the formula. If you wanted to remove the intercept (which you often don’t), you could replace 1 with 0.\n\n3.2.1 Running a model\nBefore we add our predictor freq, let’s see what our model looks like without it. We can write it as:\n\nlm(rt ~ 1, data = df_freq) \n\nBut it’s useful to save the model as an object so that we can call on it later. It’s often a good idea to have informative prefixes to your objects\n\nfit_rt_1 &lt;- lm(rt ~ 1, data = df_freq) \n\n\n\n\n\n\n\nObject naming\n\n\n\nYou may have wondered what the letters df are for when loading in our data set as df_freq. These letters stand for ‘data frame’, and serve as a reminder of what exactly that object in our environment is. We might also have wanted to plot the frequency data, in which case we could call save the plot as fig_freq or plot_freq. Here we are saving our model as fit_rt_1, using ‘fit’ to signal that this object is a model fit. You could also save it as mod_freq_1, lm_freq_1, or whatever you see fit. This simply helps keep our environment structured, which will become useful when you begin working with multiple datasets at a time.\n\n\n\n\n3.2.2 Model ouput\nNow that we’ve saved our model in our Enrivonement, we can call it by name. Printing just the model gives us the formula and the coefficients.\n\n# print model\nfit_rt_1\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nCoefficients:\n(Intercept)  \n      679.9  \n\n\nRecall that the intercept and slope are called coefficients. Why do we only see Intercept? Because we didn’t include any predictors in our model. This output isn’t very dense, however. We typically use the summary() function to print full model outputs.\n\nsummary(fit_rt_1)\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-172.537  -74.677   -9.137   91.296  197.613 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 117.8 on 11 degrees of freedom\n\n\nWe see a lot more information here.\n\n\n\n\n\n\nbroom package\n\n\n\nThe broom package has some useful functions for printing model outputs\n\ntidy() produces a tibble (type of dataframe) of the coefficients\nglance() produces goodness of fit measures (which we won’t discuss)\n\nThe outputs from tidy() and glance() can be fed into kable and/or kable_styling() to create formatted tables\n\ntidy(fit_rt_1)\n\n# A tibble: 1 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     680.      34.0      20.0 5.38e-10\n\n\n\nglance(fit_rt_1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0             0  118.        NA      NA    NA  -73.7  151.  152.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\naugment() adds model values as columns to your dataframe (e.g., useful for plotting observed vs. fitted values).\n\naugment(fit_rt_1, data = df_freq) %&gt;% summary()"
  },
  {
    "objectID": "02-simple_linear_regression.html#fit-model-treatment-contrasts",
    "href": "02-simple_linear_regression.html#fit-model-treatment-contrasts",
    "title": "2  Simple linear regression",
    "section": "5.1 Fit model (treatment contrasts)",
    "text": "5.1 Fit model (treatment contrasts)\n\n# fit simple linear model\nfit_rt_freq &lt;- lm(rt ~ freq, data = df_freq)\n\n\n5.1.1 Model summary\n\nsummary(fit_rt_freq)\n\n\nCall:\nlm(formula = rt ~ freq, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-155.947  -73.141    2.117   85.050  163.837 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 713.706298  34.639105   20.60 0.0000000016 ***\nfreq         -0.003382   0.001699   -1.99       0.0746 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 104.6 on 10 degrees of freedom\nMultiple R-squared:  0.2838,    Adjusted R-squared:  0.2121 \nF-statistic: 3.962 on 1 and 10 DF,  p-value: 0.07457\n\n\n\n\n5.1.2 Intercept\nThe intercept in our last model was the mean reaction time. But now it’s a different value.\n\n# print model intercept\ncoef(fit_rt_freq)['(Intercept)']\n\n(Intercept) \n   713.7063 \n\n\n\n# print data mean\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nOur intercept is no longer the grand mean of first-pass reading times…what is it?\n\n\n5.1.3 Slope\nOur slope was our slope -0.0033823. What does this correspond to?\n\n# print slope\ncoef(fit_rt_freq)['freq']\n\n        freq \n-0.003382289 \n\n\nThis is the change in \\(y\\) (our DV rt) for a 1-unit change in \\(x\\) (our IV: freq). So when we move up 1 unit in frequency, reaction times decrease by -0.0033823. Whether or not it makes sense to consider this number depends on the measurement unit your data is in, e.g., a unit change from one millimeter or one meter will have a drastically different slope value (say, for age), but the actual slope will be the exact same.\n\nheights_m &lt;- c(1.71, 1.56, .9, 2.06, 1.63)\nheights_cm &lt;- c(171, 156, 90, 206, 163)\nheights_mm &lt;- c(1710, 1560, 900, 2060, 1630)\nyear &lt;- c(22,15,10,26,18)\nmonths &lt;- c(22,15,10,26,18)*12\ndays &lt;- c(22,15,10,26,18)*365\n\ndf_heights_age &lt;- cbind(year, months, days, heights_mm, heights_cm, heights_m) |&gt; as.data.frame() |&gt; \n  pivot_longer(\n    cols = c(heights_mm, heights_cm, heights_m),\n    names_to = \"unit\",\n    values_to = \"height\"\n  ) |&gt; \n  pivot_longer(\n    cols = c(year, months, days),\n    names_to = \"unit_age\",\n    values_to = \"age\"\n  )\n\n\n\n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ days)\n\n\nCall:\nlm(formula = heights_cm ~ days)\n\nCoefficients:\n(Intercept)         days  \n   39.66230      0.01769  \n\nlm(heights_m ~ months)\n\n\nCall:\nlm(formula = heights_m ~ months)\n\nCoefficients:\n(Intercept)       months  \n   0.396623     0.005382  \n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ year)\n\n\nCall:\nlm(formula = heights_cm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     39.662        6.458  \n\nlm(heights_m ~ year)\n\n\nCall:\nlm(formula = heights_m ~ year)\n\nCoefficients:\n(Intercept)         year  \n    0.39662      0.06458  \n\n\n\nggplot(data = df_heights_age) +\n  aes(x = height, y = age) +\n  facet_wrap(unit ~ unit_age, scales = \"free\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw()"
  },
  {
    "objectID": "02-simple_linear_regression.html#normality",
    "href": "02-simple_linear_regression.html#normality",
    "title": "2  Simple linear regression",
    "section": "6.1 Normality",
    "text": "6.1 Normality\nWhen a model satisfies the normalit assumption, its residuals (i.e., the difference between the fitted and observed values) will be approximately normally distributed. Normality is typically visualised using a histogram (Figure 6.1 A) and/or a quantile-quantile (Q-Q) plot (Figure 6.1 B).\n\n\n\n\n\nFigure 6.1: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWinter (2019)’s description of how QQ plots are generated (p. 110):\nTo create this plot, every residual is transformed into a percentile (or quantile) […] The question the Q-Q plot answers is: what is the corresponding numerical value of the 13.8th percentile on the normal distribution? If the values are the same, they will fit on a straight line, which indicates that the two distributions (the distribution of the residuals and the theoretical normal distribution) are very similar."
  },
  {
    "objectID": "02-simple_linear_regression.html#constant-variance",
    "href": "02-simple_linear_regression.html#constant-variance",
    "title": "2  Simple linear regression",
    "section": "6.2 Constant variance",
    "text": "6.2 Constant variance\nWhen a model satisfies the constant variance assumption (also called homoscedasticity, or the absence of heteroscedasticity), the spread of residuals will be equal across the regression line. This is typically visualised using a residual plot, which should look like a blob (Figure 6.1 C)."
  },
  {
    "objectID": "02-simple_linear_regression.html#visualising-model-assumptions",
    "href": "02-simple_linear_regression.html#visualising-model-assumptions",
    "title": "2  Simple linear regression",
    "section": "6.3 Visualising model assumptions",
    "text": "6.3 Visualising model assumptions\nLet’s plot our residuals to assess whether our model satisfies the assumptions of normality and constant variance.\n\n6.3.1 Histogram\nWe can do this how it’s done in Winter (2019) (in Ch. 6, p. 110-111), by first extracting the residuals from the model and then fitting them them using the base R function hist().\n\n# extract residuals\nres &lt;- residuals(fit_rt_freq)\n\n\n# plot histogram\nhist(res)\n\n\n\n\nOr, we can use the augment() function from broom to append model values to our original data frame, and then feed this into ggplot() from ggplot2 (or even feed it into hist()).\n\n# or, add to df\ndf_freq &lt;- broom::augment(fit_rt_freq, df_freq)\n\n\n# and create ggplot\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 8, fill = \"grey\", colour = \"black\") +\n  theme_bw()\n\n\n\n\n\n\n6.3.2 Q-Q plot\nAgain, we can do it Bodo’s way:\n\nqqnorm(res)\nqqline(res)\n\n\n\n\nOr using augment() and ggplot().\n\ndf_freq |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() \n\n\n\n\n\n\n6.3.3 Residual plot\nBodo’s way:\n\nplot(fitted(fit_rt_freq), res)\n\n\n\n\nOr with ggplot:\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)"
  },
  {
    "objectID": "02-simple_linear_regression.html#performance-package",
    "href": "02-simple_linear_regression.html#performance-package",
    "title": "2  Simple linear regression",
    "section": "6.4 performance package",
    "text": "6.4 performance package\nI like to use the performance package to visualise model fit (Lüdecke et al., 2021).\n\nperformance::check_normality(fit_rt_freq)\n\nOK: residuals appear as normally distributed (p = 0.702).\n\n\n\nperformance::check_heteroscedasticity(fit_rt_freq)\n\nOK: Error variance appears to be homoscedastic (p = 0.980).\n\n\n\nperformance::check_model(fit_rt_freq)\n\n\n\n\n\n6.4.1 Coefficients table with summary()\n\n\n&gt; summary(fit_rt_freq)\n\nCall:\n1lm(formula = rt ~ lifetime, data = df_freq, subset = rt &gt; 0)\n\n2Residuals:\n    Min      1Q  Median      3Q     Max \n-228.99 -109.29  -26.99   58.86  777.71 \n\nCoefficients:\n3             Estimate Std. Error t value Pr(&gt;|t|)\n4(Intercept)  309.142      6.259  49.394 &lt;0.0000000000000002 ***\n5lifetime1     31.701     12.517   2.533              0.0116 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 57.46 on 541 degrees of freedom\n6Multiple R-squared:  0.01172,   Adjusted R-squared:  0.00989\nF-statistic: 6.414 on 1 and 541 DF,  p-value: 0.0116\n\n\n1\n\nformula\n\n2\n\nResiduals: differences between observed values and those predicted by the model\n\n3\n\nNames for columns Estimates, SE, t-value, p-value\n\n4\n\nIntercept (\\(b_0\\)), i.e., value of \\(y\\) (first-pass) with a move of one unit of \\(x\\) (lifetime)\n\n5\n\nSlope (\\(b_1\\)), i.e., change in first fixation going from dead to living\n\n6\n\nOutput from an ANOVA\n\n\n\n\n\n\n\nwhat is the intercept?\nis the slope positive or negative?\n\nwhat is it’s value?\n\nthis is what the slope would look like:"
  },
  {
    "objectID": "02-simple_linear_regression.html#exploring-the-model",
    "href": "02-simple_linear_regression.html#exploring-the-model",
    "title": "2  Simple linear regression",
    "section": "Exploring the model",
    "text": "Exploring the model\n\n# how many observed values did we enter into the model?\ndf_freq |&gt; \n  nrow()\n\n[1] 12\n\n\n\n# how many observed values did we enter into the model?\nlength(fitted(fit_rt_freq))\n\n[1] 12"
  },
  {
    "objectID": "02-simple_linear_regression.html#exploring-the-model-residuals",
    "href": "02-simple_linear_regression.html#exploring-the-model-residuals",
    "title": "2  Simple linear regression",
    "section": "Exploring the model: residuals",
    "text": "Exploring the model: residuals\n\n# what do our FITTED values look like?\nhead(fitted(fit_rt_freq))\n\n       1        2        3        4        5        6 \n525.9148 576.2873 663.3271 700.2042 700.6845 712.3229 \n\n\n\n# what do our OBSERVED values look like?\nhead(df_freq$rt)\n\n[1] 621.77 519.56 507.38 636.56 587.18 705.00\n\n\n\n# what is the difference between the FITTED and OBSERVED values?\nhead(df_freq$rt) - head(fitted(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942 \n\n\n\n# what are our RESIDUALS?\nhead(residuals(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942"
  },
  {
    "objectID": "02-simple_linear_regression.html#exploring-the-model-1",
    "href": "02-simple_linear_regression.html#exploring-the-model-1",
    "title": "2  Simple linear regression",
    "section": "Exploring the model",
    "text": "Exploring the model\n\nwhat were our coefficients?\n\n\ncoef(fit_rt_freq)\n\n  (Intercept)          freq \n713.706297951  -0.003382289 \n\n\n\nwhat would be our predicted reaction time for a word with frequency of 0?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 0\n\n(Intercept) \n   713.7063 \n\n\n\nignore the (Intercept) label here, R just takes the first label when performing an operation on 2 vectors\nwhat is the mean of our predictor coded as +0.5?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 5000\n\n(Intercept) \n   696.7949 \n\n\n\n\n\n\n\n\nImage source: Winter (2019) (all rights reserved)"
  },
  {
    "objectID": "03-continuous_predictors.html",
    "href": "03-continuous_predictors.html",
    "title": "3  Continuous predictors",
    "section": "",
    "text": "Learning Objectives\nToday we will learn…\n# suppress scientific notation\noptions(scipen=999)\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\nIn the last lectures we saw that the equation for a straight line boils down to its intercept and slope, and that linear regression fits a line to our data. This line results in predicted/fitted values, which fall along the line, and residuals, which are the difference between our observed values and the fitted values.\nWe also learned about two model assumptions: normality of residuals, and constant variance of residuals. We learned that we can plot these with histograms or Q-Q plots (normality), and residual plots (constant variance).\nNow that we understand what a simple linear does, we can take a step back and focus on what we put into the model. So far we’ve looked at reaction times (milliseconds) as a function of word frequency. However, we don’t typically feed raw continuous data into a model, because most continuous linguistic variables are not normally distributed, and so a straight line will not fit it very well (because there will be some large variance for higher values).\nLinear transformations refer to constant changes across values that do not alter the relationship between these values. For example, adding, subtracting, or multiplying by a constant value will not alter the difference between values. Think of the example in the last lecture on the relationship between heights and ages as a function of the measurement unit: the relationship between all the values did not alter, because the difference between heights millimeters, centimeters, and meters is constant, as is the difference between ages in days, months, or years. We’ll now look at some common ways of linearly transforming our data, and the reasons behind doing so.\nThis is really the meat and potates of dealing with continuous variables (depending on your subfield). In linguistic research, and especially experimental research, we often deal with continuous variables truncated/bound at 0. Reaction times, reading times and formant frequencies are all examples of such types of data: there is no such thing as a negative reading time or fundamental frequency. The problem with these types of data is that they are almost never normally distributed, which has implications for the normality of residuals for any line that tries to fit to these data. Very often, this type of data will have a ‘positive skew’, or a long tail off to the right (assuming larger values are plotting to the right). This shape is not symmetrical, meaning that the residuals tend to be much larger for larger values. It is also often the case that these very large, exceptional values will have a stronger influence on the line of best fit, leading to the coefficient estimates that are “suboptimal for the majority of data points” [@Baayen (2008); p. 92]. How do we deal with this nonnormality? We use non-linear transformations, the most common of which is the log-transformation.\nToday we learned…\nTerm\n      Definition\n      Equation/Code\n    \n  \n  \n    Centering\ntype of linear transformation\n`dplyr::mutate(variable = variable - mean(variable))`\nContinuous data are often transformed before fitting a model to this data. Linear transformations, like adding or multiplying all values by a single value, are often performed on continuous predictors by means of centring and standardizing (when there are multiple continuous predictors). Non-linear transformations are often performed on continuous data with a positive skew (a few values much larger than the majority) in order to satisfy the normality assumption. Although the normality assumption refers to the normality of residuals, the distribution of the data will have implications for the distribution of the residuals. The most common non-linear transformation is the log-transformation (the inverse of the exponential), which shrinks values, especially making big numbers smaller. This has the result of squeezing big numbers towards smaller numbers, reducing the spread in the distribution (e.g., the log of 3 is 1.0986123, the log of 30 is 3.4011974, and the log of 30 is 5.7037825).\nWhat to do with this information? If you have continuous data truncated at 0 (with no upperbound, e.g., reaction time data or fundamental frequency), visualise the data (histogram and Q-Q plot) in order to check its distribution. If it is not normally distributed, you will likely want to log-transform it. Is this data your response variable? Then that is all you will likely want to do. Is this data a predictor variable? Then you will want to centre it (subtract the mean of this variable from all values). Do you have more than one continuous predictor variable? Then standardizing these variables will facilitate the interpretation of interaction effects (we’ll talk about these soon)."
  },
  {
    "objectID": "03-continuous_predictors.html#load-data",
    "href": "03-continuous_predictors.html#load-data",
    "title": "3  Continuous predictors",
    "section": "Load data",
    "text": "Load data\n\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\n\nReminder of our variables:\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5"
  },
  {
    "objectID": "03-continuous_predictors.html#centering",
    "href": "03-continuous_predictors.html#centering",
    "title": "3  Continuous predictors",
    "section": "4.1 Centering",
    "text": "4.1 Centering\nCentering is typically applied to predictor variables. Centering refers to subtracting the mean of a variable from each value, resulting in each centered value representing the original value’s deviance from the mean (i.e., a mean-deviation score). What would a centered value of \\(0\\) represent in terms of the original values?\nLet’s try centering our frequency values. To create a new variable (or alter an existing variable), we can use the mutate() function from dplyr.\n\n# add centered variable\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_c = freq-mean(freq))\n\nThis can also be done with base R, but it’s a lot more verbose.\n\n# add centered variable with base R\ndf_freq$freq_c &lt;- df_freq$freq-mean(df_freq$freq)\n\nNow let’s fit our models.\n\n# run our model with the original predictor\nfit_rt_freq &lt;- \n  lm(rt ~ freq, data = df_freq)\n\n\n# run our model with the centered predictor\nfit_rt_freq_c &lt;- \n  lm(rt ~ freq_c, data = df_freq)\n\nIf we compare the coefficients from fit_rt_freq and fit_rt_freq_c, what do we see? The only difference is the intercept values: 713.706298 (uncentered) and 679.9166667 (centered).\n\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nThe intercept for a centered continuous predictor variable corresponds to the mean of a continuous response variable. This is crucial in interpreting interaction effects, which we will discuss tomorrow. For more detail on interpreting interactions, see Chapter 8 in Winter (2019) (we won’t be discussing this chapter as a whole).\n\n\n\n\n\n\nCentering interval data\n\n\n\nIf you have interval data with a specific upper and lower bound, you could alternatively subtract the median value. In linguistic research, this is most typically rating scale data. For example, if you have a dataset consisting of ratings from 1-7, you can centre these ratings by subtracting 4 from all responses. A centred response of -3 would correspond to the lowest rating (1), and of +3 to the highest rating (7), which 0 would correspond to a medial rating (4). This can also be helpful in plotting, as there is no question as to whether 1 or 7 was high or low, because all ratings are now centred around 0 (and negative numbers correspond to our intuition of low-ratings)."
  },
  {
    "objectID": "03-continuous_predictors.html#standardizing-z-scoring",
    "href": "03-continuous_predictors.html#standardizing-z-scoring",
    "title": "3  Continuous predictors",
    "section": "4.2 Standardizing (z-scoring)",
    "text": "4.2 Standardizing (z-scoring)\nWe can also standardize continuous predictors by dividing centered values by the standard deviation of the sample. Let’s look at our frequency/reaction time data again.\nFirst, what are our mean and standard deviation? This will help us understand the changes to our variables as we center and stardardize them.\n\nmean(df_freq$freq)\n\n[1] 9990.167\n\n\n\nsd(df_freq$freq)\n\n[1] 18558.69\n\n\nWhat are the first six values of freq in the original scale?\n\ndf_freq$freq[1:6]\n\n[1] 55522 40629 14895  3992  3850   409\n\n\nWhat are the first six values of freq_c in the centered scale? These should be the values of freq minus the mean of freq (which we saw above is 9990.1666667).\n\ndf_freq$freq_c[1:6]\n\n[1] 45531.833 30638.833  4904.833 -5998.167 -6140.167 -9581.167\n\n\nNow, let’s create our standardised z-scores for frequency by dividing these centered values by the standard deviation of freq (which will be the same as the standard deviation of freq_c), and which we saw is 18558.6881679. Again, this can be done with mutate() from dplyr, or by using base R syntax.\n\n# standardise using the tidyverse\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_z = freq_c/sd(freq))\n\n\n# standardize with base R\ndf_freq$freq_z &lt;- df_freq$freq_c/sd(df_freq$freq)\n\n\nhead(df_freq)\n\n# A tibble: 6 × 5\n  word      freq    rt freq_c freq_z\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 thing    55522  622. 45532.  2.45 \n2 life     40629  520. 30639.  1.65 \n3 door     14895  507.  4905.  0.264\n4 angel     3992  637. -5998. -0.323\n5 beer      3850  587. -6140. -0.331\n6 disgrace   409  705  -9581. -0.516\n\n\n\n\n\n\n\n\nCorrelation"
  },
  {
    "objectID": "03-continuous_predictors.html#log-transformation",
    "href": "03-continuous_predictors.html#log-transformation",
    "title": "3  Continuous predictors",
    "section": "5.1 Log-transformation",
    "text": "5.1 Log-transformation\nLet’s look at our reaction time data again. We’ll log-transform our reaction time data and frequency data. Note that in Winter (2019), frequency is transformed using log to the base 10 for interpretability, but we’ll stick to the natural logarithm.\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = log(freq)) +\n  geom_density()\n\n\n\n\n\ndf_freq &lt;-\n  df_freq |&gt; \n    mutate(rt_log = log(rt),\n           freq_log = log(freq))\n\n\nlm(rt_log ~ freq_log, data = df_freq) |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 freq_log     -0.0453   0.00871     -5.20 4.03e- 4\n\n\n\n# or, log-transform directly in the model syntax\nlm(log(rt) ~ log(freq), data = df_freq) |&gt; tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 log(freq)    -0.0453   0.00871     -5.20 4.03e- 4"
  },
  {
    "objectID": "04-multiple_regression.html",
    "href": "04-multiple_regression.html",
    "title": "4  Multiple Regression",
    "section": "",
    "text": "Summary\nToday we will learn…\n# suppress scientific notation\noptions(scipen=999)\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\nSo far we’ve worked with simple linear models, which fit a model to a predictor and response variable. These models do not differ so greatly from a one- or two-sample t-test (for a categorical predictor) or Pearson’s r (for a standardised continuous predictor). You might be wondering then we would bother with linear regression. One reason is that it allows us to include multiple predictors in our models, which still boils down to modeling the mean, but while condintioning the mean on multiple variables at once.\nRecall the equation of a line (Equation 5.1), which states that any value of \\(y\\) equals the intercept (\\(b_0\\)) plus the corresponding value of \\(x\\) multiplied by the slope (\\(b_1x\\)), plus the error, which are our residuals (\\(e\\)). In multiple regression, we can include more than one slope (Equation 5.2).\n\\[\ny = b_0 + b_1x + e\n\\tag{5.1}\\]\n\\[\ny = b_0 + b_1x + b_2x + ... + e\n\\tag{5.2}\\]\nflowchart LR\n  A[Continuous variable] \n  A --&gt; F[Zero-truncated with positive skew \\n e.g., reaction times]\n  A --&gt; H[Interval i.e., lower and upperbound \\n e.g., rating scale]\n  H --&gt; I[Centre on median value]\n  I --&gt; E(Response)\n  E --&gt; Z[Done]\n  F --&gt; G[Non-linear transformation \\n e.g., log-transform]\n  G --&gt; B(Predictor)\n  I --&gt; B(Predictor)\n  B --&gt; C{One predictor}\n  C --&gt; X[Centre]\n  D --&gt; Y[Centre and standardise]\n  B --&gt; D{Two predictor}\n  G --&gt; E(Response)\n  \n  \n\n\nFigure 5.1: Flowchart of common steps for linear and non-linear transformations of continuous variables. Such decision trees are not a one-size-fits-all solution and cannot replace critical thinking and understanding of your data.\nRecall that, when we have multiple continuous predictors, standardising them can help their interpretation, as their slopes are comparable. We could achieve this by centering each variable and then dividing by the standard deviation, or we could use the scale() function, which does just this.\n# centre and then standardize\ndf_freq_full |&gt; \n  mutate(\n         freq_z1 = (freq-mean(freq))/sd(freq),\n         freq_z2 = scale(freq)) |&gt; \n  select(freq_z1, freq_z2) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  freq_z1 freq_z2[,1]\n    &lt;dbl&gt;       &lt;dbl&gt;\n1 -0.0902     -0.0902\n2 -0.0864     -0.0864\n3 -0.0905     -0.0905\n4 -0.0864     -0.0864\n5 -0.0885     -0.0885\n6 -0.0901     -0.0901\nLet’s use scale() for freq and length.\ndf_freq_full &lt;-\n  df_freq_full |&gt; \n  mutate(freq_z = scale(freq_log),\n         length_z = scale(length))\nfit_freq_z &lt;-\n  lm(rt ~ freq_z + length_z, data = df_freq_full)\nFirst, let’s check the \\(R^2\\):\nglance(fit_freq_z)$r.squared\n\n[1] 0.4872977\nWe see that our \\(R^2\\) value is 0.4872977, just like above. This serves as a reminder that the predictors still represent the same variance in the underlying model, their units and scales have simply changed. What about our coefficients:\ntidy(fit_freq_z) |&gt; select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    770. \n2 freq_z         -60.6\n3 length_z        43.3\nHere, a 1-unit change always corresponds to a change of 1 standard deviation. Now we see that frequency has a larger magnitude than the effect of length. So, for each instease in frequency by 1 standard deviation (holiding length constant), reaction times decrease by 29.5 ms.\nWe’ve already discussed the assumptions of normality and homoscedasticity (constant variance), which both refer to the residuals of a model. We typically assess these assumptions visually, with histogram and Q-Q plots.\nToday we learned…\nToday we will learn…\nLoad in the english dataset from the languageR package (Baayen & Shafaei-Bajestan, 2019) (code below). You don’t need to load in any CSV file, because this dataset is available if you have the package loaded. From the manual:\n# load in 'english' dataset from languageR\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  relocate(word)\nWe’re keeping five variables:\nTake the following steps:"
  },
  {
    "objectID": "04-multiple_regression.html#load-data",
    "href": "04-multiple_regression.html#load-data",
    "title": "4  Multiple Regression",
    "section": "Load data",
    "text": "Load data\nWe’ll use the full dataset of the frequency data.\n\ndf_freq_full &lt;-\n  read_csv(here(\"data\", \"ELP_full_length_frequency.csv\")) |&gt; \n  clean_names() |&gt; \n  mutate(freq = 10^(log10freq), # inverse log10\n         freq_log = log(freq)) |&gt;  # use natural logarithm\n  relocate(word, rt, length, freq, freq_log)\n\nWe have 4 variables:\n\nword\nlength\nrt\nfreq\nfreq_log\nlog10freq"
  },
  {
    "objectID": "04-multiple_regression.html#one-predictor",
    "href": "04-multiple_regression.html#one-predictor",
    "title": "4  Multiple Regression",
    "section": "5.1 One predictor",
    "text": "5.1 One predictor\nLet’s re-run our simple model with this dataset. Let’s keep reaction times in the raw milliseconds for now for interpretability.\n\nfit_freq_full &lt;-\n  lm(rt ~ log(freq), data = df_freq_full)\n\n\ntidy(fit_freq_full)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    907.      1.09       828.       0\n2 log(freq)      -37.5     0.262     -143.       0\n\n\nWe see there is a decrease in reaction times (-37.5 milliseconds) for a 1-unit increase in log frequency. Let’s look at the model fit using glance().\n\nglance(fit_freq_full)$r.squared\n\n[1] 0.3834186\n\n\nWe see that the R-squared is 0.383, meaning our model describes 38% of the variance in response times. We can’t be sure that this described variance is due solely to frequency, however. Our models only know what we tell them! Other effects that are correlated with frequency might be conflating the frequency effect, e.g., more frequent words tend to be shorter (zipf_1949?). Let’s expand our model to include word length Equation 5.3.\n\\[\ny = b_0 + b_1*log frequency + b_2*word length\n\\tag{5.3}\\]"
  },
  {
    "objectID": "04-multiple_regression.html#adding-a-predictor",
    "href": "04-multiple_regression.html#adding-a-predictor",
    "title": "4  Multiple Regression",
    "section": "5.2 Adding a predictor",
    "text": "5.2 Adding a predictor\nLet’s add length as a predictor to our model.\n\nfit_freq_mult &lt;-\n  lm(rt ~ log(freq) + length, data = df_freq_full)\n\n\ntidy(fit_freq_mult) |&gt; select(term, estimate)\n\n# A tibble: 3 × 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    748. \n2 log(freq)      -29.5\n3 length          19.5\n\n\nWe see that length is also a significant predictor of reaction times, with an increase in word length (+1 letter) corresponds to a 20ms increase in reaction times. Our intercept is also now 748ms, instead of 907ms. The 907ms intercept corresponds to the prediction for reaction times to a word with 0 log frequency and 0 word length, but this is not very interpretable. If we were to center both prdictors, the intercept would be the reaction time for a wrd with average frequency and average length.\nThe slope for log frequency has also changed: from -37.5 to -29.5. This change tells us that some of the effect in our first model was confounded with length, as controlling for length weakens the effect of frequency.\n\nglance(fit_freq_mult)$r.squared\n\n[1] 0.4872977\n\n\nWe also see that including length increases the variance described by our model, reflected in the R-squared values (0.4872977 instead of 0.3834186."
  },
  {
    "objectID": "04-multiple_regression.html#adding-an-interaction-term",
    "href": "04-multiple_regression.html#adding-an-interaction-term",
    "title": "4  Multiple Regression",
    "section": "6.1 Adding an interaction term",
    "text": "6.1 Adding an interaction term\nWe won’t spent much time talking about interactions, but please check out Ch. 8 (Interations and nonlinear effects) in Winter (2019) for a more in-depth treatment. For now, what’s important to know is that interactions describe how effects of one predictor may be influenced by changes in another predictor. We can add interactin terms of two predictors by connecting them with a colon (:).\n\nlm(rt ~ freq_z + length_z + freq_z:length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nOr, we can simply connect the two predictors with an asterisk (*) to indicate that we want to look at both predictors and their interaction.\n\nlm(rt ~ freq_z*length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nThe model estimates are the same for both models. The intercept is the predicted reaction time for a word with the mean length and mean frequency. Notice that the interaction slope is negative, meaning when both freq and length increase, reaction times will decrease."
  },
  {
    "objectID": "04-multiple_regression.html#normality-and-homoscedasticity",
    "href": "04-multiple_regression.html#normality-and-homoscedasticity",
    "title": "4  Multiple Regression",
    "section": "7.1 Normality and Homoscedasticity",
    "text": "7.1 Normality and Homoscedasticity\nFor our model\n\nfig_hist &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nThe histogram looks approximately normally distributed, with a bit of a positive skew. The Q-Q plot suggests a less-normal distribution, with the model estimates fitting larger reaction times more poorly. The residual plot also shows that the variance of the residuals is not constant, with much larger residual variance for larger fitted values. This tells us we should probably log reaction times. Let’s try it all again, with log-transformed reaction times."
  },
  {
    "objectID": "04-multiple_regression.html#log-transformed-response-variable",
    "href": "04-multiple_regression.html#log-transformed-response-variable",
    "title": "4  Multiple Regression",
    "section": "7.2 Log-transformed response variable",
    "text": "7.2 Log-transformed response variable\n\nfit_freq_log_z &lt;-\n  lm(log(rt) ~ freq_z*length_z,\n     data = df_freq_full)\n\n\nglance(fit_freq_log_z)$r.squared\n\n[1] 0.5176913\n\n\n\ntidy(fit_freq_log_z) |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.63   \n2 freq_z          -0.0826 \n3 length_z         0.0524 \n4 freq_z:length_z -0.00779\n\n\nWe see now that our values are much smaller, because they’re on the log-scale.\n\nexp(6.63 + -0.0826*5 + 0.0524*2)\n\n[1] 556.5739\n\nexp(6.63 + -0.0826*4 + 0.0524*2)\n\n[1] 604.499\n\nexp(6.63 + -0.0826*1 + 0.0524*6)\n\n[1] 955.0847\n\ntidy(fit_freq_log_z)\n\n# A tibble: 4 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      6.63     0.000636   10428.  0       \n2 freq_z          -0.0826   0.000666    -124.  0       \n3 length_z         0.0524   0.000649      80.7 0       \n4 freq_z:length_z -0.00779  0.000581     -13.4 8.51e-41\n\n\n\nfig_hist &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nLooks much better."
  },
  {
    "objectID": "04-multiple_regression.html#collinearity",
    "href": "04-multiple_regression.html#collinearity",
    "title": "4  Multiple Regression",
    "section": "7.3 Collinearity",
    "text": "7.3 Collinearity\nCollinearity refers to when continuous predictor variables are correlated, which can make the interpretation of their coefficients difficult, and the results spurious. Regression assumes there is an absence of collinearity, i.e., our predictor variables are not correlatded.\nTo assess collinearity, you can use the vif() function from the car package to compare variance inflation factors. VIF values close to 1 indicates there is not a high degree of collinearity between your variables.\n\ncar::vif(fit_freq_log_z)\n\n         freq_z        length_z freq_z:length_z \n       1.246509        1.184641        1.068283 \n\n\nCollinearity is a conceptual problem, and is something that you need to consider in the planning stage. Typically, we want to include predictors that we have specific predictions or research questions about. Shoving a bunch of predictors in a model to see what comes out significant is bad practice. Rather, we should have a principled approach to model building and variable selection. This is not to say that exploratory analyses should be avoided, but that this comes with caveats."
  },
  {
    "objectID": "04-multiple_regression.html#adjusted-r2",
    "href": "04-multiple_regression.html#adjusted-r2",
    "title": "4  Multiple Regression",
    "section": "7.4 Adjusted \\(R^2\\)",
    "text": "7.4 Adjusted \\(R^2\\)\nAlthough we should avoid throwing any old predictor into our model, adjusted \\(R^2\\) is a more conservative version of \\(R^2\\) that takes into account the number of predictors in a model. For each additional predictor, adjusted \\(R^2\\) includes the number of predictors (\\(k\\)) in its denominator (bottom half of a division), which means that the more predictors there are, the smaller \\(R^2\\) will be, unless each additional predictor explains sufficient variance to counteract this penalisation.\n\nglance(fit_freq_log_z)$adj.r.squared\n\n[1] 0.5176475\n\n\nIf we were to look at the (adjusted) \\(R^2\\) of our simple linear regression model, where log reaction times are predicted by standardised log frequency, we see that there is a large increase in our model which includes length and its interaction. This suggests that our model is not overfit, and that length contributes to the variance explained by the model.\n\nglance(lm(log(rt) ~ freq_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.4148675\n\n\nIf we likewise compare to the same model without an interaction term (log reaction times ~ frequency * length), we see that the adjusted \\(R^2\\) is not very different. If the adjusted \\(R^2\\) were much lower, this would indicate that including the interaction term leads to overfitting.\n\nglance(lm(log(rt) ~ freq_z + length_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.5150461"
  },
  {
    "objectID": "04-multiple_regression.html#important-terms",
    "href": "04-multiple_regression.html#important-terms",
    "title": "4  Multiple Regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    NA\nNA\nNA"
  },
  {
    "objectID": "04-multiple_regression.html#exploratory-data-analysis",
    "href": "04-multiple_regression.html#exploratory-data-analysis",
    "title": "4  Multiple Regression",
    "section": "8.1 Exploratory data analysis",
    "text": "8.1 Exploratory data analysis\nLet’s start with a summary of the data.\n\n\n      word        rt_lexdec       rt_naming     length_in_letters age_subject \n arm    :   4   Min.   :6.205   Min.   :6.022   Min.   :2.000     old  :2284  \n barge  :   4   1st Qu.:6.426   1st Qu.:6.149   1st Qu.:4.000     young:2284  \n bark   :   4   Median :6.550   Median :6.342   Median :4.000                 \n bear   :   4   Mean   :6.550   Mean   :6.323   Mean   :4.342                 \n beef   :   4   3rd Qu.:6.653   3rd Qu.:6.490   3rd Qu.:5.000                 \n bind   :   4   Max.   :7.188   Max.   :6.696   Max.   :7.000                 \n (Other):4544                                                                 \n  freq_written   \n Min.   : 0.000  \n 1st Qu.: 3.761  \n Median : 4.832  \n Mean   : 5.021  \n 3rd Qu.: 6.247  \n Max.   :11.357  \n                 \n\n\nRemember that word is a factor, i.e., categorical variable. The numbers beside the words indicate how many observations (i.e., rows) there are per word. To see how many words have how many observations, we can use the count() function from the dplyr package, and then pipe to count(n).\n\ndf_freq_eng |&gt; \n  count(word) |&gt; \n  count(n)\n\n  n   nn\n1 2 2110\n2 4   87\n\n\nThis tells us that there are 2110 words that have 2 observations, and 87 that have 4 observations.\nNow let’s look at the distribution of our raw reaction time data (which is already logged, so we feed it into exp()). We’ll produce histograms and density plots of the raw and log reaction times.\n\n\n\n\n\nThis looks like a bimodal distribution, i.e., there are two modes (most frequent value, i.e., peak in a histogram). What might be driving this? We know that there were two subject groups: old and young. How does the distribution of these two groups look?\n\n\n\n\n\n\n\n\n\n\n\ndf_freq_eng |&gt; \n  ggplot() +\n  aes(x = freq_written, y = rt_lexdec, colour = age_subject, shape = age_subject) +\n  geom_point(alpha = .5)"
  },
  {
    "objectID": "04-multiple_regression.html#model",
    "href": "04-multiple_regression.html#model",
    "title": "4  Multiple Regression",
    "section": "8.2 Model",
    "text": "8.2 Model\nNow let’s run our model, ignoring age_subject for now.\n\nfit_freq_eng &lt;-\n  lm(rt_lexdec ~ freq_written, data = df_freq_eng)\n\n\nsummary(fit_freq_eng)\n\n\nCall:\nlm(formula = rt_lexdec ~ freq_written, data = df_freq_eng)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.45708 -0.11657 -0.00109  0.10403  0.56085 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   6.735931   0.006067 1110.19 &lt;0.0000000000000002 ***\nfreq_written -0.037010   0.001134  -32.63 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1413 on 4566 degrees of freedom\nMultiple R-squared:  0.1891,    Adjusted R-squared:  0.1889 \nF-statistic:  1065 on 1 and 4566 DF,  p-value: &lt; 0.00000000000000022\n\n\nLet’s check our model fit.\n\nperformance::check_model(fit_freq_eng)\n\n\n\n\n\ntidy(fit_freq_eng)\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    6.74     0.00607    1110.  0        \n2 freq_written  -0.0370   0.00113     -32.6 4.46e-210\n\n\n\nglance(fit_freq_eng)$r.squared\n\n[1] 0.1890641"
  },
  {
    "objectID": "05-categorical_predictors.html",
    "href": "05-categorical_predictors.html",
    "title": "5  Categorical predictors",
    "section": "",
    "text": "Learning Objectives\nToday we will learn…\n# suppress scientific notation\noptions(scipen=999)\nWe’ll also need to load in our required packages.\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\nIn linguistic research we often want to compare the effect of groups or categories, such as native or non-native speakers, or grammatical or ungrammatical stimuli. We might expect longer reading times for non-native (compared to native) speakers of a language, or for ungrammatical (versus grammatical) sentences. With our current dataset, we’d predict longer reading times for older participants than younger participants (although we should hypothesise before collecting and visualising our data!). How might these age effects interact with effects of word frequency and length?\nTreatment/dummy coding is the default contrast coding scheme. Sum coding is another frequently used coding scheme, which is essentially centring categorical variables. Just as with continuous variables, the motivation for sum contrast coding mainly lies in the interpretation of interaction effects. How can we tell R we want to use sum contrast coding, and not dummy coding? There are different ways to do this:\n# first, make sure your variable is a factor\ndf_freq_eng$age_subject &lt;- as.factor(df_freq_eng$age_subject)\n# check\nclass(df_freq_eng$age_subject)\n\n[1] \"factor\"\n# next, you could use the contr.sum() function\ncontrasts(df_freq_eng$age_subject) &lt;- contr.sum(2) # where 2 means we have 2 levels\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold      1\nyoung   -1\nHere we see that old is coded as \\(-1\\) and young as \\(+1\\). I prefer to use +/-0.5 for reasons we don’t need to go into here. I would also prefer to have young coded in the negative value, and old in the positive value. This aids in the way I interpret the slope: a change in reaction times for the older group compared to the younger group.\n#or, you could manually control the sum contrasts\n## check the order of the levels\nlevels(df_freq_eng$age_subject)\n\n[1] \"old\"   \"young\"\n\n## code 'old' as +.5 and 'young' as -.5\ncontrasts(df_freq_eng$age_subject) &lt;- c(+0.5, -0.5)\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold    0.5\nyoung -0.5\nYou could also choose to store the contrast values in their own variable.\ndf_freq_eng &lt;- \n  df_freq_eng |&gt; \n  mutate(age_numeric = ifelse(age_subject == \"young\", -0.5, +0.5))\ndf_freq_eng |&gt; \n  select(age_subject, age_numeric) |&gt; \n  head()\n\n     age_subject age_numeric\n338        young        -0.5\n1790         old         0.5\n3125       young        -0.5\n3957         old         0.5\n3313       young        -0.5\n4145         old         0.5\nNow, we can run our model using either age_subject or age_numeric.\nfit_age_sum &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\nglance(fit_age_sum)$r.squared\n\n[1] 0.4682224\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\nNo difference in variance account for by our model.\ntidy(fit_age_sum) |&gt; select(term,estimate)\n\n# A tibble: 2 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 (Intercept)      708.\n2 age_subject1     157.\nBut there is a difference in the intercept, and a change in sign in our slope. Why is this?\nfig_sum1 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-1,1)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_sum5 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-.5,.5)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment + fig_sum5 + plot_annotation(tag_levels = \"A\")\n\n\n\n\nFigure 7.1: The difference in slope corresponds to which level is coded as 0 (dummy coding) or -5/-1 (sum coding)\nAs we see in Figure 7.1, the sign of the slope depends on how we’ve contrast coded our factor levels. In Figure 7.1 A, the old group is coded as \\(0\\) and young as \\(1\\). In Figure 7.1 B, the young group is coded as \\(-.5\\) and the old group as \\(+.5\\).\nThe intercept value is also now the overall mean of all observed reaction times, because now the \\(y\\) value when \\(x\\) equals zero lies in the middle of the two groups. The slope magnitude (i.e., size of the value) hasn’t changed, because the difference betwen the two group means has not changed.\nmean(exp(df_freq_eng$rt_lexdec))\n\n[1] 708.1336\nToday we learned…"
  },
  {
    "objectID": "05-categorical_predictors.html#load-data",
    "href": "05-categorical_predictors.html#load-data",
    "title": "5  Categorical predictors",
    "section": "Load data",
    "text": "Load data\nLet’s continue working with the english dataset from the languageR package. Let’s just call it df_freq_eng.\n\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  # standardize continuous predictors\n  mutate(\n    freq_z = scale(freq_written),\n    length_z = scale(length_in_letters)\n  ) |&gt; \n  relocate(word) |&gt; \n  arrange(word)\n\nIn your exploratory data analysis, you might’ve noticed a bimodal distribution.\n\n\n\n\n\nThis looks like a bimodal distribution, i.e., there are two modes (most frequent value, i.e., peak in a histogram). What might be driving this? We know that there were two subject groups: old and young. How does the distribution of these two groups look?\nRunning our model of the log reaction times as predicted by frequency and length, we see:\n\nfit_freq_length &lt;-\n  lm(rt_lexdec ~ freq_z*length_z,\n     data = df_freq_eng)\n\n\nglance(fit_freq_length)$r.squared\n\n[1] 0.1896649\n\nglance(fit_freq_length)$adj.r.squared\n\n[1] 0.1891323\n\n\nSeems like we don’t have any overfitting in our model (\\(R^2\\) and adjusted \\(R^2\\) are comparable). Let’s look at our coeffiecients.\n\ntidy(fit_freq_length) |&gt; select(term, estimate)\n\n# A tibble: 4 × 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.55   \n2 freq_z          -0.0682 \n3 length_z         0.00328\n4 freq_z:length_z -0.00196\n\n\nThere is a negative slope for frequency, indicating shorter reaction times for words with higher frequency (when holding length constant). There is a positive slope for length, indicating longer reaction times for longer words (holding frequency constant). There is also a negative interaction estimate, indicating that when both length and frequency increase, reaction times decrease. This seems similar to the dataset we explored in the previous sections. But, this bimodal distribution is suggesting we should include age group as a predictor, since the two groups seem to pattern differently in their reading times. Could it be that the effect of frequency and length also differ as a function of age group?"
  },
  {
    "objectID": "05-categorical_predictors.html#including-a-categorical-predictor",
    "href": "05-categorical_predictors.html#including-a-categorical-predictor",
    "title": "5  Categorical predictors",
    "section": "6.1 Including a categorical predictor",
    "text": "6.1 Including a categorical predictor\nWhat would happen if we just include age_subject in our model?\n\nfit_age &lt;-\n  lm(rt_lexdec ~ freq_z*length_z + age_subject,\n     data = df_freq_eng)\n\nFirst, we see that adding age to our model results in a large increase in variance explained, and that the \\(R^2\\) and adjusted \\(R^2\\) values are comparable. In addition, the VIF values for all coefficients are near 1. This indicates that our predictors all contribute to the variance explained by the model and are not correlated.\n\nglance(fit_age)$r.squared\n\n[1] 0.6888949\n\nglance(fit_age)$adj.r.squared\n\n[1] 0.6886222\n\n\n\ncar::vif(fit_age)\n\n         freq_z        length_z     age_subject freq_z:length_z \n       1.012553        1.004461        1.000000        1.008108 \n\n\nNow that we see that our model is not overfit and that our predictors are not correlatd, let’s take a look at our model estimates.\n\ntidy(fit_age) |&gt; select(term,estimate)\n\n# A tibble: 5 × 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)       6.66   \n2 freq_z           -0.0682 \n3 length_z          0.00328\n4 age_subjectyoung -0.222  \n5 freq_z:length_z  -0.00196\n\n\nIn addition to the effects we observed in our earlier model, we see that there is a negative slope for age_subjectyoung, indicating that reaction times decrease when…what? How do we interpret a slope for a categorical variable? Regression works with numerical values, so how does a categorical variable get fit to a line? If we feed a categorical variable into the lm() function, the factor levels (i.e., the categories in a categorical variable) are given numerical values. We need to know what these values are in order to know how to interpret our model estimates. We call these numerical values mapped onto factor levels contrast coding, and we can check the contrasts of a given factor using the function contrasts().\n\ncontrasts(df_freq_eng$age_subject)\n\n      young\nold       0\nyoung     1\n\n\nWe see that old was coded at \\(0\\) and young as \\(1\\). This means that our slope for age_subjectyoung represents the change in reaction times when we move from old to young, which corresponds to a 1-unit change in our predictor (because the difference between 0 and 1 is 1). This is called treatment coding, or dummy coding, where one factor level is coded as 0 and the other as 1. Let’s remove the continuous variable for now and focus on age_subject. Let’s also look at raw reaction times, to more easily interpret the results.\n\nfit_age &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\n\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\n\n\nOur \\(R^2\\) value is lower than when we included frequency and length, but higher still than our model with frequeny and length but no age.\n\ntidy(fit_age) |&gt; select(term, estimate)\n\n# A tibble: 2 × 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)          787.\n2 age_subjectyoung    -157.\n\n\nWe see that there is an estimated decrease in reaction times of 157ms for the young group compared to the old group. But what does the intercept represent here? Let’s look at our data again.\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summary()\n\n   rt_lexdec      age_subject \n Min.   : 495.4   old  :2284  \n 1st Qu.: 617.4   young:2284  \n Median : 699.6               \n Mean   : 708.1               \n 3rd Qu.: 775.3               \n Max.   :1323.2               \n\n\nAnd how does rt_lexdec differ between the groups?\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summarise(mean = mean(rt_lexdec),\n            min = min(rt_lexdec),\n            max = max(rt_lexdec),\n    .by = \"age_subject\"\n  )\n\n  age_subject     mean    min    max\n1       young 629.5473 495.38  971.8\n2         old 786.7200 603.77 1323.2\n\n\nWe see here that the intercept for our model actually corresponds to the mean reaction time for the old group. Why is this? Recall that the intercept corresponds to the \\(y\\) value (reaction time) when \\(x\\) is \\(0\\). In treatment/dummy coding, one factor level is coded as \\(0\\). In our case this was old, and so the intercept corresponds to the mean reaction time for participants in the old group. How does R choose which variable to code as \\(0\\)? It simply takes the first level name alphabetically: old comes before young, so old was automatically taken as the ‘baseline’ to which young was compared.\nAnd if we were to add the slope to the intercept, we would get the mean for the \\(young\\) group. Why is this?\n\ncoef(fit_age)['(Intercept)'] + coef(fit_age)['age_subjectyoung']\n\n(Intercept) \n   629.5473 \n\n\nWhy are the means for the two groups used? The mean is the value closest to all values in a univariate dataset, and regression aims to inimise residuals (recall the line of best fit). So, a line is fit between the means of these two factor levels to achieve minimal residuals. This actually is the same thing as a t-test:\n\nt.test(exp(rt_lexdec) ~ age_subject, data = df_freq_eng)\n\n\n    Welch Two Sample t-test\n\ndata:  exp(rt_lexdec) by age_subject\nt = 63.406, df = 4144.6, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group old and group young is not equal to 0\n95 percent confidence interval:\n 152.3128 162.0325\nsample estimates:\n  mean in group old mean in group young \n           786.7200            629.5473 \n\n\nIf we compare this to our model, we see that the t- and p-values are identical (more on these later).\n\ntidy(fit_age)\n\n# A tibble: 2 × 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          787.      1.75     449.        0\n2 age_subjectyoung    -157.      2.48     -63.4       0\n\n\n\nfig_nocontrasts &lt;-\ndf_freq_eng |&gt; \n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"No contrasts\") +\n  # geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +  \n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",1,0)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Treatment contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_nocontrasts + fig_treatment"
  },
  {
    "objectID": "05-categorical_predictors.html#exploring-predicted-values",
    "href": "05-categorical_predictors.html#exploring-predicted-values",
    "title": "5  Categorical predictors",
    "section": "7.1 Exploring predicted values",
    "text": "7.1 Exploring predicted values\nLet’s also explore the predicted values of our model with a categorical variable.\n\nhead(fitted(fit_age), n = 10)\n\n     338     1790     3125     3957     3313     4145      337     1789 \n629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 \n    3513     4345 \n629.5473 786.7200 \n\n\nWe see that there are only 2 values, 630 and 787. These correspond to the means for each group that we saw above. They also seem to be in a pattern: young-mean, old-mean, young-mean, old-mean, etc. How does this correspond to the age group of the participant for the first ten observations?\n\nhead(df_freq_eng$age_subject, n = 10)\n\n [1] young old   young old   young old   young old   young old  \nattr(,\"contrasts\")\n      [,1]\nold    0.5\nyoung -0.5\nLevels: old young\n\n\nThe first ten observations in our data are in young-old pairs. What are the first values in the raw data?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10)\n\n [1] 623.61 775.67 617.10 715.52 575.70 742.19 592.42 748.37 541.67 824.76\n\n\nAnd what is the difference between these reaction times and the fitted values?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10) - head(fitted(fit_age), n = 10)\n\n       338       1790       3125       3957       3313       4145        337 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991 -37.127299 \n      1789       3513       4345 \n-38.349991 -87.877299  38.040009 \n\n\n\nhead(residuals(fit_age))\n\n       338       1790       3125       3957       3313       4145 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991"
  },
  {
    "objectID": "05-categorical_predictors.html#important-terms",
    "href": "05-categorical_predictors.html#important-terms",
    "title": "5  Categorical predictors",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\nterm\ndescription/other terms"
  },
  {
    "objectID": "06-logistic_regression.html",
    "href": "06-logistic_regression.html",
    "title": "6  Logistic regression",
    "section": "",
    "text": "Learning Objectives\nToday we will learn…\n# suppress scientific notation\noptions(scipen=999)\nWe’ll also need to load in our required packages. Hopefully you’ve already install the required packages (if not, go to Chapter 3).\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\nToday we learned…"
  },
  {
    "objectID": "06-logistic_regression.html#important-terms",
    "href": "06-logistic_regression.html#important-terms",
    "title": "6  Logistic regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\nterm\ndescription/other terms"
  }
]