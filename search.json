[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression for Linguists",
    "section": "",
    "text": "Course overview\nThis course fast tracks through different types of regression most relevant to linguistic research. These materials are by no means exhaustive, and should be supplemented by reading textbook length treatments. The majority of my materials lean heavily on Winter (2019), which I highly recommend. I also took inspiration from Sonderegger (2023), which came out this year and I haven‚Äôt fully explored yet. So far, it looks like a very thorough textbook that I would also recommend you check out.\nBefore you begin the course, I would like to paraphrase something Prof.¬†Shravan Vasishth said in the opening remarks for the annual summer school for Statistcal Methods for Linguistics and Psychology back in 2020 which has stuck with me: get comfortable with partial knowledge. We are not trained statisticians, and likely never will be (Vasishth himself is a certified statistician, in addition to professor of psycholinguistics). So get comfortable with partial understanding of the math behind these models, and focus on their application and interpretation."
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Regression for Linguists",
    "section": "Materials",
    "text": "Materials\nThis website is a work-in-progress. Materials will be updated/brushed up throughout the semester, with the binding course materials available on the course Moodle for those enrolled in the winter semester 2023/24.\nThis website was created to be viewed in HTML format. The accompanying (PDF) book version can be accessed by clicking on the PDF icon at the top right, but is not optimally formatted. Tables formatted for HTML output are particularly oddly formatted in PDF, as is the order of printed elements in relation to their accompanying text. For this reason, I strongly encourse to follow the web book.\n\n\n\n\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "00-course_overview.html",
    "href": "00-course_overview.html",
    "title": "Resources and Set-up",
    "section": "",
    "text": "Resources\nThis course is mainly based on Winter (2019), which is an excellent introduction into regression for linguists. For even more introductory tutorials, I recommend going through Winter (2013) and Winter (2014) For a more intermediate textbook, I‚Äôd recommend Sonderegger (2023).\nIf you‚Äôre interested in the foundational writings on the topic of (frequentist) linear mixed models in (psycho)linguistic research, I‚Äôd recommend reading Baayen (2008); Baayen et al. (2008);Barr et al. (2013); Bates et al. (2015); Jaeger (2008); Matuschek et al. (2017); Vasishth (2022); Vasishth & Nicenboim (2016).\nFor this course, I assume that you are familiar with more classical statistical tests, such as the t-test, Chi-square test, etc. I also assume you are familiar with measures of central tendency (mean, median, mode) measures dispersion/spread (standard deviation), and with the concept of a normal distribution. Lacking this knowledge will not impeded your progress in the course, but is an important foundation on which we‚Äôll be building. We can review these concepts in-class as needed."
  },
  {
    "objectID": "00-course_overview.html#install-r",
    "href": "00-course_overview.html#install-r",
    "title": "Resources and Set-up",
    "section": "Install R",
    "text": "Install R\n\nwe need the free and open source statistical software R to analyze our data\ndownload and install R: https://www.r-project.org"
  },
  {
    "objectID": "00-course_overview.html#install-rstudio",
    "href": "00-course_overview.html#install-rstudio",
    "title": "Resources and Set-up",
    "section": "Install RStudio",
    "text": "Install RStudio\n\nwe need RStudio to work with R more easily\nDownload and install RStudio: https://rstudio.com\nit can be helpful to keep English as language in RStudio\n\nwe will find more helpful information if we search error messages in English on the internet\n\nIf you have problems installing R or RStudio, check out this help page (in German): http://methods-berlin.com/wp-content/uploads/Installation.html"
  },
  {
    "objectID": "00-course_overview.html#install-latex",
    "href": "00-course_overview.html#install-latex",
    "title": "Resources and Set-up",
    "section": "Install LaTeX",
    "text": "Install LaTeX\n\nwe will not work with LaTeX directly, but it is needed in the background\nDownload and install LaTeX: https://www.latex-project.org/get/"
  },
  {
    "objectID": "00-course_overview.html#troubleshooting-en-troubleshooting",
    "href": "00-course_overview.html#troubleshooting-en-troubleshooting",
    "title": "Resources and Set-up",
    "section": "Troubleshooting (EN: Troubleshooting)",
    "text": "Troubleshooting (EN: Troubleshooting)\n\nError messages are very common in programming, at all levels.\nHow to find solutions for these error messages is an art in itself\nGoogle is your friend! If possible, google in English to get more information"
  },
  {
    "objectID": "00-course_overview.html#session-information",
    "href": "00-course_overview.html#session-information",
    "title": "Resources and Set-up",
    "section": "Session Information",
    "text": "Session Information\nThe current version of this Quarto book was developed using R version 4.4.0 (2024-04-24) (Puppy Cup) in RStudioversion 2023.3.0.386 (Cherry Blossom). At the bottom of each chapter is a list of the packages (and version info) used in that chapter (under Session Information). I highly recommend you do the same at the bottom of each script that you write. You can easily do this by writing the following at the bottom of any Rmarkdown (.Rmd) or Quarto (.qmd) script:\n# Session Info\n\n```{r}\nsessionInfo()\n```"
  },
  {
    "objectID": "01-equation_of_a_line.html#learning-objectives",
    "href": "01-equation_of_a_line.html#learning-objectives",
    "title": "1¬† Understanding straight lines",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nthe equation of a line\nabout intercepts, slopes, and residuals"
  },
  {
    "objectID": "01-equation_of_a_line.html#resources",
    "href": "01-equation_of_a_line.html#resources",
    "title": "1¬† Understanding straight lines",
    "section": "Resources",
    "text": "Resources\nThis lecture is based on the readings for today‚Äôs session: Winter (2013) and Winter (2019) (Ch. 3), and to a lesser extent (debruine_understanding_2021?); Winter (2014)."
  },
  {
    "objectID": "01-equation_of_a_line.html#when-to-model-your-data",
    "href": "01-equation_of_a_line.html#when-to-model-your-data",
    "title": "1¬† Understanding straight lines",
    "section": "1.1 When to model your data",
    "text": "1.1 When to model your data\nBy the time we get to the point of wanting to model our data, we should have a pretty good idea of how our data look. We achieve this through running an exploratory data analysis (EDA), which consists of visualising your data and determining outliers (a question for another day: what is an outlier?), generating summary (i.e., descriptive) statistics, and just overall getting to know your data, without making any claims beyond your data.\nHowever, an understanding of the data design and collection procedure is incredibly important and is necessary in order to appropriately fit a model to our data. In fact, planning out your analyses when designing your experiment is highly recommended in order to ensure your data will have the appropriate structure and that the assumptions made by your chosen analyses are taken into consideration before data collection.\nThe next step after conducting an EDA is to model your data, i.e., run inferential statistics, this is where we try to generalise beyond our data.\n\n1.1.1 Statistical tests versus models\nMany statistical courses and textbooks still put undue emphasis on classical statistical tests. However, these common statistical tests are simplified linear models, without the added benefits of linear models. In essence, statistical tests tell us something about our data, whereas statistical models can be used to make predictions about hypothetical future observations."
  },
  {
    "objectID": "01-equation_of_a_line.html#linear-regression",
    "href": "01-equation_of_a_line.html#linear-regression",
    "title": "1¬† Understanding straight lines",
    "section": "1.2 (Linear) Regression",
    "text": "1.2 (Linear) Regression\nData exploration gives us an idea about what our data look like, but if we want to be able to make predictions about hypothetical observations, i.e., to predict values of our DV based on one (or more) IV(s), we need to fit a model to our data. This model can then predict values of our DV based on one (or more) IV(s), i.e., predicting an outcome variable (dependent variable, DV) from one or more predictors (independent variable, IV). Because we‚Äôre making predictions, we need to take into account the variability (i.e., error) in our data.\n\n1.2.1 Types of regression\n\n\n\n\n\nregression type\npredictor\noutcome\n\n\n\n\nsimple regression\nSingle predictor\ncontinuous (numerical)\n\n\nmultiple regression\nmultiple predictor\ncontinuous (numerical)\n\n\nhierarchical/linear mixed models/linear mixed effect models\ninclude random effect\ncontinuous (numerical)\n\n\ngeneralised linear (mixed) models: logistic regression\nas above\nbinary/binomial data\n\n\ngeneralised linear (mixed) models: poisson regression\nas above\ncount data"
  },
  {
    "objectID": "01-equation_of_a_line.html#sec-straight-lines",
    "href": "01-equation_of_a_line.html#sec-straight-lines",
    "title": "1¬† Understanding straight lines",
    "section": "1.3 Straight lines",
    "text": "1.3 Straight lines\n\nlinear regression summarises the data with a straight line\n\nwe model our data as/fit our data to a straight line\n\nstraight lines can be defined by\n\nIntercept (\\(b_0\\))\n\nvalue of \\(Y\\) when \\(X = 0\\)\n\nSlope (\\(b_1\\))\n\ngradient (slope) of the regression line\ndirection/strength of relationship between \\(x\\) and \\(y\\)\nregression coefficient for the predictor\n\n\nso we need to define an intercept and a slope\n\n\n1.3.1 A line = intercept and slope\n\na line is defined by its intercept and slope\n\nin a regression model, these two are called coefficients\n\n\n\n\n\n\n\nFigure¬†1.1: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nEquation of a line\n\n\n\n\\[\\begin{align}\ny & = mx + c\\\\\nY_i &= (b_0 + b_1X_i) \\\\\noutcome_i & = (model) \\\\\ny_i & = (intercept + slope*x_i)\n\\end{align}\\]\n\n\n\n\n1.3.2 Intercept (\\(b_0\\))\n\nthe value of \\(y\\) when \\(x = 0\\)\n\n\n\n\n\n\n\n\n1.3.3 Slopes (\\(b_1\\))\nA slope describes a change in \\(y\\) (\\(\\Delta y\\)) over a change in \\(x\\) (\\(\\Delta x\\)), where \\(\\Delta\\) (the Greek letter delta) can be read as ‚Äòdifference‚Äô. So a slope‚Äôs value equals the difference in \\(x\\) for a difference of 1 unit in \\(y\\). Positive slopes indicate that as \\(x\\) increases, \\(y\\) increases. A negative slope value indicates that as \\(x\\) increases, \\(y\\) decreases (or vice versa). A slope of 0 indicates there is no change in \\(y\\) as a function of \\(x\\), or: there is no change in \\(y\\) when the value of \\(x\\) changes.\n\\[\\begin{align}\nslope = \\frac{\\Delta y}{\\Delta x}\n\\end{align}\\]\nThis relationship between \\(x\\) and \\(y\\) is sometimes referred to as ‚Äúrise over run‚Äù: how do you ‚Äòrise‚Äô in \\(y\\) for a given ‚Äòrun‚Äô in \\(x\\)? For example, if we were to measure children‚Äôs heights and ages, we would expect to find an increase in height for every increase in age. Or, for a linguistic example, we would expect to find longer whole-sentence reading times (a measure variable) for longer texts: if a sentence has 9 words (I find straight lines to be really interesting and fun.), we would expect longer reading times than a sentence with 3 words (I love lines.).\n\n\nwhat is the intercept of this line?\nwhat is the slope of this line?"
  },
  {
    "objectID": "01-equation_of_a_line.html#error-and-residuals",
    "href": "01-equation_of_a_line.html#error-and-residuals",
    "title": "1¬† Understanding straight lines",
    "section": "1.4 Error and residuals",
    "text": "1.4 Error and residuals\n\nfixed effects (IV/predictors): things we can understand/measure\nerror (random effects): things we cannot understand/measure\n\nin biology, social sciences (and linguistic research), there will always sources of random error that we cannot account for\nrandom error is less an issue in e.g., physics (e.g., measuring gravitational pull)\n\nresiduals: the difference (vertical difference) between observed data and the fitted values (predicted values)\n\n\n\n\n\n\n\nEquation of a line\n\n\n\n\\[\\begin{align}\ny & = mx + c\\\\\nY_i &= (b_0 + b_1X_i) + \\epsilon_i\\\\\noutcome_i & = (model) + error_i\\\\\ny_i & = (intercept + slope*x_i) + error_i\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.4.1 Method of least squares\n\nso how is any given line chosen to fit any given data?\nthe method of least squares\n\ntake a given line, and square all the residuals (i.e., \\(residual^2\\))\nthe line with the lowest sum of squares is the line with the best fit to the given data\nwhy do we square the residuals before summing them up?\n\nso all values are positive (i.e., so that negative values don‚Äôt cancel out positive values)\n\n\nthis is how we find the line of best fit\n\nR fits many lines to find the one with the best fit\n\n\n\n\n\n\n\n\nFigure¬†1.2: Observed values (A), Residuals for line of best fit (B), A line of worse fit with larger residuals (C)"
  },
  {
    "objectID": "01-equation_of_a_line.html#learning-objectives-1",
    "href": "01-equation_of_a_line.html#learning-objectives-1",
    "title": "1¬† Understanding straight lines",
    "section": "Learning Objectives üèÅ",
    "text": "Learning Objectives üèÅ\nToday we learned‚Ä¶\n\nthe equation of a line\nabout intercepts, slopes, and residuals"
  },
  {
    "objectID": "01-equation_of_a_line.html#important-terms",
    "href": "01-equation_of_a_line.html#important-terms",
    "title": "1¬† Understanding straight lines",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Intercept\nValue of y for x=0\nb0\n    Slope\na change in x over a change in y; regression coefficient for the predictor. Positive as x increases, y increases. \nNegative slopes, as x increases y decreases.\nb1\n    residuals/error\ndifference between observed data and the fitted values\ntidy(model_name)$.resid\n    interaction term\nused to describe how effects of one predictor may be influenced by changes in another predictor\nlm(response ~ predictor1*predictor2, data = data)"
  },
  {
    "objectID": "01-equation_of_a_line.html#tasks",
    "href": "01-equation_of_a_line.html#tasks",
    "title": "1¬† Understanding straight lines",
    "section": "1.5 Tasks",
    "text": "1.5 Tasks\n\n1.5.1 Task 1: pen-and-paper\nYou will receive a piece of paper with several grids on it. Follow the instructions, which include drawing some lines. If you aren‚Äôt in-class, this is the paper we are using:\n\n\n1.5.2 Task 2: simulating data\nAll of the figures we just saw (except Figure¬†1.1, which is from Winter (2019)) were generated in R. Simulating data and plotting is a great way to understand concepts, or even to map out our hypotheses. Let‚Äôs use R for the first time to try to simulate some data in order to plot lines. Our goal will be to produce a line that has the following:\n\nintercept = 4.5\nslope = 3\n\n\n1.5.2.1 Planning\nFirst, think about what steps will be required to create such plots. Can you come up with a workflow plan (without peaking at the next tasks)?\n\n\n1.5.2.2 Producing our line\n\nx &lt;- c(0,1)\ny &lt;- c(4.5,3)\ndata &lt;- cbind(x,y) |&gt; as.data.frame()\nggplot(data = data) +\n  aes(x = x, y = y) +\n  geom_line() +\n  geom_point()"
  },
  {
    "objectID": "01-equation_of_a_line.html#session-info",
    "href": "01-equation_of_a_line.html#session-info",
    "title": "1¬† Understanding straight lines",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] googlesheets4_1.1.1 gt_0.10.1           kableExtra_1.4.0   \n [4] knitr_1.43          patchwork_1.2.0     broom_1.0.5        \n [7] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[10] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[13] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[16] tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      xfun_0.40         htmlwidgets_1.6.4 lattice_0.22-6   \n [5] gargle_1.5.2      tzdb_0.4.0        vctrs_0.6.5       tools_4.4.0      \n [9] generics_0.1.3    curl_5.2.1        fansi_1.0.6       highr_0.10       \n[13] pacman_0.5.1      pkgconfig_2.0.3   Matrix_1.7-0      lifecycle_1.0.4  \n[17] compiler_4.4.0    farver_2.1.1      munsell_0.5.1     sass_0.4.7       \n[21] htmltools_0.5.8.1 yaml_2.3.7        pillar_1.9.0      magick_2.8.3     \n[25] nlme_3.1-164      tidyselect_1.2.1  digest_0.6.33     stringi_1.8.3    \n[29] splines_4.4.0     labeling_0.4.3    rprojroot_2.0.4   fastmap_1.1.1    \n[33] grid_4.4.0        here_1.0.1        colorspace_2.1-0  cli_3.6.2        \n[37] magrittr_2.0.3    utf8_1.2.4        withr_3.0.0       scales_1.3.0     \n[41] backports_1.4.1   googledrive_2.1.1 timechange_0.3.0  httr_1.4.7       \n[45] rmarkdown_2.24    cellranger_1.1.0  hms_1.1.3         evaluate_0.21    \n[49] viridisLite_0.4.2 mgcv_1.9-1        rlang_1.1.3       Rcpp_1.0.12      \n[53] glue_1.7.0        xml2_1.3.6        renv_1.0.7        svglite_2.1.3    \n[57] rstudioapi_0.16.0 jsonlite_1.8.7    R6_2.5.1          systemfonts_1.0.6\n[61] fs_1.6.3"
  },
  {
    "objectID": "01-equation_of_a_line.html#literaturverzeichnis",
    "href": "01-equation_of_a_line.html#literaturverzeichnis",
    "title": "1¬† Understanding straight lines",
    "section": "Literaturverzeichnis",
    "text": "Literaturverzeichnis\n\n\nWinter, B. (2013). Linear models and linear mixed effects models in R: Tutorial 1.\n\n\nWinter, B. (2014). A very basic tutorial for performing linear mixed effects analyses (Tutorial 2).\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "02-simple_linear_regression.html#learning-objectives",
    "href": "02-simple_linear_regression.html#learning-objectives",
    "title": "2¬† Simple linear regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nhow to fit a simple linear model with the lm() function\nhow to interpret our model output"
  },
  {
    "objectID": "02-simple_linear_regression.html#set-up-environment",
    "href": "02-simple_linear_regression.html#set-up-environment",
    "title": "2¬† Simple linear regression",
    "section": "Set-up environment",
    "text": "Set-up environment\nMake sure you always start with a clean R Environment (Session &gt; Restart R). This means you should have no objects stored in your Environment, and no packages loaded. To ensure this, you can go to the Session tab (up where you‚Äôll find File, Help, etc.), and select Restart R. You can also use the keyboard shortcut Cmd/Ctrl+Shift+0 (that‚Äôs a zero, not an ‚Äòoh‚Äô).\nIn addition, I often prefer to run options(scipen=999) in order to supress scientific notation, which writes very large or very small numbers in an unintuitive way. For example, 0.000005 is written 5e-06 in scientific notation.\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe‚Äôll also need to load in our required packages. Hopefully you‚Äôve already install the required packages (if not, go to Chapter¬†3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)"
  },
  {
    "objectID": "02-simple_linear_regression.html#simple-linear-model-rt-frequency",
    "href": "02-simple_linear_regression.html#simple-linear-model-rt-frequency",
    "title": "2¬† Simple linear regression",
    "section": "2.1 Simple linear model: RT ~ frequency",
    "text": "2.1 Simple linear model: RT ~ frequency\nRecall that \\(y \\sim x\\) can be read as ‚Äúy as a function of x‚Äù, or ‚Äúy predicted by x‚Äù. Following Winter (2019), we will first model some word frequency data. In this experiment, Our first model is given in equation \\(\\ref{eq-rt}\\):\n\\[\\begin{equation}\nRT \\sim frequency \\label{eq-rt}\n\\end{equation}\\]\nLet‚Äôs load our data using the read_csv() function from readr. I also use the clean_names() function from the janitor package, which tidies up variable names (e.g., no spaces, all lower case).\n\n# load ELP_frequency.csv\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\n\n\n2.1.1 Mini-EDA\nLet‚Äôs explore the data a little bit, which is what we would normally do before fitting any models. First, let‚Äôs see how the data is structured.\n\n# print head of df_freq\nhead(df_freq)\n\n# A tibble: 6 √ó 3\n  word      freq    rt\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 thing    55522  622.\n2 life     40629  520.\n3 door     14895  507.\n4 angel     3992  637.\n5 beer      3850  587.\n6 disgrace   409  705 \n\n\nLooks like there are only 3 columns: word, freq, and rt. We can assume that they correspond to the word, its frequency, and the reaction time, respectively. We can also see in our global environment that there are 12 observations, meaning 12 rows.\nThe summary() function provides summaries of each variable in a dataframe. For numeric variables, it will provide descriptive statistics for the centre and spread of the data (mean, median, quartiles). For categorical data, it will provide the count per category. For character variables, simply lists the number of observations.\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5  \n\n\nWe see freq has a pretty big range, from 4 to 55522. rt has a range of 507.38 to 877.53, with an average reaction time of 679.9. Let‚Äôs now get an overview of the relationship between freq and rt.\n\nplot(df_freq$freq, df_freq$rt)\n\n\n\n\nWe see there are a lot of frequency values below roughly 400, and these seem to have higher reaction times than those with a higher frequency value. Let‚Äôs fit these data to our first linear model to explore this effect of frequency on reaction times.\n\n\n2.1.2 lm()\nThe the lm() function fits simple linear models. As arguments it takes a formula and a dataset, at minimum, as in equation \\(\\ref{eq-lm}\\).\n\\[\\begin{equation}\nlm(outcome \\sim 1 + predictor,\\;data\\;=\\;df\\_name) \\label{eq-lm}\n\\end{equation}\\]\nThe lm() function formula syntax can be read as: outcome predicted by the intercept (1 is a placeholder for the intercept) and predictor. The intercept is included by default, so if you omit the 1 the intercept is still included in the formula. If you wanted to remove the intercept (which you often don‚Äôt), you could replace 1 with 0.\n\n2.1.2.1 Running a model\nBefore we add our predictor freq, let‚Äôs see what our model looks like without it. We can write it as:\n\nlm(rt ~ 1, data = df_freq) \n\nBut it‚Äôs useful to save the model as an object so that we can call on it later. It‚Äôs often a good idea to have informative prefixes to your objects\n\nfit_rt_1 &lt;- lm(rt ~ 1, data = df_freq) \n\n\n\n\n\n\n\nObject naming\n\n\n\nYou may have wondered what the letters df are for when loading in our data set as df_freq. These letters stand for ‚Äòdata frame‚Äô, and serve as a reminder of what exactly that object in our environment is. We might also have wanted to plot the frequency data, in which case we could call save the plot as fig_freq or plot_freq. Here we are saving our model as fit_rt_1, using ‚Äòfit‚Äô to signal that this object is a model fit. You could also save it as mod_freq_1, lm_freq_1, or whatever you see fit. This simply helps keep our environment structured, which will become useful when you begin working with multiple datasets at a time.\n\n\n\n\n2.1.2.2 Model ouput\nNow that we‚Äôve saved our model in our Enrivonement, we can call it by name. Printing just the model gives us the formula and the coefficients.\n\n# print model\nfit_rt_1\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nCoefficients:\n(Intercept)  \n      679.9  \n\n\nRecall that the intercept and slope are called coefficients. Why do we only see Intercept? Because we didn‚Äôt include any predictors in our model. This output isn‚Äôt very dense, however. We typically use the summary() function to print full model outputs.\n\nsummary(fit_rt_1)\n\n\nCall:\nlm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-172.537  -74.677   -9.137   91.296  197.613 \n\nCoefficients:\n            Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 117.8 on 11 degrees of freedom\n\n\nWe see a lot more information here.\n\n\n\n\n\n\nbroom package\n\n\n\nThe broom package has some useful functions for printing model outputs\n\ntidy() produces a tibble (type of dataframe) of the coefficients\nglance() produces goodness of fit measures (which we won‚Äôt discuss)\n\nThe outputs from tidy() and glance() can be fed into kable and/or kable_styling() to create formatted tables\n\ntidy(fit_rt_1)\n\n# A tibble: 1 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     680.      34.0      20.0 5.38e-10\n\n\n\nglance(fit_rt_1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         0             0  118.        NA      NA    NA  -73.7  151.  152.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\naugment() adds model values as columns to your dataframe (e.g., useful for plotting observed vs.¬†fitted values).\n\naugment(fit_rt_1, data = df_freq) %&gt;% summary()"
  },
  {
    "objectID": "02-simple_linear_regression.html#interpreting-model-output",
    "href": "02-simple_linear_regression.html#interpreting-model-output",
    "title": "2¬† Simple linear regression",
    "section": "2.2 Interpreting model output",
    "text": "2.2 Interpreting model output\n\nlet‚Äôs take a closer look at our model summary\n\n\nsummary(fit_rt_1)\n\n\nCall:\n1lm(formula = rt ~ 1, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n2-172.537  -74.677   -9.137   91.296  197.613\n\nCoefficients:\n3            Estimate Std. Error t value       Pr(&gt;|t|)\n4(Intercept)   679.92      34.02   19.99 0.000000000538 ***\n---\n5Signif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\n6Residual standard error: 117.8 on 11 degrees of freedom\n\n\n1\n\nformula repetition\n\n2\n\nresiduals: differences between observed values and those predicted by the model\n\n3\n\nnames for columns Estimates, standard error, t-value, p-value (Pr(&gt;|t|))\n\n4\n\nIntercept (\\(b_0\\))\n\n5\n\nSignificance codes\n\n6\n\nR\\(^2\\), a measure of model fit (squared residuals); percentage of variance in the data shared with the predictor (higher numbers are better‚Ä¶this is pretty low)\n\n\n\n\n\n2.2.0.1 Intercept\nOur intercept is roughly 679.9 milliseconds; what does this number represent?\n\n# print model intercept?\ncoef(fit_rt_1)['(Intercept)']\n\n(Intercept) \n   679.9167 \n\n\n\n# print data mean\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nThe intercept corresponds to the mean reaction time value. Let‚Äôs explore this.\n\n2.2.0.1.1 Intercept significance\nIn the model output, the intercept seems to be significant (indicated with a low p-value, and ***). What does this mean? Significance pretty much tells us if a number is equal to (or not statistically significantly different from) 0. So this tells us that the intercept (i.e., the mean reaction time) is different from 0. How do we interpret this? In most cases we don‚Äôt. Whether or not the intercept is significantly different from 0 this isn‚Äôt interesting or even theoretically relevant, as reaction times shouldn‚Äôt be near 0, so neither should their mean. This is also true for formant frequencies, reading times, and other types of continuous linguistic data.\n\n\n\n2.2.0.2 Standard Error\nStandard error takes both the variability in our data and the sample size into account. The equation for standard error is:\n\\[\\begin{equation}\nSE = \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\label{eq-se}\n\\end{equation}\\]\nwhere \\(\\sigma\\) is the standard deviation, and \\(n\\) is the sample size. As a refresher, the equation for standard deviation (\\(\\ref{eq-sd}\\)) is the square root of the sum of all squared deviances from the mean (\\(\\sum^n_{i=1}(x_i - \\hat{\\mu})^2\\)) divided by the sample size -1. Don‚Äôt stress about the math for now, but it‚Äôs helpful to try to understand where there values come from and what they represent.\n\\[\\begin{equation}\n\\hat{\\sigma} = \\sqrt{\\frac{\\sum^n_{i=1}(x_i - \\hat{\\mu})^2}{n-1}} \\label{eq-sd}\n\\end{equation}\\]\n\n\n2.2.0.3 t-values\nSimple linear regression is equivalent to a t-test. The one-sample t-test corresponds to an intercept-only.\n\ndf_freq %&gt;% \nt.test(rt ~ 1, data = .)\n\n\n    One Sample t-test\n\ndata:  rt\nt = 19.988, df = 11, p-value = 0.000000000538\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 605.0461 754.7872\nsample estimates:\nmean of x \n 679.9167 \n\n\n\ndf_freq %&gt;% \nlm(rt ~ 1, data = .) %&gt;% \n  tidy() %&gt;%\n  mutate_if(is.numeric, round, 10)\n\n# A tibble: 1 √ó 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)     680.      34.0      20.0 0.0000000005\n\n\nThe real power of linear regression is coming tomorrow and in January‚Ä¶multiple regression and mixed models. But for now, it‚Äôs important to remember that the larger the t-value, the smaller the p-value. But more important is to not rely too heavily on p-values, as such black-and-white classifications have proven a poor substitute for understanding our data and our models.\n\n\n2.2.0.4 p-values\n\n\n\n\n\n\nA word on t-values and p-values\n\n\n\nt-values quantify the difference between population means.\np-values quantify the probability of obtaining a result equal to or greater than what was observed, given the assumption of no effect (the null hypothesis).\nIf the null hypothesis were true, we would expect no effect (a flat line). If we have a lot of evidence/are confidence that there is an effect (the line (slope) is in fact not flat), then it would be unlikely that we would find such a result under the assumption that there is no effect (the line actually is flat) i.e., the null hypothesis. This is reflected in a small p-value.\n\n\n\n\n2.2.0.5 Plotting rt ~ 1\n\nFigure¬†2.1 shows the intercept (red dot) amongst the observed data (black dots)\n\nalong the x-axis we have abstract numerical units (the values don‚Äôt mean anything)\nwhat would the values of the intercept be?\n\n\n\n\n\n\n\nFigure¬†2.1: Visualisation of ‚Äòrt ~ 1‚Äô: observed values (black) and mean (intercept; red). Residuals would be the distance from each black dot to the y-value of the read dot"
  },
  {
    "objectID": "02-simple_linear_regression.html#adding-a-fixed-effect-slope",
    "href": "02-simple_linear_regression.html#adding-a-fixed-effect-slope",
    "title": "2¬† Simple linear regression",
    "section": "2.3 Adding a fixed effect (slope)",
    "text": "2.3 Adding a fixed effect (slope)\nNow let‚Äôs include a predictor, which will give us a slope. The slope represents the change in \\(y\\) (DV: rt) when we move 1-unit along \\(y\\) (IV: freq). In other words, it tells us the effect our IV has on the DV. Let‚Äôs first plot the data:\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = freq, y = rt) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n2.3.1 Fit model (treatment contrasts)\n\n# fit simple linear model\nfit_rt_freq &lt;- lm(rt ~ freq, data = df_freq)\n\n\n2.3.1.1 Model summary\n\nsummary(fit_rt_freq)\n\n\nCall:\nlm(formula = rt ~ freq, data = df_freq)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-155.947  -73.141    2.117   85.050  163.837 \n\nCoefficients:\n              Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 713.706298  34.639105   20.60 0.0000000016 ***\nfreq         -0.003382   0.001699   -1.99       0.0746 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 104.6 on 10 degrees of freedom\nMultiple R-squared:  0.2838,    Adjusted R-squared:  0.2121 \nF-statistic: 3.962 on 1 and 10 DF,  p-value: 0.07457\n\n\n\n\n2.3.1.2 Intercept\nThe intercept in our last model was the mean reaction time. But now it‚Äôs a different value.\n\n# print model intercept\ncoef(fit_rt_freq)['(Intercept)']\n\n(Intercept) \n   713.7063 \n\n\n\n# print data mean\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nOur intercept is no longer the grand mean of first-pass reading times‚Ä¶what is it?\n\n\n2.3.1.3 Slope\nOur slope was our slope -0.0033823. What does this correspond to?\n\n# print slope\ncoef(fit_rt_freq)['freq']\n\n        freq \n-0.003382289 \n\n\nThis is the change in \\(y\\) (our DV rt) for a 1-unit change in \\(x\\) (our IV: freq). So when we move up 1 unit in frequency, reaction times decrease by -0.0033823. Whether or not it makes sense to consider this number depends on the measurement unit your data is in, e.g., a unit change from one millimeter or one meter will have a drastically different slope value (say, for age), but the actual slope will be the exact same.\n\nheights_m &lt;- c(1.71, 1.56, .9, 2.06, 1.63)\nheights_cm &lt;- c(171, 156, 90, 206, 163)\nheights_mm &lt;- c(1710, 1560, 900, 2060, 1630)\nyear &lt;- c(22,15,10,26,18)\nmonths &lt;- c(22,15,10,26,18)*12\ndays &lt;- c(22,15,10,26,18)*365\n\ndf_heights_age &lt;- cbind(year, months, days, heights_mm, heights_cm, heights_m) |&gt; as.data.frame() |&gt; \n  pivot_longer(\n    cols = c(heights_mm, heights_cm, heights_m),\n    names_to = \"unit\",\n    values_to = \"height\"\n  ) |&gt; \n  pivot_longer(\n    cols = c(year, months, days),\n    names_to = \"unit_age\",\n    values_to = \"age\"\n  )\n\n\n\n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ days)\n\n\nCall:\nlm(formula = heights_cm ~ days)\n\nCoefficients:\n(Intercept)         days  \n   39.66230      0.01769  \n\nlm(heights_m ~ months)\n\n\nCall:\nlm(formula = heights_m ~ months)\n\nCoefficients:\n(Intercept)       months  \n   0.396623     0.005382  \n\nlm(heights_mm ~ year)\n\n\nCall:\nlm(formula = heights_mm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     396.62        64.58  \n\nlm(heights_cm ~ year)\n\n\nCall:\nlm(formula = heights_cm ~ year)\n\nCoefficients:\n(Intercept)         year  \n     39.662        6.458  \n\nlm(heights_m ~ year)\n\n\nCall:\nlm(formula = heights_m ~ year)\n\nCoefficients:\n(Intercept)         year  \n    0.39662      0.06458  \n\n\n\nggplot(data = df_heights_age) +\n  aes(x = height, y = age) +\n  facet_wrap(unit ~ unit_age, scales = \"free\") +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) +\n  theme_bw()"
  },
  {
    "objectID": "02-simple_linear_regression.html#model-assumptions",
    "href": "02-simple_linear_regression.html#model-assumptions",
    "title": "2¬† Simple linear regression",
    "section": "2.4 Model assumptions",
    "text": "2.4 Model assumptions\nNow that we‚Äôve fit a model and understand the output, it‚Äôs time to think about whether this model is a good fit for our data. We first have to understand some assumptions that need to met in regression modelling. Importantly, these assumptions reate to the residuals of our model, not the raw data points themselves. The two assumptions we‚Äôll focus on for now are the assumptions of normality of the residuals, and the constant variance of the residuals. Both assumptions are often diagnosed visually, so it takes some practice to learn what looks right.\n\n2.4.1 Normality\nWhen a model satisfies the normalit assumption, its residuals (i.e., the difference between the fitted and observed values) will be approximately normally distributed. Normality is typically visualised using a histogram (Figure¬†2.2 A) and/or a quantile-quantile (Q-Q) plot (Figure¬†2.2 B).\n\n\n\n\n\nFigure¬†2.2: Image source: Winter (2019) (all rights reserved)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWinter (2019)‚Äôs description of how QQ plots are generated (p.¬†110):\nTo create this plot, every residual is transformed into a percentile (or quantile) [‚Ä¶] The question the Q-Q plot answers is: what is the corresponding numerical value of the 13.8th percentile on the normal distribution? If the values are the same, they will fit on a straight line, which indicates that the two distributions (the distribution of the residuals and the theoretical normal distribution) are very similar.\n\n\n\n\n2.4.2 Constant variance\nWhen a model satisfies the constant variance assumption (also called homoscedasticity, or the absence of heteroscedasticity), the spread of residuals will be equal across the regression line. This is typically visualised using a residual plot, which should look like a blob (Figure¬†2.2 C).\n\n\n2.4.3 Visualising model assumptions\nLet‚Äôs plot our residuals to assess whether our model satisfies the assumptions of normality and constant variance.\n\n2.4.3.1 Histogram\nWe can do this how it‚Äôs done in Winter (2019) (in Ch. 6, p.¬†110-111), by first extracting the residuals from the model and then fitting them them using the base R function hist().\n\n# extract residuals\nres &lt;- residuals(fit_rt_freq)\n\n\n# plot histogram\nhist(res)\n\n\n\n\nOr, we can use the augment() function from broom to append model values to our original data frame, and then feed this into ggplot() from ggplot2 (or even feed it into hist()).\n\n# or, add to df\ndf_freq &lt;- broom::augment(fit_rt_freq, df_freq)\n\n\n# and create ggplot\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 8, fill = \"grey\", colour = \"black\") +\n  theme_bw()\n\n\n\n\n\n\n2.4.3.2 Q-Q plot\nAgain, we can do it Bodo‚Äôs way:\n\nqqnorm(res)\nqqline(res)\n\n\n\n\nOr using augment() and ggplot().\n\ndf_freq |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() \n\n\n\n\n\n\n2.4.3.3 Residual plot\nBodo‚Äôs way:\n\nplot(fitted(fit_rt_freq), res)\n\n\n\n\nOr with ggplot:\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F)\n\n\n\n\n\n\n\n2.4.4 performance package\nI like to use the performance package to visualise model fit (L√ºdecke et al., 2021).\n\nperformance::check_normality(fit_rt_freq)\n\nOK: residuals appear as normally distributed (p = 0.702).\n\n\n\nperformance::check_heteroscedasticity(fit_rt_freq)\n\nOK: Error variance appears to be homoscedastic (p = 0.980).\n\n\n\nperformance::check_model(fit_rt_freq)\n\n\n\n\n\n2.4.4.1 Coefficients table with summary()\n\n\n&gt; summary(fit_rt_freq)\n\nCall:\n1lm(formula = rt ~ lifetime, data = df_freq, subset = rt &gt; 0)\n\n2Residuals:\n    Min      1Q  Median      3Q     Max \n-228.99 -109.29  -26.99   58.86  777.71 \n\nCoefficients:\n3             Estimate Std. Error t value Pr(&gt;|t|)\n4(Intercept)  309.142      6.259  49.394 &lt;0.0000000000000002 ***\n5lifetime1     31.701     12.517   2.533              0.0116 *\n---\nSignif. codes:  0 ‚Äò***‚Äô 0.001 ‚Äò**‚Äô 0.01 ‚Äò*‚Äô 0.05 ‚Äò.‚Äô 0.1 ‚Äò ‚Äô 1\n\nResidual standard error: 57.46 on 541 degrees of freedom\n6Multiple R-squared:  0.01172,   Adjusted R-squared:  0.00989\nF-statistic: 6.414 on 1 and 541 DF,  p-value: 0.0116\n\n\n1\n\nformula\n\n2\n\nResiduals: differences between observed values and those predicted by the model\n\n3\n\nNames for columns Estimates, SE, t-value, p-value\n\n4\n\nIntercept (\\(b_0\\)), i.e., value of \\(y\\) (first-pass) with a move of one unit of \\(x\\) (lifetime)\n\n5\n\nSlope (\\(b_1\\)), i.e., change in first fixation going from dead to living\n\n6\n\nOutput from an ANOVA\n\n\n\n\n\n\n\nwhat is the intercept?\nis the slope positive or negative?\n\nwhat is it‚Äôs value?\n\nthis is what the slope would look like:\n\n\n\n\n\nExploring the model\n\n# how many observed values did we enter into the model?\ndf_freq |&gt; \n  nrow()\n\n[1] 12\n\n\n\n# how many observed values did we enter into the model?\nlength(fitted(fit_rt_freq))\n\n[1] 12\n\n\n\n\nExploring the model: residuals\n\n# what do our FITTED values look like?\nhead(fitted(fit_rt_freq))\n\n       1        2        3        4        5        6 \n525.9148 576.2873 663.3271 700.2042 700.6845 712.3229 \n\n\n\n# what do our OBSERVED values look like?\nhead(df_freq$rt)\n\n[1] 621.77 519.56 507.38 636.56 587.18 705.00\n\n\n\n# what is the difference between the FITTED and OBSERVED values?\nhead(df_freq$rt) - head(fitted(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942 \n\n\n\n# what are our RESIDUALS?\nhead(residuals(fit_rt_freq))\n\n          1           2           3           4           5           6 \n  95.855154  -56.727276 -155.947103  -63.644200 -113.504485   -7.322942 \n\n\n\n\nExploring the model\n\nwhat were our coefficients?\n\n\ncoef(fit_rt_freq)\n\n  (Intercept)          freq \n713.706297951  -0.003382289 \n\n\n\nwhat would be our predicted reaction time for a word with frequency of 0?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 0\n\n(Intercept) \n   713.7063 \n\n\n\nignore the (Intercept) label here, R just takes the first label when performing an operation on 2 vectors\nwhat is the mean of our predictor coded as +0.5?\n\n\ncoef(fit_rt_freq)['(Intercept)'] + coef(fit_rt_freq)['freq'] * 5000\n\n(Intercept) \n   696.7949 \n\n\n\n\n\n\n\n\nImage source: Winter (2019) (all rights reserved)"
  },
  {
    "objectID": "02-simple_linear_regression.html#reporting-your-model",
    "href": "02-simple_linear_regression.html#reporting-your-model",
    "title": "2¬† Simple linear regression",
    "section": "2.5 Reporting your model",
    "text": "2.5 Reporting your model\nSection"
  },
  {
    "objectID": "02-simple_linear_regression.html#summary",
    "href": "02-simple_linear_regression.html#summary",
    "title": "2¬† Simple linear regression",
    "section": "2.6 Summary",
    "text": "2.6 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor"
  },
  {
    "objectID": "02-simple_linear_regression.html#important-terms",
    "href": "02-simple_linear_regression.html#important-terms",
    "title": "2¬† Simple linear regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Coefficients\nthe slope and the intercept are coefficiens\nNA\n    Simple linear regression\nlinear regression with a single predictor and a continuous outcome variable\n`lm(response ~ predictor, data = data)`\n    fitted values\npredicted values\npredict(model_name)\n    continuous variable\na variable that can have an infinite number of values (an example would be reading time in ms)\nNA\n    dependent variable\nwhat we measure; a.k.a. measure/outcome/response variable\nNA\n    independent variable\nour predictor; a.k.a., predictor variable, fixed effects\nNA\n    coefficients\nvalues of the intercept and slope of a lm() model\ncoef()\n    equation of a line\nvalue of y = intercept + (slope*value of x)\ny=b0+b1*xi"
  },
  {
    "objectID": "02-simple_linear_regression.html#learning-objectives-1",
    "href": "02-simple_linear_regression.html#learning-objectives-1",
    "title": "2¬† Simple linear regression",
    "section": "Learning Objectives üèÅ",
    "text": "Learning Objectives üèÅ\nToday we learned‚Ä¶\n\nhow to fit a simple linear model with the lm() function\nhow to interpret our model output"
  },
  {
    "objectID": "02-simple_linear_regression.html#task",
    "href": "02-simple_linear_regression.html#task",
    "title": "2¬† Simple linear regression",
    "section": "2.7 Task",
    "text": "2.7 Task\nNow it‚Äôs your turn. Try to run the following lm() models:\n\ntotal reading time at the verb region\ntotal reading time at the verb+1 region."
  },
  {
    "objectID": "02-simple_linear_regression.html#session-info",
    "href": "02-simple_linear_regression.html#session-info",
    "title": "2¬† Simple linear regression",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] magick_2.8.3        performance_0.11.0  googlesheets4_1.1.1\n [4] gt_0.10.1           kableExtra_1.4.0    knitr_1.43         \n [7] patchwork_1.2.0     languageR_1.5.0     janitor_2.2.0      \n[10] lme4_1.1-35.3       Matrix_1.7-0        broom_1.0.5        \n[13] here_1.0.1          lubridate_1.9.3     forcats_1.0.0      \n[16] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n[19] readr_2.1.5         tidyr_1.3.1         tibble_3.2.1       \n[22] ggplot2_3.5.1       tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1  viridisLite_0.4.2 farver_2.1.1      fastmap_1.1.1    \n [5] bayestestR_0.13.2 pacman_0.5.1      digest_0.6.33     timechange_0.3.0 \n [9] lifecycle_1.0.4   magrittr_2.0.3    compiler_4.4.0    sass_0.4.7       \n[13] rlang_1.1.3       tools_4.4.0       utf8_1.2.4        yaml_2.3.7       \n[17] labeling_0.4.3    htmlwidgets_1.6.4 curl_5.2.1        bit_4.0.5        \n[21] xml2_1.3.6        withr_3.0.0       datawizard_0.10.0 grid_4.4.0       \n[25] googledrive_2.1.1 fansi_1.0.6       colorspace_2.1-0  scales_1.3.0     \n[29] MASS_7.3-60.2     insight_0.19.10   cli_3.6.2         rmarkdown_2.24   \n[33] crayon_1.5.2      generics_0.1.3    rstudioapi_0.16.0 httr_1.4.7       \n[37] tzdb_0.4.0        minqa_1.2.6       splines_4.4.0     parallel_4.4.0   \n[41] cellranger_1.1.0  vctrs_0.6.5       boot_1.3-30       jsonlite_1.8.7   \n[45] hms_1.1.3         ggrepel_0.9.5     bit64_4.0.5       systemfonts_1.0.6\n[49] see_0.8.4         glue_1.7.0        nloptr_2.0.3      stringi_1.8.3    \n[53] gtable_0.3.5      munsell_0.5.1     pillar_1.9.0      htmltools_0.5.8.1\n[57] R6_2.5.1          rprojroot_2.0.4   vroom_1.6.5       evaluate_0.21    \n[61] lattice_0.22-6    backports_1.4.1   snakecase_0.11.1  gargle_1.5.2     \n[65] renv_1.0.7        Rcpp_1.0.12       svglite_2.1.3     nlme_3.1-164     \n[69] mgcv_1.9-1        xfun_0.40         fs_1.6.3          pkgconfig_2.0.3"
  },
  {
    "objectID": "02-simple_linear_regression.html#references",
    "href": "02-simple_linear_regression.html#references",
    "title": "2¬† Simple linear regression",
    "section": "References",
    "text": "References\n\n\nL√ºdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., & Makowski, D. (2021). performance: An R package for assessment, comparison and testing of statistical models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "03-continuous_predictors.html#learning-objectives",
    "href": "03-continuous_predictors.html#learning-objectives",
    "title": "3¬† Continuous predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nwhy and how to centre continuous predictors\nwhen and how to standardize continuous predictors\nwhy and how to log-transform continuous variables"
  },
  {
    "objectID": "03-continuous_predictors.html#set-up-environment",
    "href": "03-continuous_predictors.html#set-up-environment",
    "title": "3¬† Continuous predictors",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe‚Äôll also need to load in our required packages. Hopefully you‚Äôve already install the required packages (if not, go to Chapter¬†3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad data\n\ndf_freq &lt;- read_csv(here(\"data\", \"ELP_frequency.csv\")) |&gt; \n  clean_names()\n\nReminder of our variables:\n\nsummary(df_freq)\n\n     word                freq               rt       \n Length:12          Min.   :    4.0   Min.   :507.4  \n Class :character   1st Qu.:   57.5   1st Qu.:605.2  \n Mode  :character   Median :  325.0   Median :670.8  \n                    Mean   : 9990.2   Mean   :679.9  \n                    3rd Qu.: 6717.8   3rd Qu.:771.2  \n                    Max.   :55522.0   Max.   :877.5"
  },
  {
    "objectID": "03-continuous_predictors.html#summary",
    "href": "03-continuous_predictors.html#summary",
    "title": "3¬† Continuous predictors",
    "section": "Summary",
    "text": "Summary\nIn the last lectures we saw that the equation for a straight line boils down to its intercept and slope, and that linear regression fits a line to our data. This line results in predicted/fitted values, which fall along the line, and residuals, which are the difference between our observed values and the fitted values.\nWe also learned about two model assumptions: normality of residuals, and constant variance of residuals. We learned that we can plot these with histograms or Q-Q plots (normality), and residual plots (constant variance).\nNow that we understand what a simple linear does, we can take a step back and focus on what we put into the model. So far we‚Äôve looked at reaction times (milliseconds) as a function of word frequency. However, we don‚Äôt typically feed raw continuous data into a model, because most continuous linguistic variables are not normally distributed, and so a straight line will not fit it very well (because there will be some large variance for higher values)."
  },
  {
    "objectID": "03-continuous_predictors.html#linear-transformations",
    "href": "03-continuous_predictors.html#linear-transformations",
    "title": "3¬† Continuous predictors",
    "section": "3.1 Linear transformations",
    "text": "3.1 Linear transformations\nLinear transformations refer to constant changes across values that do not alter the relationship between these values. For example, adding, subtracting, or multiplying by a constant value will not alter the difference between values. Think of the example in the last lecture on the relationship between heights and ages as a function of the measurement unit: the relationship between all the values did not alter, because the difference between heights millimeters, centimeters, and meters is constant, as is the difference between ages in days, months, or years. We‚Äôll now look at some common ways of linearly transforming our data, and the reasons behind doing so.\n\n3.1.1 Centering\nCentering is typically applied to predictor variables. Centering refers to subtracting the mean of a variable from each value, resulting in each centered value representing the original value‚Äôs deviance from the mean (i.e., a mean-deviation score). What would a centered value of \\(0\\) represent in terms of the original values?\nLet‚Äôs try centering our frequency values. To create a new variable (or alter an existing variable), we can use the mutate() function from dplyr.\n\n# add centered variable\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_c = freq-mean(freq))\n\nThis can also be done with base R, but it‚Äôs a lot more verbose.\n\n# add centered variable with base R\ndf_freq$freq_c &lt;- df_freq$freq-mean(df_freq$freq)\n\nNow let‚Äôs fit our models.\n\n# run our model with the original predictor\nfit_rt_freq &lt;- \n  lm(rt ~ freq, data = df_freq)\n\n\n# run our model with the centered predictor\nfit_rt_freq_c &lt;- \n  lm(rt ~ freq_c, data = df_freq)\n\nIf we compare the coefficients from fit_rt_freq and fit_rt_freq_c, what do we see? The only difference is the intercept values: 713.706298 (uncentered) and 679.9166667 (centered).\n\nmean(df_freq$rt)\n\n[1] 679.9167\n\n\nThe intercept for a centered continuous predictor variable corresponds to the mean of a continuous response variable. This is crucial in interpreting interaction effects, which we will discuss tomorrow. For more detail on interpreting interactions, see Chapter 8 in Winter (2019) (we won‚Äôt be discussing this chapter as a whole).\n\n\n\n\n\n\nCentering interval data\n\n\n\nIf you have interval data with a specific upper and lower bound, you could alternatively subtract the median value. In linguistic research, this is most typically rating scale data. For example, if you have a dataset consisting of ratings from 1-7, you can centre these ratings by subtracting 4 from all responses. A centred response of -3 would correspond to the lowest rating (1), and of +3 to the highest rating (7), which 0 would correspond to a medial rating (4). This can also be helpful in plotting, as there is no question as to whether 1 or 7 was high or low, because all ratings are now centred around 0 (and negative numbers correspond to our intuition of low-ratings).\n\n\n\n\n3.1.2 Standardizing (z-scoring)\nWe can also standardize continuous predictors by dividing centered values by the standard deviation of the sample. Let‚Äôs look at our frequency/reaction time data again.\nFirst, what are our mean and standard deviation? This will help us understand the changes to our variables as we center and stardardize them.\n\nmean(df_freq$freq)\n\n[1] 9990.167\n\n\n\nsd(df_freq$freq)\n\n[1] 18558.69\n\n\nWhat are the first six values of freq in the original scale?\n\ndf_freq$freq[1:6]\n\n[1] 55522 40629 14895  3992  3850   409\n\n\nWhat are the first six values of freq_c in the centered scale? These should be the values of freq minus the mean of freq (which we saw above is 9990.1666667).\n\ndf_freq$freq_c[1:6]\n\n[1] 45531.833 30638.833  4904.833 -5998.167 -6140.167 -9581.167\n\n\nNow, let‚Äôs create our standardised z-scores for frequency by dividing these centered values by the standard deviation of freq (which will be the same as the standard deviation of freq_c), and which we saw is 18558.6881679. Again, this can be done with mutate() from dplyr, or by using base R syntax.\n\n# standardise using the tidyverse\ndf_freq &lt;- \n  df_freq |&gt; \n  mutate(freq_z = freq_c/sd(freq))\n\n\n# standardize with base R\ndf_freq$freq_z &lt;- df_freq$freq_c/sd(df_freq$freq)\n\n\nhead(df_freq)\n\n# A tibble: 6 √ó 5\n  word      freq    rt freq_c freq_z\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 thing    55522  622. 45532.  2.45 \n2 life     40629  520. 30639.  1.65 \n3 door     14895  507.  4905.  0.264\n4 angel     3992  637. -5998. -0.323\n5 beer      3850  587. -6140. -0.331\n6 disgrace   409  705  -9581. -0.516\n\n\n\n\n\n\n\n\nCorrelation"
  },
  {
    "objectID": "03-continuous_predictors.html#non-linear-transformations",
    "href": "03-continuous_predictors.html#non-linear-transformations",
    "title": "3¬† Continuous predictors",
    "section": "3.2 Non-linear transformations",
    "text": "3.2 Non-linear transformations\nThis is really the meat and potates of dealing with continuous variables (depending on your subfield). In linguistic research, and especially experimental research, we often deal with continuous variables truncated/bound at 0. Reaction times, reading times and formant frequencies are all examples of such types of data: there is no such thing as a negative reading time or fundamental frequency. The problem with these types of data is that they are almost never normally distributed, which has implications for the normality of residuals for any line that tries to fit to these data. Very often, this type of data will have a ‚Äòpositive skew‚Äô, or a long tail off to the right (assuming larger values are plotting to the right). This shape is not symmetrical, meaning that the residuals tend to be much larger for larger values. It is also often the case that these very large, exceptional values will have a stronger influence on the line of best fit, leading to the coefficient estimates that are ‚Äúsuboptimal for the majority of data points‚Äù [@Baayen (2008); p.¬†92]. How do we deal with this nonnormality? We use non-linear transformations, the most common of which is the log-transformation.\n\n3.2.1 Log-transformation\nLet‚Äôs look at our reaction time data again. We‚Äôll log-transform our reaction time data and frequency data. Note that in Winter (2019), frequency is transformed using log to the base 10 for interpretability, but we‚Äôll stick to the natural logarithm.\n\ndf_freq |&gt; \n  ggplot() +\n  aes(x = log(freq)) +\n  geom_density()\n\n\n\n\n\ndf_freq &lt;-\n  df_freq |&gt; \n    mutate(rt_log = log(rt),\n           freq_log = log(freq))\n\n\nlm(rt_log ~ freq_log, data = df_freq) |&gt; tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 freq_log     -0.0453   0.00871     -5.20 4.03e- 4\n\n\n\n# or, log-transform directly in the model syntax\nlm(log(rt) ~ log(freq), data = df_freq) |&gt; tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   6.79     0.0611     111.   8.56e-17\n2 log(freq)    -0.0453   0.00871     -5.20 4.03e- 4"
  },
  {
    "objectID": "03-continuous_predictors.html#learning-objectives-1",
    "href": "03-continuous_predictors.html#learning-objectives-1",
    "title": "3¬† Continuous predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned‚Ä¶\n\nwhy and how to centre continuous predictors\nwhen and how to standardize continuous predictors\nwhy and how to log-transform continuous variables"
  },
  {
    "objectID": "03-continuous_predictors.html#important-terms",
    "href": "03-continuous_predictors.html#important-terms",
    "title": "3¬† Continuous predictors",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Centering\ntype of linear transformation\n`dplyr::mutate(variable = variable - mean(variable))`\n    standardizing\nlinear transformation (applied for multiple continuous predictors)\ndf_freq$freq_z &lt;- df_freq$freq_c/sd(df_freq$freq)\n    log-transformation\nnon-linear transformation for positively skewed continuous variables\nlog()\n    Error (random effects)\nhings we cannot understand/measure. There are always sources of random error.\nNA"
  },
  {
    "objectID": "03-continuous_predictors.html#take-home-messages",
    "href": "03-continuous_predictors.html#take-home-messages",
    "title": "3¬† Continuous predictors",
    "section": "Take-home messages",
    "text": "Take-home messages\nContinuous data are often transformed before fitting a model to this data. Linear transformations, like adding or multiplying all values by a single value, are often performed on continuous predictors by means of centring and standardizing (when there are multiple continuous predictors). Non-linear transformations are often performed on continuous data with a positive skew (a few values much larger than the majority) in order to satisfy the normality assumption. Although the normality assumption refers to the normality of residuals, the distribution of the data will have implications for the distribution of the residuals. The most common non-linear transformation is the log-transformation (the inverse of the exponential), which shrinks values, especially making big numbers smaller. This has the result of squeezing big numbers towards smaller numbers, reducing the spread in the distribution (e.g., the log of 3 is 1.0986123, the log of 30 is 3.4011974, and the log of 30 is 5.7037825).\nWhat to do with this information? If you have continuous data truncated at 0 (with no upperbound, e.g., reaction time data or fundamental frequency), visualise the data (histogram and Q-Q plot) in order to check its distribution. If it is not normally distributed, you will likely want to log-transform it. Is this data your response variable? Then that is all you will likely want to do. Is this data a predictor variable? Then you will want to centre it (subtract the mean of this variable from all values). Do you have more than one continuous predictor variable? Then standardizing these variables will facilitate the interpretation of interaction effects (we‚Äôll talk about these soon)."
  },
  {
    "objectID": "03-continuous_predictors.html#task",
    "href": "03-continuous_predictors.html#task",
    "title": "3¬† Continuous predictors",
    "section": "3.3 Task",
    "text": "3.3 Task"
  },
  {
    "objectID": "03-continuous_predictors.html#assessing-assumptions",
    "href": "03-continuous_predictors.html#assessing-assumptions",
    "title": "3¬† Continuous predictors",
    "section": "3.4 Assessing assumptions",
    "text": "3.4 Assessing assumptions\n\nRe-run the models fit_rt_freq, fit_rt_freq_c, and fit_log\nProduce diagnostic plots for each of them (histograms, Q-Q plots, residual plots)\nInterpret the plots"
  },
  {
    "objectID": "03-continuous_predictors.html#model-comparison",
    "href": "03-continuous_predictors.html#model-comparison",
    "title": "3¬† Continuous predictors",
    "section": "3.5 Model comparison",
    "text": "3.5 Model comparison\n\nUse the glance() function to inspect the \\(R^2\\), AIC, and BIC of each model.\nWhich is the best fit? Why?"
  },
  {
    "objectID": "03-continuous_predictors.html#session-info",
    "href": "03-continuous_predictors.html#session-info",
    "title": "3¬† Continuous predictors",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] googlesheets4_1.1.1 gt_0.10.1           kableExtra_1.4.0   \n [4] knitr_1.43          patchwork_1.2.0     languageR_1.5.0    \n [7] janitor_2.2.0       lme4_1.1-35.3       Matrix_1.7-0       \n[10] broom_1.0.5         here_1.0.1          lubridate_1.9.3    \n[13] forcats_1.0.0       stringr_1.5.1       dplyr_1.1.4        \n[16] purrr_1.0.2         readr_2.1.5         tidyr_1.3.1        \n[19] tibble_3.2.1        ggplot2_3.5.1       tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      xfun_0.40         htmlwidgets_1.6.4 gargle_1.5.2     \n [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.0      \n [9] generics_0.1.3    curl_5.2.1        parallel_4.4.0    fansi_1.0.6      \n[13] pacman_0.5.1      pkgconfig_2.0.3   lifecycle_1.0.4   farver_2.1.1     \n[17] compiler_4.4.0    munsell_0.5.1     snakecase_0.11.1  sass_0.4.7       \n[21] htmltools_0.5.8.1 yaml_2.3.7        crayon_1.5.2      pillar_1.9.0     \n[25] nloptr_2.0.3      MASS_7.3-60.2     boot_1.3-30       nlme_3.1-164     \n[29] tidyselect_1.2.1  digest_0.6.33     stringi_1.8.3     labeling_0.4.3   \n[33] splines_4.4.0     rprojroot_2.0.4   fastmap_1.1.1     grid_4.4.0       \n[37] colorspace_2.1-0  cli_3.6.2         magrittr_2.0.3    utf8_1.2.4       \n[41] withr_3.0.0       scales_1.3.0      backports_1.4.1   bit64_4.0.5      \n[45] googledrive_2.1.1 timechange_0.3.0  httr_1.4.7        rmarkdown_2.24   \n[49] bit_4.0.5         cellranger_1.1.0  hms_1.1.3         evaluate_0.21    \n[53] viridisLite_0.4.2 rlang_1.1.3       Rcpp_1.0.12       glue_1.7.0       \n[57] xml2_1.3.6        renv_1.0.7        vroom_1.6.5       svglite_2.1.3    \n[61] rstudioapi_0.16.0 minqa_1.2.6       jsonlite_1.8.7    R6_2.5.1         \n[65] fs_1.6.3          systemfonts_1.0.6"
  },
  {
    "objectID": "03-continuous_predictors.html#references",
    "href": "03-continuous_predictors.html#references",
    "title": "3¬† Continuous predictors",
    "section": "References",
    "text": "References\n\n\nBaayen, R. H. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics using R.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "04-multiple_regression.html#summary",
    "href": "04-multiple_regression.html#summary",
    "title": "4¬† Multiple Regression",
    "section": "Summary",
    "text": "Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a continuous predictor"
  },
  {
    "objectID": "04-multiple_regression.html#learning-objectives",
    "href": "04-multiple_regression.html#learning-objectives",
    "title": "4¬† Multiple Regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nwhat multiple regression is\nhow to include multiple predictor variables\nhow to interpret slopes in multiple regression\nhow to interpret interaction effects\nabout the assumption of the absence of collinearity"
  },
  {
    "objectID": "04-multiple_regression.html#set-up-environment",
    "href": "04-multiple_regression.html#set-up-environment",
    "title": "4¬† Multiple Regression",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe‚Äôll also need to load in our required packages. Hopefully you‚Äôve already install the required packages (if not, go to Chapter¬†3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               janitor,\n               languageR)\n\n\nLoad data\nWe‚Äôll use the full dataset of the frequency data.\n\ndf_freq_full &lt;-\n  read_csv(here(\"data\", \"ELP_full_length_frequency.csv\")) |&gt; \n  clean_names() |&gt; \n  mutate(freq = 10^(log10freq), # inverse log10\n         freq_log = log(freq)) |&gt;  # use natural logarithm\n  relocate(word, rt, length, freq, freq_log)\n\nWe have 4 variables:\n\nword\nlength\nrt\nfreq\nfreq_log\nlog10freq"
  },
  {
    "objectID": "04-multiple_regression.html#multiple-regression",
    "href": "04-multiple_regression.html#multiple-regression",
    "title": "4¬† Multiple Regression",
    "section": "4.1 Multiple regression",
    "text": "4.1 Multiple regression\nSo far we‚Äôve worked with simple linear models, which fit a model to a predictor and response variable. These models do not differ so greatly from a one- or two-sample t-test (for a categorical predictor) or Pearson‚Äôs r (for a standardised continuous predictor). You might be wondering then we would bother with linear regression. One reason is that it allows us to include multiple predictors in our models, which still boils down to modeling the mean, but while condintioning the mean on multiple variables at once.\nRecall the equation of a line (\\(\\ref{eq-simple-lin-2}\\)), which states that any value of \\(y\\) equals the intercept (\\(b_0\\)) plus the corresponding value of \\(x\\) multiplied by the slope (\\(b_1x\\)), plus the error, which are our residuals (\\(e\\)). In multiple regression, we can include more than one slope (\\(\\ref{eq-multiple-reg}\\)).\n\\[\\begin{align}\ny &= b_0 + b_1x + e \\label{eq-simple-lin-2} \\\\\ny &= b_0 + b_1x + b_2x + ... + e \\label{eq-multiple-reg}\n\\end{align}\\]\n\n\n\n\n\nflowchart LR\n  A[Continuous variable] \n  A --&gt; F[Zero-truncated with positive skew \\n e.g., reaction times]\n  A --&gt; H[Interval i.e., lower and upperbound \\n e.g., rating scale]\n  H --&gt; I[Centre on median value]\n  I --&gt; E(Response)\n  E --&gt; Z[Done]\n  F --&gt; G[Non-linear transformation \\n e.g., log-transform]\n  G --&gt; B(Predictor)\n  I --&gt; B(Predictor)\n  B --&gt; C{One predictor}\n  C --&gt; X[Centre]\n  D --&gt; Y[Centre and standardise]\n  B --&gt; D{Two predictor}\n  G --&gt; E(Response)\n  \n  \n\n\nFigure¬†4.1: Flowchart of common steps for linear and non-linear transformations of continuous variables. Such decision trees are not a one-size-fits-all solution and cannot replace critical thinking and understanding of your data.\n\n\n\n\n\n4.1.1 One predictor\nLet‚Äôs re-run our simple model with this dataset. Let‚Äôs keep reaction times in the raw milliseconds for now for interpretability.\n\nfit_freq_full &lt;-\n  lm(rt ~ log(freq), data = df_freq_full)\n\n\ntidy(fit_freq_full)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    907.      1.09       828.       0\n2 log(freq)      -37.5     0.262     -143.       0\n\n\nWe see there is a decrease in reaction times (-37.5 milliseconds) for a 1-unit increase in log frequency. Let‚Äôs look at the model fit using glance().\n\nglance(fit_freq_full)$r.squared\n\n[1] 0.3834186\n\n\nWe see that the R-squared is 0.383, meaning our model describes 38% of the variance in response times. We can‚Äôt be sure that this described variance is due solely to frequency, however. Our models only know what we tell them! Other effects that are correlated with frequency might be conflating the frequency effect, e.g., more frequent words tend to be shorter (zipf_1949?). Let‚Äôs expand our model to include word length [\\(\\ref{eq-freq-length}\\)].\n\\[\\begin{equation}\ny = b_0 + b_1*log frequency + b_2*word length \\label{eq-freq-length}\n\\end{equation}\\]\n\n\n4.1.2 Adding a predictor\nLet‚Äôs add length as a predictor to our model.\n\nfit_freq_mult &lt;-\n  lm(rt ~ log(freq) + length, data = df_freq_full)\n\n\ntidy(fit_freq_mult) |&gt; select(term, estimate)\n\n# A tibble: 3 √ó 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    748. \n2 log(freq)      -29.5\n3 length          19.5\n\n\nWe see that length is also a significant predictor of reaction times, with an increase in word length (+1 letter) corresponds to a 20ms increase in reaction times. Our intercept is also now 748ms, instead of 907ms. The 907ms intercept corresponds to the prediction for reaction times to a word with 0 log frequency and 0 word length, but this is not very interpretable. If we were to center both prdictors, the intercept would be the reaction time for a wrd with average frequency and average length.\nThe slope for log frequency has also changed: from -37.5 to -29.5. This change tells us that some of the effect in our first model was confounded with length, as controlling for length weakens the effect of frequency.\n\nglance(fit_freq_mult)$r.squared\n\n[1] 0.4872977\n\n\nWe also see that including length increases the variance described by our model, reflected in the R-squared values (0.4872977 instead of 0.3834186."
  },
  {
    "objectID": "04-multiple_regression.html#standardising-our-predictors",
    "href": "04-multiple_regression.html#standardising-our-predictors",
    "title": "4¬† Multiple Regression",
    "section": "4.2 Standardising our predictors",
    "text": "4.2 Standardising our predictors\nRecall that, when we have multiple continuous predictors, standardising them can help their interpretation, as their slopes are comparable. We could achieve this by centering each variable and then dividing by the standard deviation, or we could use the scale() function, which does just this.\n\n# centre and then standardize\ndf_freq_full |&gt; \n  mutate(\n         freq_z1 = (freq-mean(freq))/sd(freq),\n         freq_z2 = scale(freq)) |&gt; \n  select(freq_z1, freq_z2) |&gt; \n  head()\n\n# A tibble: 6 √ó 2\n  freq_z1 freq_z2[,1]\n    &lt;dbl&gt;       &lt;dbl&gt;\n1 -0.0902     -0.0902\n2 -0.0864     -0.0864\n3 -0.0905     -0.0905\n4 -0.0864     -0.0864\n5 -0.0885     -0.0885\n6 -0.0901     -0.0901\n\n\nLet‚Äôs use scale() for freq and length.\n\ndf_freq_full &lt;-\n  df_freq_full |&gt; \n  mutate(freq_z = scale(freq_log),\n         length_z = scale(length))\n\n\nfit_freq_z &lt;-\n  lm(rt ~ freq_z + length_z, data = df_freq_full)\n\nFirst, let‚Äôs check the \\(R^2\\):\n\nglance(fit_freq_z)$r.squared\n\n[1] 0.4872977\n\n\nWe see that our \\(R^2\\) value is 0.4872977, just like above. This serves as a reminder that the predictors still represent the same variance in the underlying model, their units and scales have simply changed. What about our coefficients:\n\ntidy(fit_freq_z) |&gt; select(term, estimate)\n\n# A tibble: 3 √ó 2\n  term        estimate\n  &lt;chr&gt;          &lt;dbl&gt;\n1 (Intercept)    770. \n2 freq_z         -60.6\n3 length_z        43.3\n\n\nHere, a 1-unit change always corresponds to a change of 1 standard deviation. Now we see that frequency has a larger magnitude than the effect of length. So, for each instease in frequency by 1 standard deviation (holiding length constant), reaction times decrease by 29.5 ms.\n\n4.2.1 Adding an interaction term\nWe won‚Äôt spent much time talking about interactions, but please check out Ch. 8 (Interations and nonlinear effects) in Winter (2019) for a more in-depth treatment. For now, what‚Äôs important to know is that interactions describe how effects of one predictor may be influenced by changes in another predictor. We can add interactin terms of two predictors by connecting them with a colon (:).\n\nlm(rt ~ freq_z + length_z + freq_z:length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 √ó 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nOr, we can simply connect the two predictors with an asterisk (*) to indicate that we want to look at both predictors and their interaction.\n\nlm(rt ~ freq_z*length_z, \n   data = df_freq_full) |&gt; \n  tidy() |&gt; select(term, estimate)\n\n# A tibble: 4 √ó 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)        766. \n2 freq_z             -63.9\n3 length_z            41.8\n4 freq_z:length_z    -11.4\n\n\nThe model estimates are the same for both models. The intercept is the predicted reaction time for a word with the mean length and mean frequency. Notice that the interaction slope is negative, meaning when both freq and length increase, reaction times will decrease."
  },
  {
    "objectID": "04-multiple_regression.html#model-assumptions",
    "href": "04-multiple_regression.html#model-assumptions",
    "title": "4¬† Multiple Regression",
    "section": "4.3 Model assumptions",
    "text": "4.3 Model assumptions\nWe‚Äôve already discussed the assumptions of normality and homoscedasticity (constant variance), which both refer to the residuals of a model. We typically assess these assumptions visually, with histogram and Q-Q plots.\n\n4.3.1 Normality and Homoscedasticity\nFor our model\n\nfig_hist &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nThe histogram looks approximately normally distributed, with a bit of a positive skew. The Q-Q plot suggests a less-normal distribution, with the model estimates fitting larger reaction times more poorly. The residual plot also shows that the variance of the residuals is not constant, with much larger residual variance for larger fitted values. This tells us we should probably log reaction times. Let‚Äôs try it all again, with log-transformed reaction times.\n\n\n4.3.2 Log-transformed response variable\n\nfit_freq_log_z &lt;-\n  lm(log(rt) ~ freq_z*length_z,\n     data = df_freq_full)\n\n\nglance(fit_freq_log_z)$r.squared\n\n[1] 0.5176913\n\n\n\ntidy(fit_freq_log_z) |&gt; select(term, estimate)\n\n# A tibble: 4 √ó 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.63   \n2 freq_z          -0.0826 \n3 length_z         0.0524 \n4 freq_z:length_z -0.00779\n\n\nWe see now that our values are much smaller, because they‚Äôre on the log-scale.\n\nexp(6.63 + -0.0826*5 + 0.0524*2)\n\n[1] 556.5739\n\nexp(6.63 + -0.0826*4 + 0.0524*2)\n\n[1] 604.499\n\nexp(6.63 + -0.0826*1 + 0.0524*6)\n\n[1] 955.0847\n\ntidy(fit_freq_log_z)\n\n# A tibble: 4 √ó 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)      6.63     0.000636   10428.  0       \n2 freq_z          -0.0826   0.000666    -124.  0       \n3 length_z         0.0524   0.000649      80.7 0       \n4 freq_z:length_z -0.00779  0.000581     -13.4 8.51e-41\n\n\n\nfig_hist &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .resid) +\n  geom_histogram(bins = 20, fill = \"grey\", colour = \"black\") +\n  theme_bw() +\n  labs(title='Histogram', x='Residuals', y='Count')\n\nfig_qq &lt;-\nfit_freq_log_z |&gt; \n  ggplot() +\n  aes(sample = .resid) +\n  geom_qq(colour = \"red\") +\n  geom_qq_line() +\n  labs(title='Q-Q Plot', x='Theoretical quantiles', y='Sample quantiles')\n\nfig_res &lt;-\n  fit_freq_log_z |&gt; \n  ggplot() +\n  aes(x = .fitted, y = .resid) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"blue\") +\n  labs(title='Residual vs. Fitted Values Plot', x='Fitted Values', y='Residuals')\n\nfig_hist + fig_qq + fig_res\n\n\n\n\nLooks much better.\n\n\n4.3.3 Collinearity\nCollinearity refers to when continuous predictor variables are correlated, which can make the interpretation of their coefficients difficult, and the results spurious. Regression assumes there is an absence of collinearity, i.e., our predictor variables are not correlatded.\nTo assess collinearity, you can use the vif() function from the car package to compare variance inflation factors. VIF values close to 1 indicates there is not a high degree of collinearity between your variables.\n\ncar::vif(fit_freq_log_z)\n\n         freq_z        length_z freq_z:length_z \n       1.246509        1.184641        1.068283 \n\n\nCollinearity is a conceptual problem, and is something that you need to consider in the planning stage. Typically, we want to include predictors that we have specific predictions or research questions about. Shoving a bunch of predictors in a model to see what comes out significant is bad practice. Rather, we should have a principled approach to model building and variable selection. This is not to say that exploratory analyses should be avoided, but that this comes with caveats.\n\n\n4.3.4 Adjusted \\(R^2\\)\nAlthough we should avoid throwing any old predictor into our model, adjusted \\(R^2\\) is a more conservative version of \\(R^2\\) that takes into account the number of predictors in a model. For each additional predictor, adjusted \\(R^2\\) includes the number of predictors (\\(k\\)) in its denominator (bottom half of a division), which means that the more predictors there are, the smaller \\(R^2\\) will be, unless each additional predictor explains sufficient variance to counteract this penalisation.\n\nglance(fit_freq_log_z)$adj.r.squared\n\n[1] 0.5176475\n\n\nIf we were to look at the (adjusted) \\(R^2\\) of our simple linear regression model, where log reaction times are predicted by standardised log frequency, we see that there is a large increase in our model which includes length and its interaction. This suggests that our model is not overfit, and that length contributes to the variance explained by the model.\n\nglance(lm(log(rt) ~ freq_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.4148675\n\n\nIf we likewise compare to the same model without an interaction term (log reaction times ~ frequency * length), we see that the adjusted \\(R^2\\) is not very different. If the adjusted \\(R^2\\) were much lower, this would indicate that including the interaction term leads to overfitting.\n\nglance(lm(log(rt) ~ freq_z + length_z, data = df_freq_full))$adj.r.squared\n\n[1] 0.5150461"
  },
  {
    "objectID": "04-multiple_regression.html#important-terms",
    "href": "04-multiple_regression.html#important-terms",
    "title": "4¬† Multiple Regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Collinearity\nCorrelation between two predictors (linear model assumes non-collinearity)\ncar::vif(model)"
  },
  {
    "objectID": "04-multiple_regression.html#learning-objectives-1",
    "href": "04-multiple_regression.html#learning-objectives-1",
    "title": "4¬† Multiple Regression",
    "section": "Learning Objectives üèÅ",
    "text": "Learning Objectives üèÅ\nToday we learned‚Ä¶\n\nwhat multiple regression is\nhow to include multiple predictor variables\nhow to interpret slopes in multiple regression\nhow to interpret interaction effects\nabout the assumption of the absence of collinearity"
  },
  {
    "objectID": "04-multiple_regression.html#task",
    "href": "04-multiple_regression.html#task",
    "title": "4¬† Multiple Regression",
    "section": "4.4 Task",
    "text": "4.4 Task\nLoad in the english dataset from the languageR package (Baayen & Shafaei-Bajestan, 2019) (code below). You don‚Äôt need to load in any CSV file, because this dataset is available if you have the package loaded. From the manual:\n\nThis data set gives mean visual lexical decision latencies and word naming latencies to 2284 monomorphemic English nouns and verbs, averaged for old and young subjects, with various predictor variables.\n\n(languageR manual, p.¬†29)\n\n# load in 'english' dataset from languageR\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  relocate(word)\n\nWe‚Äôre keeping five variables:\n\nword: a factor with 2284 words\nrt_lexdec: numeric vector of log RT in visual lexical decision\nrt_naming: numeric vector of log RT in word naming\nlength_in_letters: numeric vector with length of the word in letters\nAgeSubject: a factor with as levels the age group of the subject: young versus old.\nfreq_written: numeric vector with log frequency in the CELEX lexical database\n\nTake the following steps:\n\nPerform an exploratory data analysis to understand the data (produce plots, tables, whatever you think necessary and can do).\nModel the data, with back-transformed (raw) reaction times as a response variable and written frequency and length in letters as predictors. Perform any tranformations you think necessary. Run model diagnostic checks and assess model fit.\n\n\nRe-run the model with log reaction times as a response variable and written frequency and length in letters as predictors. Perform any tranformations you think necessary. Run model diagnostic checks and assess model fit.\n\n\nRemove length in letters as a predictor. How is model fit affected? What can you conclude?"
  },
  {
    "objectID": "04-multiple_regression.html#session-info",
    "href": "04-multiple_regression.html#session-info",
    "title": "4¬† Multiple Regression",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] googlesheets4_1.1.1 gt_0.10.1           kableExtra_1.4.0   \n [4] knitr_1.43          patchwork_1.2.0     languageR_1.5.0    \n [7] janitor_2.2.0       broom_1.0.5         here_1.0.1         \n[10] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[13] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[16] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[19] tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      xfun_0.40         htmlwidgets_1.6.4 gargle_1.5.2     \n [5] tzdb_0.4.0        vctrs_0.6.5       tools_4.4.0       generics_0.1.3   \n [9] curl_5.2.1        parallel_4.4.0    fansi_1.0.6       pacman_0.5.1     \n[13] pkgconfig_2.0.3   lifecycle_1.0.4   farver_2.1.1      compiler_4.4.0   \n[17] munsell_0.5.1     carData_3.0-5     snakecase_0.11.1  sass_0.4.7       \n[21] htmltools_0.5.8.1 yaml_2.3.7        car_3.1-2         crayon_1.5.2     \n[25] pillar_1.9.0      abind_1.4-5       tidyselect_1.2.1  digest_0.6.33    \n[29] stringi_1.8.3     labeling_0.4.3    rprojroot_2.0.4   fastmap_1.1.1    \n[33] grid_4.4.0        colorspace_2.1-0  cli_3.6.2         magrittr_2.0.3   \n[37] utf8_1.2.4        withr_3.0.0       scales_1.3.0      backports_1.4.1  \n[41] bit64_4.0.5       googledrive_2.1.1 timechange_0.3.0  httr_1.4.7       \n[45] rmarkdown_2.24    bit_4.0.5         cellranger_1.1.0  hms_1.1.3        \n[49] evaluate_0.21     viridisLite_0.4.2 rlang_1.1.3       glue_1.7.0       \n[53] xml2_1.3.6        renv_1.0.7        svglite_2.1.3     rstudioapi_0.16.0\n[57] vroom_1.6.5       jsonlite_1.8.7    R6_2.5.1          systemfonts_1.0.6\n[61] fs_1.6.3"
  },
  {
    "objectID": "04-multiple_regression.html#references",
    "href": "04-multiple_regression.html#references",
    "title": "4¬† Multiple Regression",
    "section": "References",
    "text": "References\n\n\nBaayen, R. H., & Shafaei-Bajestan, E. (2019). languageR: Analyzing linguistic data: A practical introduction to statistics. https://CRAN.R-project.org/package=languageR\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "05-categorical_predictors.html#learning-objectives",
    "href": "05-categorical_predictors.html#learning-objectives",
    "title": "5¬† Categorical predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nabout cateogorical predictors\nhow to interpret different contrast coding"
  },
  {
    "objectID": "05-categorical_predictors.html#set-up-environment",
    "href": "05-categorical_predictors.html#set-up-environment",
    "title": "5¬† Categorical predictors",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\n\nWe‚Äôll also need to load in our required packages.\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\nLoad data\nLet‚Äôs continue working with the english dataset from the languageR package. Let‚Äôs just call it df_freq_eng.\n\ndf_freq_eng &lt;-\n  as.data.frame(english) |&gt; \n  dplyr::select(RTlexdec, RTnaming, Word, LengthInLetters, AgeSubject, WrittenFrequency) |&gt; \n  rename(rt_lexdec = RTlexdec,\n         rt_naming = RTnaming,\n         freq_written = WrittenFrequency) |&gt; \n  clean_names() |&gt; \n  # standardize continuous predictors\n  mutate(\n    freq_z = scale(freq_written),\n    length_z = scale(length_in_letters)\n  ) |&gt; \n  relocate(word) |&gt; \n  arrange(word)\n\nIn your exploratory data analysis, you might‚Äôve noticed a bimodal distribution.\n\n\n\n\n\nThis looks like a bimodal distribution, i.e., there are two modes (most frequent value, i.e., peak in a histogram). What might be driving this? We know that there were two subject groups: old and young. How does the distribution of these two groups look?\nRunning our model of the log reaction times as predicted by frequency and length, we see:\n\nfit_freq_length &lt;-\n  lm(rt_lexdec ~ freq_z*length_z,\n     data = df_freq_eng)\n\n\nglance(fit_freq_length)$r.squared\n\n[1] 0.1896649\n\nglance(fit_freq_length)$adj.r.squared\n\n[1] 0.1891323\n\n\nSeems like we don‚Äôt have any overfitting in our model (\\(R^2\\) and adjusted \\(R^2\\) are comparable). Let‚Äôs look at our coeffiecients.\n\ntidy(fit_freq_length) |&gt; select(term, estimate)\n\n# A tibble: 4 √ó 2\n  term            estimate\n  &lt;chr&gt;              &lt;dbl&gt;\n1 (Intercept)      6.55   \n2 freq_z          -0.0682 \n3 length_z         0.00328\n4 freq_z:length_z -0.00196\n\n\nThere is a negative slope for frequency, indicating shorter reaction times for words with higher frequency (when holding length constant). There is a positive slope for length, indicating longer reaction times for longer words (holding frequency constant). There is also a negative interaction estimate, indicating that when both length and frequency increase, reaction times decrease. This seems similar to the dataset we explored in the previous sections. But, this bimodal distribution is suggesting we should include age group as a predictor, since the two groups seem to pattern differently in their reading times. Could it be that the effect of frequency and length also differ as a function of age group?"
  },
  {
    "objectID": "05-categorical_predictors.html#categorical-predictors",
    "href": "05-categorical_predictors.html#categorical-predictors",
    "title": "5¬† Categorical predictors",
    "section": "5.1 Categorical predictors",
    "text": "5.1 Categorical predictors\nIn linguistic research we often want to compare the effect of groups or categories, such as native or non-native speakers, or grammatical or ungrammatical stimuli. We might expect longer reading times for non-native (compared to native) speakers of a language, or for ungrammatical (versus grammatical) sentences. With our current dataset, we‚Äôd predict longer reading times for older participants than younger participants (although we should hypothesise before collecting and visualising our data!). How might these age effects interact with effects of word frequency and length?\n\n5.1.1 Including a categorical predictor\nWhat would happen if we just include age_subject in our model?\n\nfit_age &lt;-\n  lm(rt_lexdec ~ freq_z*length_z + age_subject,\n     data = df_freq_eng)\n\nFirst, we see that adding age to our model results in a large increase in variance explained, and that the \\(R^2\\) and adjusted \\(R^2\\) values are comparable. In addition, the VIF values for all coefficients are near 1. This indicates that our predictors all contribute to the variance explained by the model and are not correlated.\n\nglance(fit_age)$r.squared\n\n[1] 0.6888949\n\nglance(fit_age)$adj.r.squared\n\n[1] 0.6886222\n\n\n\ncar::vif(fit_age)\n\n         freq_z        length_z     age_subject freq_z:length_z \n       1.012553        1.004461        1.000000        1.008108 \n\n\nNow that we see that our model is not overfit and that our predictors are not correlatd, let‚Äôs take a look at our model estimates.\n\ntidy(fit_age) |&gt; select(term,estimate)\n\n# A tibble: 5 √ó 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)       6.66   \n2 freq_z           -0.0682 \n3 length_z          0.00328\n4 age_subjectyoung -0.222  \n5 freq_z:length_z  -0.00196\n\n\nIn addition to the effects we observed in our earlier model, we see that there is a negative slope for age_subjectyoung, indicating that reaction times decrease when‚Ä¶what? How do we interpret a slope for a categorical variable? Regression works with numerical values, so how does a categorical variable get fit to a line? If we feed a categorical variable into the lm() function, the factor levels (i.e., the categories in a categorical variable) are given numerical values. We need to know what these values are in order to know how to interpret our model estimates. We call these numerical values mapped onto factor levels contrast coding, and we can check the contrasts of a given factor using the function contrasts().\n\ncontrasts(df_freq_eng$age_subject)\n\n      young\nold       0\nyoung     1\n\n\nWe see that old was coded at \\(0\\) and young as \\(1\\). This means that our slope for age_subjectyoung represents the change in reaction times when we move from old to young, which corresponds to a 1-unit change in our predictor (because the difference between 0 and 1 is 1). This is called treatment coding, or dummy coding, where one factor level is coded as 0 and the other as 1. Let‚Äôs remove the continuous variable for now and focus on age_subject. Let‚Äôs also look at raw reaction times, to more easily interpret the results.\n\nfit_age &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\n\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\n\n\nOur \\(R^2\\) value is lower than when we included frequency and length, but higher still than our model with frequeny and length but no age.\n\ntidy(fit_age) |&gt; select(term, estimate)\n\n# A tibble: 2 √ó 2\n  term             estimate\n  &lt;chr&gt;               &lt;dbl&gt;\n1 (Intercept)          787.\n2 age_subjectyoung    -157.\n\n\nWe see that there is an estimated decrease in reaction times of 157ms for the young group compared to the old group. But what does the intercept represent here? Let‚Äôs look at our data again.\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summary()\n\n   rt_lexdec      age_subject \n Min.   : 495.4   old  :2284  \n 1st Qu.: 617.4   young:2284  \n Median : 699.6               \n Mean   : 708.1               \n 3rd Qu.: 775.3               \n Max.   :1323.2               \n\n\nAnd how does rt_lexdec differ between the groups?\n\ndf_freq_eng |&gt; \n  select(rt_lexdec, age_subject) |&gt; \n  mutate(rt_lexdec = exp(rt_lexdec)) |&gt; \n  summarise(mean = mean(rt_lexdec),\n            min = min(rt_lexdec),\n            max = max(rt_lexdec),\n    .by = \"age_subject\"\n  )\n\n  age_subject     mean    min    max\n1       young 629.5473 495.38  971.8\n2         old 786.7200 603.77 1323.2\n\n\nWe see here that the intercept for our model actually corresponds to the mean reaction time for the old group. Why is this? Recall that the intercept corresponds to the \\(y\\) value (reaction time) when \\(x\\) is \\(0\\). In treatment/dummy coding, one factor level is coded as \\(0\\). In our case this was old, and so the intercept corresponds to the mean reaction time for participants in the old group. How does R choose which variable to code as \\(0\\)? It simply takes the first level name alphabetically: old comes before young, so old was automatically taken as the ‚Äòbaseline‚Äô to which young was compared.\nAnd if we were to add the slope to the intercept, we would get the mean for the \\(young\\) group. Why is this?\n\ncoef(fit_age)['(Intercept)'] + coef(fit_age)['age_subjectyoung']\n\n(Intercept) \n   629.5473 \n\n\nWhy are the means for the two groups used? The mean is the value closest to all values in a univariate dataset, and regression aims to inimise residuals (recall the line of best fit). So, a line is fit between the means of these two factor levels to achieve minimal residuals. This actually is the same thing as a t-test:\n\nt.test(exp(rt_lexdec) ~ age_subject, data = df_freq_eng)\n\n\n    Welch Two Sample t-test\n\ndata:  exp(rt_lexdec) by age_subject\nt = 63.406, df = 4144.6, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means between group old and group young is not equal to 0\n95 percent confidence interval:\n 152.3128 162.0325\nsample estimates:\n  mean in group old mean in group young \n           786.7200            629.5473 \n\n\nIf we compare this to our model, we see that the t- and p-values are identical (more on these later).\n\ntidy(fit_age)\n\n# A tibble: 2 √ó 5\n  term             estimate std.error statistic p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)          787.      1.75     449.        0\n2 age_subjectyoung    -157.      2.48     -63.4       0\n\n\n\nfig_nocontrasts &lt;-\ndf_freq_eng |&gt; \n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"No contrasts\") +\n  # geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +  \n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",1,0)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Treatment contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_nocontrasts + fig_treatment"
  },
  {
    "objectID": "05-categorical_predictors.html#sum-contrasts",
    "href": "05-categorical_predictors.html#sum-contrasts",
    "title": "5¬† Categorical predictors",
    "section": "5.2 Sum contrasts",
    "text": "5.2 Sum contrasts\nTreatment/dummy coding is the default contrast coding scheme. Sum coding is another frequently used coding scheme, which is essentially centring categorical variables. Just as with continuous variables, the motivation for sum contrast coding mainly lies in the interpretation of interaction effects. How can we tell R we want to use sum contrast coding, and not dummy coding? There are different ways to do this:\n\n# first, make sure your variable is a factor\ndf_freq_eng$age_subject &lt;- as.factor(df_freq_eng$age_subject)\n# check\nclass(df_freq_eng$age_subject)\n\n[1] \"factor\"\n\n\n\n# next, you could use the contr.sum() function\ncontrasts(df_freq_eng$age_subject) &lt;- contr.sum(2) # where 2 means we have 2 levels\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold      1\nyoung   -1\n\n\nHere we see that old is coded as \\(-1\\) and young as \\(+1\\). I prefer to use +/-0.5 for reasons we don‚Äôt need to go into here. I would also prefer to have young coded in the negative value, and old in the positive value. This aids in the way I interpret the slope: a change in reaction times for the older group compared to the younger group.\n\n#or, you could manually control the sum contrasts\n## check the order of the levels\nlevels(df_freq_eng$age_subject)\n\n[1] \"old\"   \"young\"\n\n## code 'old' as +.5 and 'young' as -.5\ncontrasts(df_freq_eng$age_subject) &lt;- c(+0.5, -0.5)\ncontrasts(df_freq_eng$age_subject)\n\n      [,1]\nold    0.5\nyoung -0.5\n\n\nYou could also choose to store the contrast values in their own variable.\n\ndf_freq_eng &lt;- \n  df_freq_eng |&gt; \n  mutate(age_numeric = ifelse(age_subject == \"young\", -0.5, +0.5))\n\n\ndf_freq_eng |&gt; \n  select(age_subject, age_numeric) |&gt; \n  head()\n\n     age_subject age_numeric\n338        young        -0.5\n1790         old         0.5\n3125       young        -0.5\n3957         old         0.5\n3313       young        -0.5\n4145         old         0.5\n\n\nNow, we can run our model using either age_subject or age_numeric.\n\nfit_age_sum &lt;-\n  lm(exp(rt_lexdec) ~ age_subject,\n     data = df_freq_eng)\n\n\nglance(fit_age_sum)$r.squared\n\n[1] 0.4682224\n\nglance(fit_age)$r.squared\n\n[1] 0.4682224\n\n\nNo difference in variance account for by our model.\n\ntidy(fit_age_sum) |&gt; select(term,estimate)\n\n# A tibble: 2 √ó 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 (Intercept)      708.\n2 age_subject1     157.\n\n\nBut there is a difference in the intercept, and a change in sign in our slope. Why is this?\n\nfig_sum1 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-1,1)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_sum5 &lt;-\ndf_freq_eng |&gt; \n  mutate(age_subject = if_else(age_subject==\"young\",-.5,.5)) |&gt;\n  ggplot() +\n  aes(x = age_subject, y = exp(rt_lexdec)) +\n  labs(title = \"Sum contrasts\") +\n  geom_vline(xintercept = 0, linetype=\"dashed\", size = .5) +\n  geom_point(position = position_dodge(.6)) + \n  geom_smooth(method = 'lm', aes(group=1)) + theme_minimal() +\n  theme_bw()\n\nfig_treatment + fig_sum5 + plot_annotation(tag_levels = \"A\")\n\n\n\n\nFigure¬†5.1: The difference in slope corresponds to which level is coded as 0 (dummy coding) or -5/-1 (sum coding)\n\n\n\n\nAs we see in Figure¬†5.1, the sign of the slope depends on how we‚Äôve contrast coded our factor levels. In Figure¬†5.1 A, the old group is coded as \\(0\\) and young as \\(1\\). In Figure¬†5.1 B, the young group is coded as \\(-.5\\) and the old group as \\(+.5\\).\nThe intercept value is also now the overall mean of all observed reaction times, because now the \\(y\\) value when \\(x\\) equals zero lies in the middle of the two groups. The slope magnitude (i.e., size of the value) hasn‚Äôt changed, because the difference betwen the two group means has not changed.\n\nmean(exp(df_freq_eng$rt_lexdec))\n\n[1] 708.1336\n\n\n\n5.2.1 Exploring predicted values\nLet‚Äôs also explore the predicted values of our model with a categorical variable.\n\nhead(fitted(fit_age), n = 10)\n\n     338     1790     3125     3957     3313     4145      337     1789 \n629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 629.5473 786.7200 \n    3513     4345 \n629.5473 786.7200 \n\n\nWe see that there are only 2 values, 630 and 787. These correspond to the means for each group that we saw above. They also seem to be in a pattern: young-mean, old-mean, young-mean, old-mean, etc. How does this correspond to the age group of the participant for the first ten observations?\n\nhead(df_freq_eng$age_subject, n = 10)\n\n [1] young old   young old   young old   young old   young old  \nattr(,\"contrasts\")\n      [,1]\nold    0.5\nyoung -0.5\nLevels: old young\n\n\nThe first ten observations in our data are in young-old pairs. What are the first values in the raw data?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10)\n\n [1] 623.61 775.67 617.10 715.52 575.70 742.19 592.42 748.37 541.67 824.76\n\n\nAnd what is the difference between these reaction times and the fitted values?\n\nhead(exp(df_freq_eng$rt_lexdec), n = 10) - head(fitted(fit_age), n = 10)\n\n       338       1790       3125       3957       3313       4145        337 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991 -37.127299 \n      1789       3513       4345 \n-38.349991 -87.877299  38.040009 \n\n\n\nhead(residuals(fit_age))\n\n       338       1790       3125       3957       3313       4145 \n -5.937299 -11.049991 -12.447299 -71.199991 -53.847299 -44.529991"
  },
  {
    "objectID": "05-categorical_predictors.html#summary",
    "href": "05-categorical_predictors.html#summary",
    "title": "5¬† Categorical predictors",
    "section": "5.3 Summary",
    "text": "5.3 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor\n\n\nImportant terms\n\n\n\n\n\nterm\ndescription/other terms\n\n\n\n\nNA\nNA\n\n\n:----\n:-----------------------"
  },
  {
    "objectID": "05-categorical_predictors.html#learning-objectives-1",
    "href": "05-categorical_predictors.html#learning-objectives-1",
    "title": "5¬† Categorical predictors",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned‚Ä¶"
  },
  {
    "objectID": "05-categorical_predictors.html#task",
    "href": "05-categorical_predictors.html#task",
    "title": "5¬† Categorical predictors",
    "section": "5.4 Task",
    "text": "5.4 Task\nWe‚Äôll use a dataset from Biondo et al. (2022), an eye-tracking reading study exploring the processing of adverb-tense concord in Spanish past and future tenses. Participants read sentences that began with a temporal adverb (e.g., yesterday/tomorrow), and had a verb marked with the congruent or incongruent tense (past/future).\nLoad in the data.\n\ndf_tense &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") # for special characters in Spanish\n           ) |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  clean_names()\n\n\n5.4.1 Treatment contrasts\nWe will look at the measure total reading time (tt) at the verb region (roi == 4). Subset the data to only include the verb region.\n\ndf_verb &lt;-\n  df_tense |&gt; \n  filter(roi == 4)\n\n\nRun a simple linear model with (log-transformed) total reading time (tt) as an independent variable and grammaticality (gramm) as a dependent variable. Use treatment contrasts.\nInspect your coefficients again. What conclusions do you draw?\nRun model diagnostics:\n\ncheck model assumptions where relevant (normality, constant variance, collinearity)\ncheck model fit (\\(R^2\\))\n\n\n\n\n5.4.2 Sum contrasts\n\nRe-run your model with sum contrasts.\nInspect your coefficients again. Do your conclusions change?\nRe-run your model diagnostics. How does it compare to your first model?\n\n\n\n5.4.3 Multiple regression\n\nAdd verb tense (verb_t: past, future) as a predictor, including an interaction term. Use sum contrasts.\nInspect your coefficients again. Do your conclusions change?\nRe-run your model diagnostics. How does it compare to the last models?"
  },
  {
    "objectID": "05-categorical_predictors.html#session-info",
    "href": "05-categorical_predictors.html#session-info",
    "title": "5¬† Categorical predictors",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] kableExtra_1.4.0 knitr_1.43       patchwork_1.2.0  languageR_1.5.0 \n [5] janitor_2.2.0    lme4_1.1-35.3    Matrix_1.7-0     broom_1.0.5     \n [9] here_1.0.1       lubridate_1.9.3  forcats_1.0.0    stringr_1.5.1   \n[13] dplyr_1.1.4      purrr_1.0.2      readr_2.1.5      tidyr_1.3.1     \n[17] tibble_3.2.1     ggplot2_3.5.1    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.5      xfun_0.40         htmlwidgets_1.6.4 lattice_0.22-6   \n [5] tzdb_0.4.0        vctrs_0.6.5       tools_4.4.0       generics_0.1.3   \n [9] parallel_4.4.0    fansi_1.0.6       highr_0.10        pacman_0.5.1     \n[13] pkgconfig_2.0.3   lifecycle_1.0.4   farver_2.1.1      compiler_4.4.0   \n[17] munsell_0.5.1     carData_3.0-5     snakecase_0.11.1  htmltools_0.5.8.1\n[21] yaml_2.3.7        crayon_1.5.2      car_3.1-2         pillar_1.9.0     \n[25] nloptr_2.0.3      MASS_7.3-60.2     abind_1.4-5       boot_1.3-30      \n[29] nlme_3.1-164      tidyselect_1.2.1  digest_0.6.33     stringi_1.8.3    \n[33] labeling_0.4.3    splines_4.4.0     rprojroot_2.0.4   fastmap_1.1.1    \n[37] grid_4.4.0        colorspace_2.1-0  cli_3.6.2         magrittr_2.0.3   \n[41] utf8_1.2.4        withr_3.0.0       scales_1.3.0      backports_1.4.1  \n[45] bit64_4.0.5       timechange_0.3.0  rmarkdown_2.24    bit_4.0.5        \n[49] hms_1.1.3         evaluate_0.21     viridisLite_0.4.2 mgcv_1.9-1       \n[53] rlang_1.1.3       Rcpp_1.0.12       glue_1.7.0        xml2_1.3.6       \n[57] renv_1.0.7        vroom_1.6.5       svglite_2.1.3     rstudioapi_0.16.0\n[61] minqa_1.2.6       jsonlite_1.8.7    R6_2.5.1          systemfonts_1.0.6"
  },
  {
    "objectID": "05-categorical_predictors.html#references",
    "href": "05-categorical_predictors.html#references",
    "title": "5¬† Categorical predictors",
    "section": "References",
    "text": "References\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053"
  },
  {
    "objectID": "06-logistic_regression.html#learning-objectives",
    "href": "06-logistic_regression.html#learning-objectives",
    "title": "6¬† Logistic regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nhow to model binomial data with logistic regression\nhow to interpret log-odds and odds ratio"
  },
  {
    "objectID": "06-logistic_regression.html#set-up-environment",
    "href": "06-logistic_regression.html#set-up-environment",
    "title": "6¬† Logistic regression",
    "section": "Set-up environment",
    "text": "Set-up environment\n\n# suppress scientific notation\noptions(scipen=999)\noptions(pillar.sigfig = 5)\n\n\nlibrary(broman)\n# function to format p-values\nformat_pval &lt;- function(x){\n  if (x &lt; .001) return(paste('&lt;', '.001'))\n  if (x &lt; .01) return(paste('&lt;', '.01'))\n  if (x &lt; .05) return(paste('&lt;', '.05'))\n  paste('=', myround(x, 3))  # if above .05, print p-value to 3 decimalp points\n}\n\nWe‚Äôll also need to load in our required packages. Hopefully you‚Äôve already install the required packages (if not, go to Chapter¬†3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               lme4,\n               janitor,\n               languageR)\n\n\n# set preferred ggplot2 theme\ntheme_set(theme_bw() + theme(plot.title = element_text(size = 10)))"
  },
  {
    "objectID": "06-logistic_regression.html#generalised-linear-models",
    "href": "06-logistic_regression.html#generalised-linear-models",
    "title": "6¬† Logistic regression",
    "section": "6.1 Generalised linear models",
    "text": "6.1 Generalised linear models\nLogistic regression is a type of genearlised linear model (GLM), and is used to model binomial response data. Whereas continuous response variables, such as reaction times, assume a normal distribution (a.k.a., a Gaussian distribution), logistic regression assumes a binomial distribution (a.k.a., Bernoulli distribution). These are formalised in equations \\(\\ref{eq-normal}\\), where \\(\\mu\\) and \\(\\sigma\\) correspond to the mean and standard deviation, and \\(\\ref{eq-binomial}\\), where \\(N\\) and \\(p\\) refer to the number of trials and the probability of \\(y\\) being \\(1\\) or \\(0\\).\n\\[\\begin{align}\ny &\\sim Normal(\\mu,\\sigma) \\label{eq-normal} \\\\\ny &\\sim binomial(N = 1, p) \\label{eq-binomial}\n\\end{align}\\]\nDon‚Äôt stress about this for now, I find the math behind everything will start to make more sense the more often you see it. However, some math is necessary in order to understand the output of our models, namely the relation between probabilities, odds, and log odds.\n\n6.1.1 Log-odds, odds ratio, and probabilities\nIn logistic regression, we the probability (\\(p\\)) of observing one outcome or another as a function of a predictor variable. In linguistic research, these outcomes could be the absence or presence of some phenomenon (pause, schwa, etc.) or button responses (yes/no, accept/reject). In logistic regression, we describe the probability, odds, or log-odds of a particular outcome over another.\nProbability is quite intuitive, and ranges from 0 (no chance) to 1 (certain). A 50% chance corresponds to a probability of 0.5. You‚Äôre also likely familiar with odds, which can range from 0 to infinity. Odds are often used in betting, such as the odds that I‚Äôll win are 2:1, which corresponds to \\(\\frac{2}{1} = 2\\) in favour of my winning. Conversely, the odds that you‚Äôll win are 1:2, corresponding to \\(\\frac{1}{2} = 0.5\\), meaning it‚Äôs less likely that you‚Äôll win compared to you losing. If the odds are even, then: \\(\\frac{1}{1} = 1\\). So, odds of 1 correspond to a probability of 0.5. Log-odds are just the logarithmically-transformed odds: \\(log(2) =\\) 0.6931472; \\(log(0.5) =\\) -0.6931472; \\(log(1) =\\) 0. Probability can also be computed using the odds, as shown in \\(\\ref{eq-odds}\\): \\(\\frac{2}{1+2} =\\) 0.6666667; \\(\\frac{1}{1+1} =\\) 0.5; \\(\\frac{0.5}{1+0.5} =\\) 0.3333333.\nWe can get the probability from a log odds value using plogis(), which performs the following calculation:\n\\[\\begin{equation}\np = \\frac{exp(log\\;odds)}{1 + exp(log\\;odds)} = \\frac{odds}{1 + odds} \\label{eq-odds}\n\\end{equation}\\]\nTable¬†6.1 gives an example of how the three relate to each other. The grey cells are all where chances re 50/50, with increasingly more likely (green) or less likely (red) values/\n\n\n\n\nTable¬†6.1: Comparison of different values of probabilities/odds/log-odds\n\n\nname\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nprob\n0.0066929\n0.0229774\n0.0758582\n0.2227001\n0.5\n0.7772999\n0.9241418\n0.9770226\n0.9933071\n\n\nodds\n0.0067379\n0.0235177\n0.0820850\n0.2865048\n1.0\n3.4903430\n12.1824940\n42.5210820\n148.4131591\n\n\nlog_odds\n-5.0000000\n-3.7500000\n-2.5000000\n-1.2500000\n0.0\n1.2500000\n2.5000000\n3.7500000\n5.0000000\n\n\n\n\n\n\n\n\nThis relationship is demonstrated in Figure¬†6.1. Take your time to really understand these plots, as it will help understand the output of our models.\n\n\n\n\n\nFigure¬†6.1: Relationship between probability, odds, and log-odds"
  },
  {
    "objectID": "06-logistic_regression.html#logistic-regression",
    "href": "06-logistic_regression.html#logistic-regression",
    "title": "6¬† Logistic regression",
    "section": "6.2 Logistic regression",
    "text": "6.2 Logistic regression\nI find the more we talk about the math behind the models before even running a model, the more overwhelmed we become. So, let‚Äôs run our first logistic regression and then dissect it to understand it. Most relevant to the output of a logistic regression model is Figure¬†6.1 C, as the model will output log-odds, and we most likely want to interpret them in terms of probabilities.\nWe‚Äôll use a dataset from Biondo et al. (2022), an eye-tracking reading study exploring the processing of adverb-tense concord in Spanish past and future tenses. Participants read sentences that began with a temporal adverb (e.g., yesterday/tomorrow), and had a verb marked with the congruent or incongruent tense (past/future). We will look at the measure regression in at the verb region.\nLet‚Äôs start by loading in the data:\n\ndf_tense &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") # for special characters in Spanish\n           ) |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  clean_names() |&gt; \n  filter(roi == 4,\n         adv_type == \"Deic\")\n\n\n6.2.1 EDA\nAnd conducting a quick EDA: print summaries and plot the response variables.\n\nhead(df_tense)\n\n# A tibble: 6 √ó 13\n  sj     item adv_type adv_t  verb_t gramm     roi label    fp    gp    tt    ri\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        54 Deic     Past   Past   gramm       4 enca‚Ä¶  1027  1027  1027     0\n2 1         4 Deic     Future Future gramm       4 cole‚Ä¶   562   562  1337     1\n3 1        62 Deic     Past   Past   gramm       4 esqu‚Ä¶   293  1664  1141     0\n4 1        96 Deic     Future Past   ungramm     4 cons‚Ä¶   713  1963  1868     0\n5 1        52 Deic     Past   Past   gramm       4 desa‚Ä¶   890   890  1707     1\n6 1        90 Deic     Future Past   ungramm     4 dece‚Ä¶   962   962   962     0\n# ‚Ñπ 1 more variable: ro &lt;dbl&gt;\n\n\nLet‚Äôs look at only our binomial dependent variables, regression in (ri) and regression out (ro). For each variable, 1 indicates a regression in/out, 0 indicates there was no regression in/out.\n\ndf_tense |&gt; \n  select(roi, ri, ro) |&gt; \n  summary()\n\n      roi          ri               ro         \n Min.   :4   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:4   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :4   Median :0.0000   Median :0.00000  \n Mean   :4   Mean   :0.2248   Mean   :0.08169  \n 3rd Qu.:4   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :4   Max.   :1.0000   Max.   :1.00000  \n             NA's   :45       NA's   :45       \n\n\nLet‚Äôs plot out our dependent variable of interest: regression in.\n\n# make grammaticality a factor\ndf_tense |&gt; \n  mutate(gramm = as_factor(gramm)) \n\n# A tibble: 3,840 √ó 13\n   sj     item adv_type adv_t  verb_t gramm    roi label    fp    gp    tt    ri\n   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  &lt;fct&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1        54 Deic     Past   Past   gramm      4 enca‚Ä¶  1027  1027  1027     0\n 2 1         4 Deic     Future Future gramm      4 cole‚Ä¶   562   562  1337     1\n 3 1        62 Deic     Past   Past   gramm      4 esqu‚Ä¶   293  1664  1141     0\n 4 1        96 Deic     Future Past   ungra‚Ä¶     4 cons‚Ä¶   713  1963  1868     0\n 5 1        52 Deic     Past   Past   gramm      4 desa‚Ä¶   890   890  1707     1\n 6 1        90 Deic     Future Past   ungra‚Ä¶     4 dece‚Ä¶   962   962   962     0\n 7 1         8 Deic     Future Future gramm      4 evid‚Ä¶   718   718   718     0\n 8 1         9 Deic     Future Future gramm      4 excu‚Ä¶  1453  1453  1453     0\n 9 1        56 Deic     Past   Past   gramm      4 equi‚Ä¶   769   769   769     0\n10 1        11 Deic     Future Future gramm      4 cena‚Ä¶   778   778   778     0\n# ‚Ñπ 3,830 more rows\n# ‚Ñπ 1 more variable: ro &lt;dbl&gt;\n\n\n\n\n\n\n\nIt looks like there are more regressions in for the grammatical versus ungrammatical conditions in both the future and past tenses. There doesn‚Äôt seem to be a large difference between the two tenses in overall regressions in, nor in the effect of grammaticality on the proportion of regressions in. We can also see that it was more likely overall for there to not be a regression in (versus for there to be a regression in).\n\n\n6.2.2 Model\nNow let‚Äôs run our model. Verb tense and grammaticality are each two-level factors, so we‚Äôll want to set sum coding for each of them. Let‚Äôs set past and grammatical to \\(-0.5\\), and future and ungrammatical to +0.5.\n\ndf_tense$verb_t &lt;- as.factor(df_tense$verb_t)\nlevels(df_tense$verb_t)\n\n[1] \"Future\" \"Past\"  \n\ncontrasts(df_tense$verb_t) &lt;- c(+0.5, -0.5)\ncontrasts(df_tense$verb_t)\n\n       [,1]\nFuture  0.5\nPast   -0.5\n\n\n\ndf_tense$gramm &lt;- as.factor(df_tense$gramm)\nlevels(df_tense$gramm)\n\n[1] \"gramm\"   \"ungramm\"\n\ncontrasts(df_tense$gramm) &lt;- c(-0.5, +0.5)\ncontrasts(df_tense$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5\n\n\nNow that we‚Äôve set our contrasts (if you have continuous predictors, you would centre and potentially standardise them instead), we can fit our model. We use the glm() function to fit a genearlised linear model, and use the argument family = \"binomial\" to indicate our data are binomial.\n\nfit_tense_ri &lt;-\n  glm(ri ~ verb_t*gramm,\n    data = df_tense,\n    family = \"binomial\")\n\nWhat do our coefficients look like?\n\ntidy(fit_tense_ri) %&gt;%\n  mutate(p.value = as.numeric(p.value)) |&gt; \n  mutate(p.value = round(p.value*4,10)\n         ) |&gt; \n  knitr::kable() |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.2472175\n0.0392027\n-31.8146220\n0.0000000\n\n\nverb_t1\n0.0209637\n0.0784053\n0.2673755\n3.1567201\n\n\ngramm1\n0.3668992\n0.0784053\n4.6795205\n0.0000115\n\n\nverb_t1:gramm1\n-0.1197221\n0.1568106\n-0.7634823\n1.7807033\n\n\n\n\n\n\n\nLet‚Äôs first consider the estimates. The intercept is negative, meaning it is below 0. Verb tense is positive, meaning that there are more regressions in for the future compared to the past, holding grammaticality constant. Grammaticality is positive, meaning that there were more regressions in for the ungrammatical than grammatical conditions. But what does zero mean here? Logistic regression gives the estimates in log-odds. This means that a value of 0 means there is an equal probability of a regression in or out for both conditions (as in (tab-odds?)), i.e., the slope is flat (or not significantly different from 0). How can we convert our log-odds estimates to something more interpretable, like probabilities? Recall the equation in \\(\\ref{eq-odds}\\), which would require a lot of typing. Luckily, we can just use the plogis() function, which takes a log-odds value and spits out the corresponding probability. We can also just use the exp() function to get the odds ratio from the log-odds.\n\nplogis(-1.23) # intercept prob\n\n[1] 0.2261814\n\nplogis(0.0277) # tense prob\n\n[1] 0.5069246\n\nexp(-1.23) # intercept odds\n\n[1] 0.2922926\n\nexp(0.0277) # tense odds\n\n[1] 1.028087\n\n\nThis is great, but a bit tedious. We can also just feed a tibble column through the plogis() and exp() functions to print a table with the relevant probabilities and odds.\n\ntidy(fit_tense_ri) %&gt;%\n  mutate(p.value = round(p.value*4,10),\n         prob = plogis(estimate),\n         odds = exp(estimate)\n         ) |&gt; \n  mutate_if(is.numeric, round, 4) |&gt; \n  knitr::kable() |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nprob\nodds\n\n\n\n\n(Intercept)\n-1.2472\n0.0392\n-31.8146\n0.0000\n0.2232\n0.2873\n\n\nverb_t1\n0.0210\n0.0784\n0.2674\n3.1567\n0.5052\n1.0212\n\n\ngramm1\n0.3669\n0.0784\n4.6795\n0.0000\n0.5907\n1.4433\n\n\nverb_t1:gramm1\n-0.1197\n0.1568\n-0.7635\n1.7807\n0.4701\n0.8872\n\n\n\n\n\n\n\nWe see that the odds of the future tense have a regression in versus the past tense is ~1, with the corresponding probability of 0.51. Unsuprisingly, we see this p-value indicates this effect was not significant (p &gt; .05), and the z-value (note: not t-value!) is also low.\n\n\n\n\n\n\nz-values\n\n\n\nz-values correspond to the estimate divided by the standard error. It‚Äôs interpretation is similar to that of the t-value: a z-value of ~2 or higher will likely have a p-value below 0.05.\n\n\nThe interaction term is negative, what does this mean? We can interpret this as indicating that the effect of congruence is different in either level of tense. These effects are often more easily interpreted with a visualisation, e.g., using the plot_model() function from the sjPlot package. This effect is not significant, however.\n\nsjPlot::plot_model(fit_tense_ri, \n                   type = \"eff\", \n                   terms = c(\"gramm\", \"verb_t\")) + \n  geom_line(position = position_dodge(0.1))\n\n\n\n\nFigure¬†6.2: Interaction plot of\n\n\n\n\nWe can also use the predict() function to extract the predicted values for each condition. We could just simply print the predicted values (predict(fit_tense_ri)), append the predicted values to the data frame\n\n# make sure dataset is the same length as the model data\ndf_tense_v &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri)\n\n# append model estimates\naugment(fit_tense_ri, data = df_tense_v) |&gt; \n  distinct(verb_t, gramm, .keep_all = T) |&gt;\n  arrange(verb_t) |&gt;  \n  select(verb_t, gramm, .fitted)\n\n# A tibble: 4 √ó 3\n  verb_t gramm   .fitted\n  &lt;fct&gt;  &lt;fct&gt;     &lt;dbl&gt;\n1 Future gramm   -1.3903\n2 Future ungramm -1.0832\n3 Past   gramm   -1.4711\n4 Past   ungramm -1.0443\n\n\nOr we could create a list of the unique conditions.\n\ndf_sim &lt;-\n    tibble(\n    verb_t = rep(c('Past', 'Future'), each = 2),\n    gramm = rep(c('0', '1'), times = 2))\n\n# alternatively, just extract the relevant factor levels from your datafram\ndf_sim &lt;-\n  df_tense |&gt; \n  arrange(verb_t) |&gt; \n  distinct(verb_t, gramm) \n\n# and add predicted values\ndf_sim$fit &lt;- num(predict(fit_tense_ri, df_sim), digits = 5)\n\ndf_sim\n\n# A tibble: 4 √ó 3\n  verb_t gramm         fit\n  &lt;fct&gt;  &lt;fct&gt;   &lt;num:.5!&gt;\n1 Future gramm    -1.39025\n2 Future ungramm  -1.08322\n3 Past   gramm    -1.47108\n4 Past   ungramm  -1.04432\n\n\nAnd now if we look at the predicted log-odds values for the future and past tenses:\n\ndf_sim |&gt; \n  summarise(\n    mean_tense = mean(fit),\n    .by = verb_t)\n\n# A tibble: 2 √ó 2\n  verb_t mean_tense\n  &lt;fct&gt;   &lt;num:.5!&gt;\n1 Future   -1.23674\n2 Past     -1.25770\n\n\nWhat is the difference between these two numbers (in our model summary)?\n\ndf_sim |&gt; \n  summarise(\n    mean_tense = mean(fit),\n    .by = gramm)\n\n# A tibble: 2 √ó 2\n  gramm   mean_tense\n  &lt;fct&gt;    &lt;num:.5!&gt;\n1 gramm     -1.43067\n2 ungramm   -1.06377\n\n\nWhat is the difference between these two numbers (in our model summary)?\nSo, our slopes for verb_t and gramm correspond to the predicted difference between the two levels of each factor."
  },
  {
    "objectID": "06-logistic_regression.html#interpreting-our-coefficients",
    "href": "06-logistic_regression.html#interpreting-our-coefficients",
    "title": "6¬† Logistic regression",
    "section": "6.3 Interpreting our coefficients",
    "text": "6.3 Interpreting our coefficients\nWhat do our coefficient estimates reflect, though? Let‚Äôs remind ourselves of the rate of regressions in at the verb region:\n\ndf_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  summary()\n\n      sj                 item          adv_type            adv_t          \n Length:3795        Min.   :  1.00   Length:3795        Length:3795       \n Class :character   1st Qu.: 25.00   Class :character   Class :character  \n Mode  :character   Median : 52.00   Mode  :character   Mode  :character  \n                    Mean   : 50.91                                        \n                    3rd Qu.: 78.00                                        \n                    Max.   :101.00                                        \n    verb_t         gramm           roi       label                 fp        \n Future:1897   gramm  :1901   Min.   :4   Length:3795        Min.   :  81.0  \n Past  :1898   ungramm:1894   1st Qu.:4   Class :character   1st Qu.: 266.0  \n                              Median :4   Mode  :character   Median : 371.0  \n                              Mean   :4                      Mean   : 440.5  \n                              3rd Qu.:4                      3rd Qu.: 535.0  \n                              Max.   :4                      Max.   :2833.0  \n       gp             tt               ri               ro         \n Min.   :  87   Min.   :  90.0   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.: 285   1st Qu.: 326.5   1st Qu.:0.0000   1st Qu.:0.00000  \n Median : 405   Median : 493.0   Median :0.0000   Median :0.00000  \n Mean   : 510   Mean   : 607.4   Mean   :0.2248   Mean   :0.08169  \n 3rd Qu.: 606   3rd Qu.: 747.0   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :3877   Max.   :3936.0   Max.   :1.0000   Max.   :1.00000  \n\nptab_gramm &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  select(gramm, ri) |&gt; \n  table() |&gt; \n  prop.table()\n\nptab_tense &lt;-\n  df_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  select(verb_t, ri) |&gt; \n  table() |&gt; \n  prop.table()\n\ndf_tense |&gt; \n  filter(roi == \"4\") |&gt; \n  drop_na(ri) |&gt; \n  tabyl(gramm, ri, verb_t) |&gt; \n  adorn_percentages() |&gt; \n  adorn_totals()\n\n$Future\n   gramm         0         1\n   gramm 0.8006329 0.1993671\n ungramm 0.7471022 0.2528978\n   Total 1.5477351 0.4522649\n\n$Past\n   gramm         0         1\n   gramm 0.8132214 0.1867786\n ungramm 0.7396825 0.2603175\n   Total 1.5529039 0.4470961\n\n\nWe want to measure how much more likely a regression in (y = 1) is for ungrammatical conditions (x = 1) than in grammatical conditions (x = 0). Si we want to calculate the odds of a regression in for each case, and take their ratio:\n\n# odds(y = 1 | x = 0)\nodds_ri1_gramm0 &lt;- \n  ptab_gramm[1,2] / ptab_gramm[1,1] # in gramm conditions: ri 0/1\nodds_ri1_gramm1 &lt;- \n  ptab_gramm[2,2] / ptab_gramm[2,1] # in ungramm condiitons: ri 0/1\n\n## odds ratio\nodds_ri1_gramm1 / odds_ri1_gramm0\n\n[1] 1.442756\n\n## log odds\nlog(odds_ri1_gramm1) - log(odds_ri1_gramm0)\n\n[1] 0.3665552\n\n# or\nlog(odds_ri1_gramm1 / odds_ri1_gramm0)\n\n[1] 0.3665552\n\n## probability\nplogis(log(odds_ri1_gramm1 / odds_ri1_gramm0))\n\n[1] 0.5906263\n\n\nSo the odds of a regression into the verb region is 1.4 times more likely in ungrammatical versus grammatical conditions.\n\nintercept &lt;- tidy(fit_tense_ri)$estimate[1]\ntense &lt;- tidy(fit_tense_ri)$estimate[2]\ngramm &lt;- tidy(fit_tense_ri)$estimate[3]\ninteract &lt;- tidy(fit_tense_ri)$estimate[4]\n\nWhat are the log odds for the past (tense = -0.5) grammatical (gramm = -0.5)?\n\nplogis(intercept)\n\n[1] 0.2231822\n\nplogis(tense)\n\n[1] 0.5052407\n\nplogis(gramm)\n\n[1] 0.5907095\n\nplogis(interact)\n\n[1] 0.4701052\n\n\n\ntidy(fit_tense_ri) |&gt; \n  mutate(prob = plogis(estimate))\n\n# A tibble: 4 √ó 6\n  term            estimate std.error statistic     p.value    prob\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    -1.2472    0.039203 -31.815   4.0637e-222 0.22318\n2 verb_t1         0.020964  0.078405   0.26738 7.8918e-  1 0.50524\n3 gramm1          0.36690   0.078405   4.6795  2.8755e-  6 0.59071\n4 verb_t1:gramm1 -0.11972   0.15681   -0.76348 4.4518e-  1 0.47011\n\n\n\nplogis(intercept + tense*-.5 + gramm*-.5)\n\n[1] 0.1913675\n\n\nAnd past ungrammatical (gramm = +0.5)?\n\nplogis(intercept + tense*-.5 + gramm*.5)\n\n[1] 0.2545957\n\n\nAnd for the future conditions?\n\nplogis(intercept + tense*.5 + gramm*-.5)\n\n[1] 0.1946325\n\n\nAnd past ungrammatical (gramm = +0.5)?\n\nplogis(intercept + tense*.5 + gramm*.5)\n\n[1] 0.2585946\n\n\n\nplogis(-1.22521)\n\n[1] 0.2270209\n\n\n\nplogis(-1.22521)\n\n[1] 0.2270209\n\n\n\\[\\begin{align}\np &= \\frac{odds}{1 + odds} \\label{eq1}\\\\\nodds &= \\frac{p}{1-p} \\label{eq2}\\\\\nlog\\;odds &= exp(odds) \\label{eq3}\n\\end{align}\\]"
  },
  {
    "objectID": "06-logistic_regression.html#visualising-model-predictions",
    "href": "06-logistic_regression.html#visualising-model-predictions",
    "title": "6¬† Logistic regression",
    "section": "6.4 Visualising model predictions",
    "text": "6.4 Visualising model predictions\nSomething we haven‚Äôt really covered is how to visualise our model predictions. So far we‚Äôve only visualised the raw data, but when interpreting model results it helps to see the predictions. This is especially true for logistic regression, because our estimates are given in log odds, which are not very intuitive.\nWe can use the sjPlot package, which is very handy:\n\nlibrary(sjPlot)\n\nplot_model(fit_tense_ri)\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"eff\",\n           terms = \"verb_t\")\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"eff\",\n           terms = \"gramm\")\n\n\n\n\n\nplot_model(fit_tense_ri, type = \"int\")\n\n\n\n\nOr we can use the ggeffects package to extract summaries of effects, and then feed them into ggplot2.\n\nlibrary(ggeffects)\n\n\nggeffect(fit_tense_ri)\n\n$verb_t\n# Predicted probabilities of ri\n\nverb_t | Predicted |     95% CI\n-------------------------------\nFuture |      0.22 | 0.21, 0.24\nPast   |      0.22 | 0.20, 0.24\n\n\n$gramm\n# Predicted probabilities of ri\n\ngramm   | Predicted |     95% CI\n--------------------------------\ngramm   |      0.19 | 0.18, 0.21\nungramm |      0.26 | 0.24, 0.28\n\n\nattr(,\"class\")\n[1] \"ggalleffects\" \"list\"        \nattr(,\"model.name\")\n[1] \"fit_tense_ri\"\n\nggeffect(fit_tense_ri,\n         terms = c(\"gramm\", \"verb_t\"))\n\n# Predicted probabilities of ri\n\nverb_t: Future\n\ngramm   | Predicted |     95% CI\n--------------------------------\ngramm   |      0.20 | 0.18, 0.23\nungramm |      0.25 | 0.23, 0.28\n\nverb_t: Past\n\ngramm   | Predicted |     95% CI\n--------------------------------\ngramm   |      0.19 | 0.16, 0.21\nungramm |      0.26 | 0.23, 0.29"
  },
  {
    "objectID": "06-logistic_regression.html#reporting",
    "href": "06-logistic_regression.html#reporting",
    "title": "6¬† Logistic regression",
    "section": "6.5 Reporting",
    "text": "6.5 Reporting\nSonderegger (2023) (Section 6.9) says the following:\n\nReporting a logistic regression model in a write-up is generally similar to reporting a linear regression model‚Ä¶Reporting a logistic regression model in a write-up is generally similar to reporting a linear regression model: the guidelines and rationale in section 4.6 for reporting individual coefficients and the whole model hold, with some adjustments.\n\nWe can produce such a table using the papaja package, as in Table¬†6.2.\n\nlibrary(papaja)\n\nfit_tense_ri |&gt; \n  apa_print() |&gt;\n  apa_table(caption = \"Model summary for regressions in at the verb region. Estimates are given in log odds.\")\n\n\nTable¬†6.2: ?(caption)\n\n\n\n\n(a) (#tab:tbl-glm-summary) Model summary for regressions in at the verb region. Estimates are given in log odds.\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(z\\)\n\\(p\\)\n\n\n\n\nIntercept\n-1.25\n[-1.32, -1.17]\n-31.81\n&lt; .001\n\n\nVerb t1\n0.02\n[-0.13, 0.17]\n0.27\n.789\n\n\nGramm1\n0.37\n[0.21, 0.52]\n4.68\n&lt; .001\n\n\nVerb t1 \\(\\times\\) Gramm1\n-0.12\n[-0.43, 0.19]\n-0.76\n.445\n\n\n\n\n\n\n\nOr by extracting the model summary with tidy(), and even adding our probabilities, as in Table¬†6.3.\n\ntidy(fit_tense_ri) |&gt; \n  mutate(prob = plogis(estimate)) |&gt; \n  relocate(prob, .after = std.error) |&gt; \n  apa_table()\n\n\nTable¬†6.3: ?(caption)\n\n\n\n\n(a) (#tab:tbl-glm-summary2)\n\n\nterm\nestimate\nstd.error\nprob\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.25\n0.04\n0.22\n-31.81\n0.00\n\n\nverb_t1\n0.02\n0.08\n0.51\n0.27\n0.79\n\n\ngramm1\n0.37\n0.08\n0.59\n4.68\n0.00\n\n\nverb_t1:gramm1\n-0.12\n0.16\n0.47\n-0.76\n0.45"
  },
  {
    "objectID": "06-logistic_regression.html#summary",
    "href": "06-logistic_regression.html#summary",
    "title": "6¬† Logistic regression",
    "section": "6.6 Summary",
    "text": "6.6 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor\n\n\nImportant terms\n\n\n\n\n\nterm\ndescription/other terms\n\n\n\n\nNA\nNA\n\n\n:----\n:-----------------------"
  },
  {
    "objectID": "06-logistic_regression.html#important-terms-1",
    "href": "06-logistic_regression.html#important-terms-1",
    "title": "6¬† Logistic regression",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    Bernoulli distribution\nNA\nNA\n    plogis()\nNA\nNA\n    log odds\nNA\nNA"
  },
  {
    "objectID": "06-logistic_regression.html#learning-objectives-1",
    "href": "06-logistic_regression.html#learning-objectives-1",
    "title": "6¬† Logistic regression",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we learned‚Ä¶\n\nhow to model binomial data with logistic regression\nhow to interpret log-odds and odds ratio"
  },
  {
    "objectID": "06-logistic_regression.html#task",
    "href": "06-logistic_regression.html#task",
    "title": "6¬† Logistic regression",
    "section": "Task",
    "text": "Task\n\n6.6.1 Regressions out\nUsing the same dataset, run a logistic model exploring regressions in (ri) at the adverb region (roi = \"2\"). Before you run the model, do you have any predictions? Try plotting the regressions in for this region first, and generate some summary tables to get an idea of the distributions of regressions in across conditions.\n\n\n6.6.2 Dutch verb regularity\nLoad in the regularity data from the languageR package.\n\ndf_reg &lt;-\n  regularity |&gt; \n  clean_names()\n\n\nRegular and irregular Dutch verbs and selected lexical and distributional properties.\n\nOur relevant variables will be:\n\nwritten_frequency: a numeric vector of logarithmically transformed frequencies in written Dutch (as available in the CELEX lexical database).\nregularity: a factor with levels irregular (1) and regular (0).\nverb: a factor with the verbs as levels.\n\n\nFit a logistic regression model to the data which predicts verb regularity by written frequency. Consider: What type of predictor variable do you have, and what steps should you take before fitting your model?\nPrint the model coefficients, e.g., using tidy().\nInterpret the coefficients, either in log-odds or probabilities. Report your findings."
  },
  {
    "objectID": "06-logistic_regression.html#session-info",
    "href": "06-logistic_regression.html#session-info",
    "title": "6¬† Logistic regression",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] papaja_0.1.2        tinylabels_0.2.4    ggeffects_1.5.2    \n [4] sjPlot_2.8.15       gt_0.10.1           googlesheets4_1.1.1\n [7] kableExtra_1.4.0    knitr_1.43          patchwork_1.2.0    \n[10] languageR_1.5.0     janitor_2.2.0       lme4_1.1-35.3      \n[13] Matrix_1.7-0        broom_1.0.5         here_1.0.1         \n[16] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[19] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[22] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[25] tidyverse_2.0.0     broman_0.80        \n\nloaded via a namespace (and not attached):\n [1] DBI_1.2.2          rlang_1.1.3        magrittr_2.0.3     snakecase_0.11.1  \n [5] compiler_4.4.0     systemfonts_1.0.6  vctrs_0.6.5        pkgconfig_2.0.3   \n [9] crayon_1.5.2       fastmap_1.1.1      backports_1.4.1    labeling_0.4.3    \n[13] effectsize_0.8.7   utf8_1.2.4         rmarkdown_2.24     tzdb_0.4.0        \n[17] haven_2.5.4        nloptr_2.0.3       bit_4.0.5          xfun_0.40         \n[21] jsonlite_1.8.7     highr_0.10         sjmisc_2.8.9       parallel_4.4.0    \n[25] R6_2.5.1           RColorBrewer_1.1-3 stringi_1.8.3      boot_1.3-30       \n[29] cellranger_1.1.0   estimability_1.5   Rcpp_1.0.12        modelr_0.1.11     \n[33] parameters_0.21.6  pacman_0.5.1       nnet_7.3-19        splines_4.4.0     \n[37] timechange_0.3.0   tidyselect_1.2.1   rstudioapi_0.16.0  effects_4.2-2     \n[41] yaml_2.3.7         sjlabelled_1.2.0   curl_5.2.1         lattice_0.22-6    \n[45] withr_3.0.0        bayestestR_0.13.2  coda_0.19-4.1      evaluate_0.21     \n[49] survival_3.5-8     survey_4.4-2       xml2_1.3.6         pillar_1.9.0      \n[53] carData_3.0-5      renv_1.0.7         insight_0.19.10    generics_0.1.3    \n[57] vroom_1.6.5        rprojroot_2.0.4    hms_1.1.3          munsell_0.5.1     \n[61] scales_1.3.0       minqa_1.2.6        xtable_1.8-4       glue_1.7.0        \n[65] emmeans_1.10.1     tools_4.4.0        fs_1.6.3           mvtnorm_1.2-4     \n[69] grid_4.4.0         mitools_2.4        datawizard_0.10.0  colorspace_2.1-0  \n[73] nlme_3.1-164       performance_0.11.0 googledrive_2.1.1  cli_3.6.2         \n[77] fansi_1.0.6        gargle_1.5.2       viridisLite_0.4.2  svglite_2.1.3     \n[81] sjstats_0.18.2     gtable_0.3.5       sass_0.4.7         digest_0.6.33     \n[85] htmlwidgets_1.6.4  farver_2.1.1       htmltools_0.5.8.1  lifecycle_1.0.4   \n[89] httr_1.4.7         bit64_4.0.5        MASS_7.3-60.2"
  },
  {
    "objectID": "06-logistic_regression.html#references",
    "href": "06-logistic_regression.html#references",
    "title": "6¬† Logistic regression",
    "section": "References",
    "text": "References\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "reports/report 1/report1.html#dataset",
    "href": "reports/report 1/report1.html#dataset",
    "title": "7¬† Report 1",
    "section": "7.1 Dataset",
    "text": "7.1 Dataset\nFor this report you will continue using the data from Biondo et al. (2022), an eye-tracking reading study on adverb-tense congruence effects on reading time measures. Participants‚Äô eye movements were recorded as they read Spanish sentences where temporal adverbs and verb tense were either congruent or incongruent. For both sentence regions, the time reference was either past (e.g., yesterda, bought) or future (e.g., tomorrow, will buy). Example stimuli from this experiment are given in Table¬†14.1. You will be fitting models to different eye-tracking reading measures from this experiment, with the predictors verb tense and grammaticality.\n\n\n\n\nTable¬†7.1: Example stimuli\n\n\nsentence\nadverb\nverb\ngramm\n\n\n\n\nA la salida del trabajo, **ayer** las chicas **compraron** pan en la tienda.&lt;br&gt; *After leaving work* **yesterday** *the girls* **bought** *bread at the shop*\npast\npast\ngramm\n\n\nA la salida del trabajo, **ayer** las chicas **\\*comprar√°n** pan en la tienda.&lt;br&gt; *After leaving work* **yesterday** *the girls* **\\*will buy** *bread at the shop*\npast\nfuture\nungramm\n\n\nA la salida del trabajo, **ma√±ana** las chicas **comprar√°n** pan en la tienda.&lt;br&gt; *After leaving work* **tomorrow** *the girls* **will buy** *bread at the shop*\nfuture\nfuture\ngramm\n\n\nA la salida del trabajo, **ma√±ana** las chicas **\\*compraron** pan en la tienda.&lt;br&gt; *After leaving work* **tomorrow** *the girls* **\\*bought** *bread at the shop*\nfuture\npast\nungramm"
  },
  {
    "objectID": "reports/report 1/report1.html#set-up",
    "href": "reports/report 1/report1.html#set-up",
    "title": "7¬† Report 1",
    "section": "7.2 Set-up",
    "text": "7.2 Set-up\nMake sure you begin with a clear working environment. To achieve this, you can go to Session &gt; Restart R. Your Environment should have no objects in it, and you should not have any packages loaded.\n\n7.2.1 Packages\nLoad the packages below. Give a short description of why we load in each package, i.e., what these packages help us do (1-2 sentences each). Tip: remember you can type ?tidyverse into the Console to get an overview of a package or function.\n\npacman::p_load(\n  tidyverse,\n  janitor,\n  here,\n  broom\n)\n\n\ntidyverse:\njanitor:\nhere:\n\n\n\n7.2.2 Data\nRun the code below. Give a short description of what each line of code does (you can skip the locale line). Tip: roi == 2 corresponds to the temporal adverb condiiton.\n\ndf_tense &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ## for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\"))  |&gt; \n  filter(roi == 2,\n         adv_type == \"Deic\") |&gt; \n  mutate(length = nchar(label))\n\nYou can write your answer like this, for example:\n\ndf_tense &lt;- :\nread_csv:\nclean_names:\nmutate:\nfilter:"
  },
  {
    "objectID": "reports/report 1/report1.html#how-to-report-your-models",
    "href": "reports/report 1/report1.html#how-to-report-your-models",
    "title": "7¬† Report 1",
    "section": "7.3 How to report your models",
    "text": "7.3 How to report your models\nYou will be running linear and logisitc regression models. Our variables of interest will be:\n\n\n\n\n\nvariable\ndescription\ntype\nclass\n\n\n\n\n`fp`\nfirst-pass reading time (summation of fixations from when a reader first fixates on a region to when they first leave that region)\ndependent\ncontinuous\n\n\n`tt`\ntotal reading time (summation of all fixations within a region during a trial)\ndependent\ncontinuous\n\n\n`ri`\nregressions in (whether there was at least one regression into a region)\ndependent\nbinomial\n\n\n`ro`\nregressions out (whether there was at least one regression out of a region)\ndependent\nbinomial\n\n\n`verb_t`\nverb tense: past or future\nindependent\ncategorical\n\n\n`gramm`\ngrammaticality: grammatical or ungrammatical\nindependent\ncategorical\n\n\n`length`\nregion length in letters\nindependent\ncontinuous\n\n\n\n\n\n\n\n\n7.3.1 Example model report\nImagine we fit a linear model, called fit_verb_tt, to log-tranformed total reading times at the verb region. Our fixed effects (i.e., predictors) are verb tense, grammaticality, and their interaction. Below I report the findings of the model, which is what you should aim to do with the models you run.\n\nThe model summary is given in ?tbl-fit_verb_tt, with back-transformed model predictions visualised in Figure¬†7.1. A main effect of grammaticality was found in total reading times at the verb region, with ungrammatical conditions eliciting longer total reading times than grammatical conditions (Est = -0.06, t = -2.4, p &lt; 0.05). A main effect of tense was also found, with the future condition eliciting longer total reading times than the past condition (Est = 0.07, t = 2.48, p &lt; .05). An interaction of tense and grammaticality was not significant (Est = 0.04, t = 1).\n\n\nlibrary(papaja)\n\nfit_verb_tt |&gt; \n  apa_print() |&gt;\n  apa_table(label = \"tbl-fit_verb_tt\",\n            caption = \"Model summary for (log-transformed) total reading times at the verb region.\")\n\n\n(#tab:unnamed-chunk-12) Model summary for (log-transformed) total reading times at the verb region.\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n6.22\n[6.19, 6.26]\n332.72\n3791\n&lt; .001\n\n\nVerb tPast\n-0.06\n[-0.11, -0.01]\n-2.38\n3791\n.017\n\n\nGrammungramm\n0.07\n[0.01, 0.12]\n2.47\n3791\n.013\n\n\nVerb tPast \\(\\times\\) Grammungramm\n0.04\n[-0.04, 0.11]\n1.01\n3791\n.312\n\n\n\n\n\n\nlibrary(sjPlot)\nplot_model(fit_verb_tt, type = \"int\") + \n  geom_line(position = position_dodge(0.1)) +\n  labs(title = \"Predicted total reading times at the verb region\",\n       x = \"Verb tense\",\n       y = \"Reading time (ms)\") +\n  theme_bw()\n\n\n\n\nFigure¬†7.1: Back-transformed model predictions for total reading time at the verb region (with 95% confidence intervals)."
  },
  {
    "objectID": "reports/report 1/report1.html#variable-prep",
    "href": "reports/report 1/report1.html#variable-prep",
    "title": "7¬† Report 1",
    "section": "7.4 Variable prep",
    "text": "7.4 Variable prep\nWe need to prepare our predictors: centering continuous variables and sum contrast coding categorical variables.\n\n7.4.1 Centring continuous variables\nCreate a new variable length_c which contains the centred values of length (just centre, you don‚Äôt need to standardise). Centre using the median rather than the mean (hint: there is a function median()).\n\n\n7.4.2 Contrast coding\nSet sum contrast coding for tense and gramm. You might need to first use the as.factor() function to save the variables as factors. Your contrasts should look like this:\n\ncontrasts(df_tense$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5\n\ncontrasts(df_tense$verb_t)\n\n       [,1]\nFuture  0.5\nPast   -0.5"
  },
  {
    "objectID": "reports/report 1/report1.html#linear-regression",
    "href": "reports/report 1/report1.html#linear-regression",
    "title": "7¬† Report 1",
    "section": "7.5 Linear regression",
    "text": "7.5 Linear regression\nWe run linear regression when we have a continuous dependent variable. You will be fitting a. model to first-pass reading time.\n\n7.5.1 Fitting our model\nFit a model of log-transformed first-pass reading times with verb tense, grammaticality, and their interaction as fixed effects (hint: you might want to use * in your model).\n\n\n7.5.2 Assessing assumptions\nVisually assess the model assumptions of normality and homoscedasticity and write 1-2 sentences about each assumption, referring to the figures you produced.\n\n\n7.5.3 Extracting predictions\n\nCreate objects intercept, b1 (verb_t), and b2 (gramm) that contain the corresponding model coefficient estimate for each term (hint: each object should contain a single value, which corresponds to the estimate for this term in your model summary output).\nGenerate the fitted values for each of our four conditions. We covered a number of ways to do this in class:\n\nUsing our model formula (\\(\\ref{eq-fp}\\)) and the sum contrast values for each level of verb_t and gramm (i.e., +/-0.5, given in Table¬†7.2), compute the back-transformed predicted total reading time for each condition. Recall: \\(b_0\\) is our intercept, \\(b_1\\) is our slope for verb_t (i.e., the value of the object you just named verb_t), and \\(b_2\\) is our slope (estimate) for gramm (i.e., the value of the object you just named gramm).\nThe ggeffects package: we used the ggeffect() function, but the ggpredict() function back-transforms the estimates for us.\n\n\n\\[\\begin{equation}\nfp = exp(b_0 + b_1*verb\\_t + b_2*gramm) \\label{eq-fp}\n\\end{equation}\\]\n\n\n\n\nTable¬†7.2: Corresponding value of factor levels to be plugged into equation ef{eq-fp}\n\n\n\n-0.5\n+0.5\n\n\n\n\n`verb_t`\npast\nfuture\n\n\n`gramm`\ngramm\nungramm\n\n\n\n\n\n\n\n\n\n\n7.5.4 Report model\nWrite a short report of the model findings. Produce a table and plot like in the example above to supplement your report."
  },
  {
    "objectID": "reports/report 1/report1.html#logistic-regression",
    "href": "reports/report 1/report1.html#logistic-regression",
    "title": "7¬† Report 1",
    "section": "7.6 Logistic regression",
    "text": "7.6 Logistic regression\nWe run logistic regression when we have a binimial dependent variable. You will be fitting a logistic regression model to regression in.\n\n7.6.1 Fit model\nFit a generalied linear model (logistic regression) to the regression in data, with the same fixed effects as your linear model above (verb_t, gramm, and their interaction). Remember, you will need a different function (not lm), and to add another argument (family = ...).\n\n\n7.6.2 Interpretation\nWrite a short report of the model findings. Produce a table and plot like in the example above to supplement yor report. Recall that our coefficient estimates are in log odds).\n\n\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053"
  },
  {
    "objectID": "07-independence_assumption.html",
    "href": "07-independence_assumption.html",
    "title": "8¬† Independence",
    "section": "",
    "text": "Learning Objectives\nToday we will learn about‚Ä¶\n‚Äî Winter (2011), p.¬†2137\nToday we learned about‚Ä¶\nDiscuss the following questions."
  },
  {
    "objectID": "07-independence_assumption.html#non-independence",
    "href": "07-independence_assumption.html#non-independence",
    "title": "8¬† Independence",
    "section": "9.1 (Non-)Independence",
    "text": "9.1 (Non-)Independence\n\nnon-independence: any possible link or connection between groups of data points\n\ne.g., two observations from the same participant will tend to be more similar than to completely independent observations\nany case where you might expect some clustering of observations by some grouping factor\n\nthe independence assumption assumes that our data points are not linked\n\ni.e., the value of one observation is completely independent from another\n\nviolations of this assumption have major implications for Type I (alpha) error\n\ni.e., the chances of observing an effect where there is none (false positive)\n\nit also artificially inflates sample size, which affects statistical power"
  },
  {
    "objectID": "07-independence_assumption.html#repeated-measures-design",
    "href": "07-independence_assumption.html#repeated-measures-design",
    "title": "8¬† Independence",
    "section": "9.2 Repeated measures design",
    "text": "9.2 Repeated measures design\n\nthe reason most (experimental) linguistic data is non-independent is the use of the repeated-measures design\n\ncollecting multiple data points from e.g., the same participant and for the same item\nincreases statistical power, needing fewer participants (more data points, lower variance due to control in variability between subjects)\nsaves resources (fewer subjects)"
  },
  {
    "objectID": "07-independence_assumption.html#other-sources-of-non-independence",
    "href": "07-independence_assumption.html#other-sources-of-non-independence",
    "title": "8¬† Independence",
    "section": "9.3 Other sources of non-independence",
    "text": "9.3 Other sources of non-independence\n\nnon-independence is prevalent in other fields of linguistics, e.g.,\n\ncorpus studies: text, author, language, dialect, register\nphonetic experiments: speaker, listener, exact repetitions\nsocio-phonetics: dialect/geographical proximity, register, speaker"
  },
  {
    "objectID": "07-independence_assumption.html#problem-generalizability",
    "href": "07-independence_assumption.html#problem-generalizability",
    "title": "8¬† Independence",
    "section": "10.1 Problem: Generalizability",
    "text": "10.1 Problem: Generalizability\n\nbeyond spurious results, how researchers interpret the implications of their findings is problematic\n\n\nUnfortunately, outside of a few domains such as psycholinguistics, it remains rare to see psychologists model stimuli as random effects ‚Äì despite the fact that most inferences researchers draw are clearly meant to generalize over populations of stimuli.\n\n‚Äî Yarkoni (2022), p.¬†4\n\nif we don‚Äôt include grouping factors in our models, our findings are not generalisable beyond our sample\n\nit could be that our findings are due to a few participants or experimental items who deviate from the rest\n\nwe need to take this by-grouping factor variation into account, but how?"
  },
  {
    "objectID": "07-independence_assumption.html#solution-1-averaging",
    "href": "07-independence_assumption.html#solution-1-averaging",
    "title": "8¬† Independence",
    "section": "10.2 Solution 1: Averaging",
    "text": "10.2 Solution 1: Averaging\n\ne.g., repeated measures ANOVA\n\nseperate models for by-participant and by-item variance (with averaging) interpreted together\n\nPRO: takes both by-participant and -item variance into account\nCONs: not flexible or approrpriate for complex designs, and:\n\nloses information regarding the variation across the grouped observations\nlowers N\n\ne.g., if we average over participants, we‚Äôd have 1 only data point per participant!\n\ntherefore loses statistical power (Type II error)\ninflates Type I error (chance of a false positive)\n\nin sum: not optimal"
  },
  {
    "objectID": "07-independence_assumption.html#solution-2-single-observations",
    "href": "07-independence_assumption.html#solution-2-single-observations",
    "title": "8¬† Independence",
    "section": "10.3 Solution 2: Single observations",
    "text": "10.3 Solution 2: Single observations\n\nrun an experiment without repeated measures\n\nbut this lowers statistical power\nand drastically reduces generalizability\n\ne.g., we could present 60 participants with a single item\n\nor we could present 1 participant with 60 trials\nbut these findings also can‚Äôt be generalised beyond that one item or one participant‚Ä¶\n\nin sum: not optimal"
  },
  {
    "objectID": "07-independence_assumption.html#solution-3-linear-mixed-models",
    "href": "07-independence_assumption.html#solution-3-linear-mixed-models",
    "title": "8¬† Independence",
    "section": "10.4 Solution 3: Linear mixed models",
    "text": "10.4 Solution 3: Linear mixed models\n\nbest available solution: use repeated-measures design and mixed models\na.k.a. mixed (effects) models/LM(E)Ms, multi-level models, hierarchical models\n‚Äúmixed‚Äù because they contain:\n\nfixed effects: usually predictors; describe systematic variation in our data that we wish to explain\nrandom effects: unsystematic variation that are due to random sampling\n\nrandom effects take dependence between observations into account\n\ncontain varying intercepts and slopes per level of a grouping factor\n\nfixed effects estimates are usually qualititatively unchanged\n\nwhat is affected in the measures of variance"
  },
  {
    "objectID": "07-independence_assumption.html#language-as-fixed-effect-fallacy",
    "href": "07-independence_assumption.html#language-as-fixed-effect-fallacy",
    "title": "8¬† Independence",
    "section": "11.1 1973: Language-as-fixed-effect-fallacy",
    "text": "11.1 1973: Language-as-fixed-effect-fallacy\n\nnone of these ideas are new to linguistics\nClark (1973):\n\nwithout including dependencies between repeated observations from the same linguistic items in our models, we cannot generalise our findings beyond our stimuli\nour results are relevant only for the subset of the population from which we sampled\n\n\n\nThe remedies for the language-as-fixed-effect fallacy are for the most part obvious. They include doing the right statistics, choosing the appropriate experimental design, and selecting a random or representative sample of language.\n\n‚Äî Clark (1973), p.¬†347"
  },
  {
    "objectID": "07-independence_assumption.html#anovas-aggregation",
    "href": "07-independence_assumption.html#anovas-aggregation",
    "title": "8¬† Independence",
    "section": "11.2 ANOVAs: aggregation",
    "text": "11.2 ANOVAs: aggregation\n\nrepeated measures ANOVAs were commonly used to take dependence between observations into account (and are still common in come fields today)\n\nrequire aggregation (i.e., averaging) over items or subjects, not both simultaneously\ndrastically reduces our number of observations\nloss of information in the variance of observed data\ni.e., a loss of power (Type II error) and inflated Type I error (false positive)"
  },
  {
    "objectID": "07-independence_assumption.html#baayen_mixed-effects_2008-and-lme4",
    "href": "07-independence_assumption.html#baayen_mixed-effects_2008-and-lme4",
    "title": "8¬† Independence",
    "section": "11.3 2008: Baayen et al. (2008) and lme4",
    "text": "11.3 2008: Baayen et al. (2008) and lme4\n\nenter mixed models with crossed random effects\nJournal of Memory and Language, Special Issue: Emerging Data Analysis\n\nBaayen et al. (2008): introduction of lme4 package for linear mixed models\nJaeger (2008): overview of generalised linear mixed models\n\nin addition, Baayen (2008) was published, a textbook for analysing linguistic data with R with an emphasis on LMMs with lme4"
  },
  {
    "objectID": "07-independence_assumption.html#session-info",
    "href": "07-independence_assumption.html#session-info",
    "title": "8¬† Independence",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.4.0    fastmap_1.1.1     cli_3.6.2        \n [5] htmltools_0.5.8.1 tools_4.4.0       rstudioapi_0.16.0 yaml_2.3.7       \n [9] rmarkdown_2.24    knitr_1.43        jsonlite_1.8.7    xfun_0.40        \n[13] digest_0.6.33     rlang_1.1.3       renv_1.0.7        evaluate_0.21"
  },
  {
    "objectID": "07-independence_assumption.html#references",
    "href": "07-independence_assumption.html#references",
    "title": "8¬† Independence",
    "section": "References",
    "text": "References\n\n\nBaayen, R. H. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics using R.\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390‚Äì412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nClark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12(4), 335‚Äì359. https://doi.org/10.1016/S0022-5371(73)80014-3\n\n\nJaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434‚Äì446. https://doi.org/10.1016/j.jml.2007.11.007\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2011). PSEUDOREPLICATION IN PHONETIC RESEARCH.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547\n\n\nWinter, B., & Grice, M. (2021). Independence and generalizability in linguistics. Linguistics, 59(5), 1251‚Äì1277. https://doi.org/10.1515/ling-2019-0049\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685"
  },
  {
    "objectID": "08-mixed_models1.html#learning-objectives",
    "href": "08-mixed_models1.html#learning-objectives",
    "title": "9¬† Random intercepts",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n\nhow to run our fixed mixed effects model with random intercepts\nhow to interpret random intercepts"
  },
  {
    "objectID": "08-mixed_models1.html#set-up-environment",
    "href": "08-mixed_models1.html#set-up-environment",
    "title": "9¬† Random intercepts",
    "section": "Set-up environment",
    "text": "Set-up environment\nOur first steps are to set-up our environment. The next two chunks are not necessary, but I use them to set some preferences, such as supressing scientific notation, and establishing a function to nicely format p-values.\n\n# suppress scientific notation\noptions(scipen=999)\noptions(pillar.sigfig = 5)\n\n\n\nCode for a function to format p-values\nlibrary(broman)\n# function to format p-values\nformat_pval &lt;- function(pval){\n    dplyr::case_when(\n        pval &lt; .001 ~ \"&lt; .001\",\n        pval &lt; .01 ~ \"&lt; .01\",\n        pval &lt; .05 ~ \"&lt; .05\",\n        TRUE ~ broman::myround(pval, 3)\n    )\n}\n\n\n\nLoad packages\nWe‚Äôll also need to load in our required packages. Hopefully you‚Äôve already install the required packages (if not, go to Chapter¬†3).\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               broom,\n               janitor,\n               ggeffects,\n               sjPlot,\n               # new packages:\n               lme4,\n               lmerTest,\n               broom.mixed,\n               lattice)\n\nHere I also globally set my preferred ggplot2 theme so that all of my plots are formatted how I like them, without have to repeat the code for each plot. This is completely optional.\n\n# set preferred ggplot2 theme\ntheme_set(theme_bw() + theme(plot.title = element_text(size = 10)))\n\n\nResolve conflicts\nSometimes different packages have functions with the same name. In these cases, when you call such a function the package that was last loaded will be used. Both lme4 and lmerTest have a function lmer(), but for now we want to use the lme4 version. We‚Äôll discuss the differences later, but for now let‚Äôs make sure that lme4 is used. We could also do this each time we call the function by using lme4::lmer(), but this can become cumbersome. Instead, let‚Äôs explicitly define lme4::lmer() as the function version that should be used.\n\nlmer &lt;- lme4::lmer\n\n\n\n\nLoad data\nNow let‚Äôs load in our dataset from Biondo et al. (2022).\n\ndf_biondo &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ## for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\"))\n\nAnd take a look at the data:\n\nhead(df_biondo)\n\n# A tibble: 6 √ó 13\n  sj     item adv_type adv_t verb_t gramm   roi label       fp    gp    tt    ri\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1        54 Deic     Past  Past   gramm     1 En la c‚Ä¶  1173  1173  1173     0\n2 1        54 Deic     Past  Past   gramm     2 ayer te‚Ä¶   474   474   474     0\n3 1        54 Deic     Past  Past   gramm     3 los car‚Ä¶   910   910   910     0\n4 1        54 Deic     Past  Past   gramm     4 encarga‚Ä¶  1027  1027  1027     0\n5 1        54 Deic     Past  Past   gramm     5 muchas ‚Ä¶   521   521   521     0\n6 1        54 Deic     Past  Past   gramm     6 al prov‚Ä¶  1029  1029  1029     0\n# ‚Ñπ 1 more variable: ro &lt;dbl&gt;\n\n\n?tbl-data_dictionary gives an overview of the variables in the dataset. Relevant for this chapter are the variables fp, verb_t, gramm, and roi. For the tasks at the end of the chapter, you‚Äôll also be working with tt and adv_t.\n\n\n\n\n\nvariable\ndescription\ntype\nclass\n\n\n\n\n`sj`\nparticipant ID\ngrouping\nfactor\n\n\n`item`\nitem ID\ngrouping\nfactor\n\n\n`adv_type`\nadverb type: `Deic`tic (e.g., on Monday), `Non-deic`tic (e.g., last Monday)\nindependent\nfactor\n\n\n`adv_t`\nadverb tense: `Past`, `Future`\nindependent\nfactor\n\n\n`verb_t`\nverb tense: `Past`, `Future`\nindependent\nfactor\n\n\n`gramm`\ngrammaticality: `gram`atical or `ungram`atical\nindependent\ncategorical\n\n\n`roi`\nsentence region (Region Of Interest); `2` = adverb, `4` = verb\nindependent\nordered factor\n\n\n`label`\nsentence region text\nindependent\nstring\n\n\n`fp`\nfirst-pass reading time (summation of fixations from when a reader first fixates on a region to when they first leave that region)\ndependent\ncontinuous\n\n\n`gp`\nregression path duration/go-past time\ndependent\ncontinuous\n\n\n`tt`\ntotal reading time (summation of all fixations within a region during a trial)\ndependent\ncontinuous\n\n\n`ri`\nregressions in (whether there was at least one regression into a region)\ndependent\nbinomial\n\n\n`ro`\nregressions out (whether there was at least one regression out of a region)\ndependent\nbinomial"
  },
  {
    "objectID": "08-mixed_models1.html#review",
    "href": "08-mixed_models1.html#review",
    "title": "9¬† Random intercepts",
    "section": "9.1 Review",
    "text": "9.1 Review\nUp until now, we‚Äôve learned about the equation of a line (Section¬†1.3), simple (?sec-simple-regression) and multiple linear regression (?sec-multiple-regression), and logisitic regression (?sec-logistic-regression). We‚Äôve also learned about centering and standardizing continuous predictors (?sec-continuous-predictors), and contrast coding categorical predictors (?sec-contrast-coding). We discussed non-linear transformations for dependent variables in linear regression, such as log-transforming data with a positive skew (?sec-log-transformation), and how to interpret the coefficients of logistic regressions in log-odds, odds, and probabilities (?sec-log-odds). If any of these topics don‚Äôt sound familiar to you, I suggest going back and reviewing the relevant chapter. If you feel you have a somewhat good handle on these topics, then proceed.\n\n9.1.1 Model equation\nRecall the equation of multiple linear regression model, given in Equation \\(\\ref{eq-mult_reg}\\).\n\\[\\begin{equation}\ny_i = b_0 + b_1x_i + b_2x_1 + ... +e_i \\label{eq-mult_reg}\n\\end{equation}\\]\nWhere the value of some value \\(y\\) (indexed by \\(i\\)) equals the intercept (\\(b_0\\)) plus the corresponding value \\(x\\) (indexed by \\(i\\)) of our first predictor (\\(b_1\\)) plus that of our second predictor (\\(b_2\\)), plus the corresponding error \\(e\\) (indexed by \\(i\\)), which is simply the difference between the predicted value and the observed value (i.e., residual). Here, \\(i\\) indicates values corresponding to the same observation \\(i\\). Such a model assumes that all possible groups within our data have the same intercept and the same slope.\nThe estimated parameters, i.e., our coefficients (\\(b_0\\), \\(b_1\\), \\(b_2\\)), are our fixed effects. The estimated values model the mean/population-level effects in our data. Mixed models try to model some of the variance, i.e., residual error (\\(e_i\\)), by including random effects. Though we will never completely get rid of the unexplained variance in our model (\\(e\\)), we can try to minimise it by including some expected variation present in our data. When, why, and how we can do that is the topic of this chapter."
  },
  {
    "objectID": "08-mixed_models1.html#mixed-models-why-when-and-how",
    "href": "08-mixed_models1.html#mixed-models-why-when-and-how",
    "title": "9¬† Random intercepts",
    "section": "9.2 Mixed models: why, when, and how?",
    "text": "9.2 Mixed models: why, when, and how?\nMixed models are ‚Äòmixed‚Äô in that they have both fixed and random effects. Fixed effects are our predictors (i.e., independent variables), the variance in the data we are trying to explain and generalise beyond our data. We would expect the model estimates of our fixed effects to be similar if we were to re-run our experiment with different participants, and even with different linguistic items that contain the same manipulation.\nRandom effects take into account the random variance, i.e., the variance in our data we are not trying to explain and that we would not expect to replicate across experiments. This is because they are dependent on e.g., the participants or specific linguistic items we collected our data from. Whether or not we have non-independence in our data depends on how/from where we collected our data. It‚Äôs common for experiments to involve multiple observations per participant and for the same stimuli (i.e., items) to be presented across participants. In production studies for example, participants may be asked to read the same sentences or words out loud. In corpus studies, data may be collected from several sources with multiple data points collected from the same text and/or author.\nTake participants for example: different people will tend to have different reading speeds, fundamental frequencies, and even different effects of our critical manipulations. That is to say, data points from a certain participant will tend to be grouped together since one participant might tend to be a faster reader than another participant, or have a higher fundamental frequency than another. The same can be said for experimental items: one item (e.g., Yesterday/Tomorrow, the workers went/will go to the bakery) might tend to have longer reading times or a larger effect of grammaticality than another item for one reason or another.\nOf course, participant and item are not the only sources of non-independence in linguistic data. Winter & Grice (2021) provides a description of other possible sources of non-independence in language research, such as phonetic production studies (speaker, exact repetitions), and corpora (author, text, register). In essence, random effects are grouping factors in our data across beyond which we want to generalise our observed effects.\n\n9.2.1 Why?\nSimply put, because of the independence assumption and Type I (alpha) error! The ‚Äúsexy‚Äù answer: Not accounting for non-independence in our data can lead to unreliable p-values.\n\n\n9.2.2 When?\nWhenever you have observations (i.e., data points) that are somehow linked. One such case is when you have a repeated measures design, as is often the case in linguistic experiments: each participant sees the same experimental items. Therefore, we have multiple (non-independent) observations per participant, and also multiple (non-independent) observations per item. See Section 3 in Winter & Grice (2021) for a discussion of sources of dependence beyond participant and item in different subfields of language research.\nLet‚Äôs look at an example. We perviously used the data from from Biondo et al. (2022), which contains data from an eye-tracking during reading experiment with a repeated measures design. We‚Äôre interested in whether reading times were affected by adverb-tense congruence (grammaticality) and tense (past vs.¬†future). Let‚Äôs review your model from Report 1.\n\n9.2.2.1 An example: report 1 model\nIn the first report for this class, you fit a model of first-pass reading times from Biondo et al. (2022) with the predictors (i.e., fixed effects) grammaticality, tense, and their interaction. The resulting coefficients should look something like Table¬†9.1.\n\n\nCode for data prep\n# prep data for model\ndf_deic_verb &lt;-\n  df_biondo |&gt; \n  # filter for verb region (roi = 4) and Deictic adverbs\n  filter(roi == 4,\n         adv_type == \"Deic\") |&gt; \n  # set predictors as factors for contrast coding\n  mutate(gramm = as_factor(gramm),\n         verb_t = as_factor(verb_t))\n\n# sum contrast coding: gramm and Past = -0.5\ncontrasts(df_deic_verb$gramm) &lt;- c(-0.5, 0.5)\ncontrasts(df_deic_verb$verb_t) &lt;- c(-0.5, 0.5)\n\n# check contrasts\n# contrasts(df_deic_verb$gramm)\n# contrasts(df_deic_verb$verb_t)\n\n\n\nfit_lm_fp &lt;-\n  lm(log(fp) ~ gramm*verb_t,\n     data = df_deic_verb)\n\n\n\nCode for table\n# model\n# print model coefficients only\ntidy(fit_lm_fp) |&gt; \n  kable(digits = 3) |&gt; \n  kable_styling()\n\n\n\n\nTable¬†9.1: Output from report 1 model of first-pass reading times\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n5.957\n0.008\n741.568\n0.000\n\n\ngramm1\n0.003\n0.016\n0.193\n0.847\n\n\nverb_t1\n0.061\n0.016\n3.809\n0.000\n\n\ngramm1:verb_t1\n-0.015\n0.032\n-0.474\n0.635\n\n\n\n\n\n\n\n\nWe see there is a significant effect of verb tense (Est = 0.06, t = 3.9, p &lt; .001), where the slope (and therefore the t-value) is positive. Since we coded Past as -0.5 and Future as +0.5, a positive slope means there were longer first-pass reading times for the Future condition, which we also saw when we plotted the raw data. So, this means that future-tensed verbs elicited longer first-pass reading times, and that the congruence of the verb with a preceding temporal adverb did not affect first-pass reading times. When we back-transform the log-transformed predicted values per condition into milliseconds we get Table¬†9.2.\n\n\nClick here to see code for the table\nggpredict(fit_lm_fp, terms = c(\"verb_t\", \"gramm\")) |&gt; \n  as_tibble() |&gt; \n  rename(\n    tense = x,\n    gramm = group\n  ) |&gt; \n  relocate(tense, gramm) |&gt; \n  knitr::kable(digits = 3) |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable¬†9.2: Predicted values from our lm() model back-transformed into milliseconds\n\n\ntense\ngramm\npredicted\nstd.error\nconf.low\nconf.high\n\n\n\n\nPast\ngramm\n372.892\n0.016\n361.354\n384.798\n\n\nPast\nungramm\n376.912\n0.016\n365.202\n388.998\n\n\nFuture\ngramm\n399.460\n0.016\n387.069\n412.249\n\n\nFuture\nungramm\n397.658\n0.016\n385.329\n410.382\n\n\n\n\n\n\n\n\nBut if we look at each participants observations, e.g., their first-pass reading times for past versus future tensed verbs, we see there is quite some variation in their means and in the effect of tense. Figure¬†10.2 shows by-participant variation in first-pass reading times for past and future tenses from seven example participants, in raw milliseconds (A) and log-transformed first-pass reading times (B). In each plot, the vertical grey dotted line indicates \\(x = 0\\) (because we used sum contrast coding \\(x = 0\\) is smack dab in the middle between past and future), the blue line represents the by-participant intercept and slope, while the black line represents the intercept and slope from our model. The black line therefore represents the population-level values, i.e., the mean of all first-pass reading times and the mean effect of tense.\n\n\nCode for plots\nfig_biondo_sj_ms &lt;-\n  df_biondo |&gt; \n  filter(sj %in% c(1,10,2,35,46,57,63)) |&gt; \n  mutate(verb_t = factor(verb_t, levels = c(\"Past\", \"Future\"))) |&gt; \nggplot() + \n  aes(x = verb_t, y = fp,\n      colour = verb_t,\n      shape = verb_t) + \n  facet_wrap(\"sj\", nrow = 1) +\n  # Put the points on top of lines\n  geom_point(position = position_jitter(0.2),\n             alpha = .2) +\n  stat_smooth(aes(group = 1), method = \"lm\") +\n  # geom_boxplot(colour = \"black\", alpha = 0) +\n  labs(y = \"First-pass RT (ms)\", \n       x = \"Tense\") +\n  geom_vline(xintercept = 1.5, colour = \"grey\", linetype = \"dashed\")+\n  theme(legend.position = \"none\")  +\n  geom_abline(\n    intercept = exp(coef(fit_lm_fp)[1]-(coef(fit_lm_fp)[3]*1.5)) , \n    slope = (exp(coef(fit_lm_fp)[1]+(coef(fit_lm_fp)[3]*0.5)) - exp(coef(fit_lm_fp)[1]+(coef(fit_lm_fp)[3]*-0.5)))\n                )\n\nfig_biondo_sj_log &lt;-\ndf_biondo |&gt; \n  mutate(verb_t = factor(verb_t, levels = c(\"Past\", \"Future\"))) |&gt; \n  filter(sj %in% c(1,10,2,35,46,57,63)) |&gt; \nggplot() + \n  aes(x = verb_t, y = log(fp),\n      colour = verb_t,\n      shape = verb_t) + \n  facet_wrap(\"sj\", nrow = 1) +\n  # Put the points on top of lines\n  geom_point(position = position_jitter(0.2),\n             alpha = .2) +\n  stat_smooth(aes(group = 1), method = \"lm\") +\n  labs(y = \"First-pass RT (log)\", x = \"Tense\") +\n  geom_vline(xintercept = 1.5, colour = \"grey\", linetype = \"dashed\") +\n  theme(legend.position = \"none\")  +\n  geom_abline(\n    intercept = coef(fit_lm_fp)[1]-(coef(fit_lm_fp)[3]*1.5),\n    slope = coef(fit_lm_fp)[3])\n\n# print\nfig_biondo_sj_ms / fig_biondo_sj_log + theme(legend.position = \"none\") +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure¬†9.1: Demonstration of by-participant variation in intercept and slope (blue line) versus fixed-effects-only model intercept and slope (black line) for seven example participants. Grey dashed line indicates \\(x = 0\\). Points represent single observations.\n\n\n\n\nNote in Figure¬†10.2 that there is variation in the central tendency of observations per participant. For example, Participant 10 had overall longer first-pass reading times than the other participants, as did Participant 1. Conversely, some participants had overall faster first-pass reading times, such as Participants 46 and 63. Meanwhile, some participants were pretty near the grand mean, like Participants 2 and 57. This is not to mention the differences in the slopes: some participants have a flatter slope than the model‚Äôs fitted slope (e.g., Participants 35 and 46), while some even have a slope in the opposite direction (e.g., Participants 1 and 63).\nFigure¬†9.2 shows the same trend across a sample of seven experimental items. Some by-item intercepts were similar to the model intercept (e.g., items 1 and 85), while some deviated (e.g., item 10 and 26).\n\n\nCode for plots\nfig_biondo_item_ms &lt;-\n  df_biondo |&gt; \n  filter(item %in% c(1,10,26,33,58,101,85)) |&gt;\n  mutate(verb_t = factor(verb_t, levels = c(\"Past\", \"Future\"))) |&gt; \nggplot() + \n  aes(x = verb_t, y = fp,\n      colour = verb_t,\n      shape = verb_t) + \n  facet_wrap(\"item\", nrow = 1) +\n  # Put the points on top of lines\n  geom_point(position = position_jitter(0.2),\n             alpha = .2) +\n  stat_smooth(aes(group = 1), method = \"lm\") +\n  # geom_boxplot(colour = \"black\", alpha = 0) +\n  labs(y = \"First-pass RT (ms)\", \n       x = \"Tense\") +\n  geom_vline(xintercept = 1.5, colour = \"grey\", linetype = \"dashed\")+\n  theme(legend.position = \"none\")  +\n  geom_abline(\n    intercept = exp(coef(fit_lm_fp)[1]-(coef(fit_lm_fp)[3]*1.5)) , \n    slope = (exp(coef(fit_lm_fp)[1]+(coef(fit_lm_fp)[3]*0.5)) - exp(coef(fit_lm_fp)[1]+(coef(fit_lm_fp)[3]*-0.5)))\n                )\n\nfig_biondo_item_log &lt;-\ndf_biondo |&gt; \n  mutate(verb_t = factor(verb_t, levels = c(\"Past\", \"Future\"))) |&gt; \n  filter(item %in% c(1,10,26,33,58,101,85)) |&gt;\nggplot() + \n  aes(x = verb_t, y = log(fp),\n      colour = verb_t,\n      shape = verb_t) + \n  facet_wrap(\"item\", nrow = 1) +\n  # Put the points on top of lines\n  geom_point(position = position_jitter(0.2),\n             alpha = .2) +\n  stat_smooth(aes(group = 1), method = \"lm\") +\n  labs(y = \"First-pass RT (log)\", x = \"Tense\") +\n  geom_vline(xintercept = 1.5, colour = \"grey\", linetype = \"dashed\") +\n  theme(legend.position = \"none\")  +\n  geom_abline(\n    intercept = coef(fit_lm_fp)[1]-(coef(fit_lm_fp)[3]*1.5),\n    slope = coef(fit_lm_fp)[3])\n\n# print\nfig_biondo_item_ms / fig_biondo_item_log + theme(legend.position = \"none\") +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure¬†9.2: Demonstration of by-item variation in intercept and slope (blue line) versus fixed-effects-only model intercept and slope (black line) for seven example items. Grey dashed line indicates \\(x = 0\\). Points represent single observations.\n\n\n\n\nIf we were to gather by-participant and by-item slopes from all participants and items and plotted them together, we would get Figure¬†9.3 (where colour indicates direction of slope: positive or negative).\n\nfig_item_sj_fp_means\n\n\n\n\nFigure¬†9.3: Means per condition per item (left) and subject (right) with overall mean in black\n\n\n\n\nWe see saw in seven examples in Figure¬†10.2 and (fit-pooling_item?), and across all participants and items in Figure¬†9.3, that there is a lot of variability in terms of the overall mean (intercept, which would correspond to the grey dotted line) across items and participants, as well and in the differences between past and future verbs. Firstly, focusing on the intercept (values crossing the vertical grey dotted line), there is a range of approximately 300 to 675 ms between items, and a range of 250 and 875 between participants. Compared to the overall mean of 440.5ms, this is quite some variation. Looking now at the effect of tense, i.e.¬†the slope, we see not only differences in the magnitude of effects between items and participants (i.e., how steep the slope is), but also in the direction of the effect: There are quite a few items and participants that have a slope in the opposite direction of the overall mean in black, which is positive.\nSo why does it matter that there‚Äôs variability by item and by participant? All of this data was already included in our model, and so it was taken into consideration when calculating standard error and confidence intervals, so why should this matter? The answer is simply: dependence of data points affects the number of observations, which in turn affects our degrees of freedom and measures like standard error and confidence intervals. In essence, it alters our measures of uncertainty in the presence or absence of a reliable effect.\nTo drive this point home, let‚Äôs look at an ordered plot of by-item and -participant intercepts with 95% confidence intervals (Figure¬†9.4) and boxplots of the raw observations of first-pass reading times at the verb region (Figure¬†9.5). In Figure¬†9.4, we see the intercept value on the x-axis, again highlighting the range of varying intercept values for across items and participants. In Figure¬†9.5, we see the range in the spread of values, with wider inter-quartile ranges especially for particpants with higher a median first-pass reading time. This was also represented in Figure¬†9.4 by wider 95% confidence intervals for both items and participants with higher intercepts.\n\n\n\n\n\nFigure¬†9.4: By-item (left) and -participant (right) varying intercepts (x-axis) per grouping factor level (i.e., per item and per participant). Errorbars indicate 95% confidence intervals.\n\n\n\n\n\n\nCode for plots\nfig_sj_boxplot &lt;-\n  df_deic_verb |&gt;\n  mutate(sj_median = median(fp, na.rm = T), .by=sj) |&gt; \n  ggplot() +\n  aes(x = reorder(sj, sj_median), y = fp) +\n  labs(title = \"By-participant boxplot of first-pass reading times at the verb\",\n       y = \"First-pass RT (ms)\") +\n  geom_boxplot() +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank()\n      )\n\nfig_item_boxplot &lt;-\ndf_deic_verb |&gt; \n  mutate(item_median = median(fp, na.rm = T), .by=item) |&gt; \n  ggplot() +\n  aes(x = as_factor(reorder(item, item_median)), y = fp) +\n  labs(title = \"By-item boxplot of first-pass reading times at the verb\",\n       y = \"First-pass RT (ms)\") +\n  geom_boxplot() +\n  theme(\n    axis.ticks.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.x = element_blank()\n      )\n\nfig_sj_boxplot / fig_item_boxplot +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure¬†9.5: Boxplots of first-pass reading times per participant (A) and item (B)\n\n\n\n\nIn order for our model to take this by-participant and by-item variance into account, we can add by-participant and by-item random terms in a mixed model.\n\n\n\n9.2.3 How?\nWe can add random intercepts and random slopes per grouping factor. Random intercepts would correspond to the average \\(y\\)-value (if we‚Äôre using sum contrast coding) per level of a grouping factor, e.g., per participant. Random slopes would give us the fitted effect per level of a grouping factor, e.g., per participant. So, if we fit a model with by-participant random intercepts and slopes, our model will also fit an intercept and slope per participant, thereby taking the by-participant variance into account. Importantly, each grouping factor must be a factor (i.e., categorical), and each level of this grouping factor must have sufficient observations. In this chapter we‚Äôll focus on random intercepts, and we will be running what‚Äôs called random-intercept-only or intercept-only random effects models. A word of warning: such models can lead to inflated Type I (alpha) error, i.e., a false positive result (Barr et al., 2013). Mixed models with intercept-only random effects are often the final model reported because of something called convergence issues, meaning the model cannot be fit because of a lack of computational power or too few observations ‚Äúper cell‚Äù.\nThe lme4 (lme4-package?) or lmerTest (lmerTest-package?) packages are commonly used to produced mixed models in R. The main difference between the two is that lmerTest produces p-values while lme4 does not. The coefficients from the two packages should be otherwise identical. For a more in-depth discussion on p-values in mixed models, see for example Section 8.5.1.3 (t/F-tests with approximate df) in Sonderegger (2023). We‚Äôll be using the lme4 package to start off with to fit mixed models with the lmer() function, which uses similar sytax to the lm() function:\n\\[\\begin{align}\ndv &\\sim 1 + iv, data = data\\_name \\label{eq-lm}\\\\\ndv &\\sim 1 + iv + (1 + iv|gf), data = data\\_name \\label{eq-lmer}\n\\end{align}\\]\nWhere dv is our dependent variable (measure, outcome variable), iv is our independent variable(s) (predictor variable), and gf refers to a grouping factor. The 1s stand-in for intercept, so 1 + iv means fit an intercept (1) and slope (iv). Recall that the 1 is optional, and we often don‚Äôt write it in our models. We see a 1 in the random effects structure, however: (1 + iv|gf). This represents random effects for a grouping factor (gf): We are fitting an intercept and slope per level of this grouping factor. Basically, this model is a mixed model fit to some dependent variable with an independent variable(s) as fixed effect, and by-grouping factor random intercepts and slopes for our fixed effect iv. You could replace the highlighted terms in the last sentence with the names of your own variables in a model to describe your formula.\nThis might all sound abstract at the moment, but it helps to see it in action. For the rest of the chapter we‚Äôll focus on random intercepts, and we will get to random slopes in the next chapter. Let‚Äôs now fit and explore some mixed models."
  },
  {
    "objectID": "08-mixed_models1.html#by-participant-random-intercepts",
    "href": "08-mixed_models1.html#by-participant-random-intercepts",
    "title": "9¬† Random intercepts",
    "section": "9.3 By-participant random intercepts",
    "text": "9.3 By-participant random intercepts\nRecall the equation in Equation \\(\\ref{eq-mult_reg}\\). To model first-pass reading times as a function of verb tense and grammaticality, we would get Equation \\(\\ref{eq-mixed_model}\\). Adding random intercepts for a single grouping factor would give us Equation \\(\\ref{eq-mixed_model}\\).\n\\[\\begin{align}\nfp &= \\beta_0 + \\beta_{verb\\_t}x + \\beta_{gramm}x \\label{eq-biondo_average} \\\\\nfp_i &= \\beta_0 + \\alpha_{j[i]} + \\beta_{verb\\_t}x + \\beta_{gramm}x + e_i\n\\label{eq-mixed_model}\n\\end{align}\\]\nWhere \\(\\alpha\\) represents the deviation of some level \\([i]\\) in some group \\(j\\) from the population-level intercept (\\(b_0\\)). In other words, we assume here that there is some grouping factor within our data structure and that each level of this grouping factor will have an intercept value that deviates somewhat from the population-level intercept. As we saw above, the data from Biondo et al. (2022) contains non-independent observations from 60 participants. If we wanted to take that into considerat\nA simplified version in is the model for the average participant in our data, where the intercept is the average first-pass reading time across all participants, and the slopes for tense and grammaticality are also the average effect of each across all participants. models the first-pass reading time with varying intercepts for participant (sj), where \\(\\beta_0 + \\alpha_{sj[i]}\\) represents the intercept for participants. models the first-pass reading time for participant (sj) 60, where \\(\\beta_0 + \\alpha_{sj[60]}\\) represents the intercept for participant 60.\n\\[\\begin{align}\nfp &= \\beta_0 + \\alpha_{sj[i]} + \\beta_1x + \\beta_2x  \\label{eq-biondo-sj} \\\\\nfp &= \\beta_0 + \\alpha_{sj[60]} + \\beta_1x + \\beta_2x \\label{eq-biondo-sj60}\n\\end{align}\\]\nLet‚Äôs continue with our model from Report 1, with log-transformed first-pass reading times (fp) as dependent variable, grammaticality (gramm), verb tense (verb_t), and their interaction as fixed effects, and by-participant (sj) random intercepts:\n\nfit_lmm_fp_sj &lt;-\n  lmer(log(fp) ~ gramm*verb_t +\n         (1|sj),\n       data = df_deic_verb)\n\nWe see the only difference between this code and that above is that we are using lmer() instead of lm(), and that we have added + (1|sj) to the model equation.\nWhat happens if we try to run this model without + (1|sj)?\n\nfit_lmm_fp_sj &lt;-\n  lmer(log(fp) ~ gramm*verb_t,\n       data = df_deic_verb)\n\nError: No random effects terms specified in formula\n\n\nWe get an informative error message: Error: No random effects terms specified in formula. The lmer() function requires a random effects structure, so if it is missing a model will not be fit.\n\n9.3.1 Inspecting your model output\nThe summary() function also worth lmer() models, but there are some differences in the output.\n\nsummary(fit_lmm_fp_sj)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(fp) ~ gramm * verb_t + (1 | sj)\n   Data: df_deic_verb\n\nREML criterion at convergence: 4479.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.0560 -0.6427 -0.0419  0.6168  4.0901 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n sj       (Intercept) 0.06573  0.2564  \n Residual             0.18030  0.4246  \nNumber of obs: 3795, groups:  sj, 60\n\nFixed effects:\n                Estimate Std. Error t value\n(Intercept)     5.957102   0.033809 176.199\ngramm1          0.003466   0.013787   0.251\nverb_t1         0.062209   0.013787   4.512\ngramm1:verb_t1 -0.015741   0.027573  -0.571\n\nCorrelation of Fixed Effects:\n            (Intr) gramm1 vrb_t1\ngramm1       0.000              \nverb_t1      0.000 -0.002       \ngrmm1:vrb_1  0.000  0.000  0.002\n\n\nSimilar to the model output from a lm() model, we have the model formula at the top. We also have have the distribution of the residuals, which look quite normally distributed. Our fixed effects are under Fixed Effects (instead of Coefficients in lm() output). Here you might notice we don‚Äôt have any p-values, we‚Äôll talk about why below. Lastly, instead of information about model fit (e.g., \\(R^2\\)), we have Correlation of Fixed Effects, which is exactly what the title suggests: the correlation between fixed effects. This corresponds to the assumption of multicollinearity, and should have small values.\nThere are two other main differences near the top: REML criterion at convergence: ..., and our Random effects. We won‚Äôt be getting into REML in this course, but know that it is important when doing model comparisons.\n\n\n\n\n\n\nREML: restricted maximum likelihood\n\n\n\nSonderegger (2023), Section 8.5 and Box 8.4\n\n\nWe can use the broom.mixed package to extract tidy coefficient summaries from lmer() models, similar to the broom package for lm() models. The broom.mixed package also has a function tidy() for this purpose. This function also has an optional argument effects which can be used to control what information you extract from your model:\n\neffects = fixed: fixed-effect parameters\neffects = ran_pars: random effects of our model (standard deviations of our random effect terms)\n\n\n\nClick to see code for table\ntidy(fit_lmm_fp_sj, effects = \"fixed\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†9.3: broom.mixed::tidy(fit_lmm_fp_sj)\n\n\neffect\nterm\nestimate\nstd.error\nstatistic\n\n\n\n\nfixed\n(Intercept)\n5.9571020\n0.0338089\n176.1991812\n\n\nfixed\ngramm1\n0.0034662\n0.0137866\n0.2514208\n\n\nfixed\nverb_t1\n0.0622091\n0.0137873\n4.5120666\n\n\nfixed\ngramm1:verb_t1\n-0.0157409\n0.0275733\n-0.5708757\n\n\n\n\n\n\n\n\n\n\nClick to see code for table\ntidy(fit_lmm_fp_sj, effects = \"ran_pars\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†9.4: broom.mixed::tidy(fit_lmm_fp_sj)\n\n\neffect\ngroup\nterm\nestimate\n\n\n\n\nran_pars\nsj\nsd__(Intercept)\n0.2563809\n\n\nran_pars\nResidual\nsd__Observation\n0.4246230\n\n\n\n\n\n\n\n\nWe can also use the VarCorr() function from lme4 to extract the variance components (i.e., random effects) from our model summary:\n\nprint(VarCorr(fit_lmm_fp_sj),comp=c(\"Variance\",\"Std.Dev.\"))\n\n Groups   Name        Variance Std.Dev.\n sj       (Intercept) 0.065731 0.25638 \n Residual             0.180305 0.42462 \n\n\nBut what exactly do these random effect parameters mean? We see the estimated degree of by-participant intercept variability is approximately 0.07 has a standard deviation of approximately 0.26.\n\n\n9.3.2 lmerTest\n\nfit_lmm_fp_sj &lt;-\n  lmerTest::lmer(log(fp) ~ gramm*verb_t +\n         (1|sj),\n       data = df_deic_verb)\n\n\n\n9.3.3 Comparing to simple regression\nVisualise both models‚Äô coefficents\n\nfig_lmer &lt;- plot_model(fit_lmm_fp_sj, type = \"int\") + \n  geom_line(position = position_dodge(0.1)) +\n  labs(title = \"Mixed model\",\n       x = \"Grammaticality\",\n       y = \"First-pass (ms)\") +\n  theme_bw() +\n  ylim(340, 440)\n\nfig_lm &lt;- plot_model(fit_lm_fp, type = \"int\") + \n  geom_line(position = position_dodge(0.1)) +\n  labs(title = \"Fixed effects only\",\n       x = \"Grammaticality\",\n       y = \"First-pass (ms)\") +\n  theme_bw() +\n  ylim(340, 440)\n\nfig_lmer + fig_lm + plot_annotation(tag_levels = \"A\") +\n  plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "08-mixed_models1.html#adding-another-grouping-factor",
    "href": "08-mixed_models1.html#adding-another-grouping-factor",
    "title": "9¬† Random intercepts",
    "section": "9.4 Adding another grouping factor",
    "text": "9.4 Adding another grouping factor\nSo far we‚Äôve fit the data using one grouping factor: participant. The experimental design in (biondo_tomorrow_2022?) used repeated measures, however. This means that each participant was presented items from multiple items. In other words, each item (item) was presented to multiple participants (sj). This is a crossed-design, also called a factorial design, where both participant and item are grouping factors. We should be using crossed random effects, i.e., two grouping factors. We can do this by simply adding another + (1|gf) to our model syntax.\n\nfit_lmm_fp_sj_item &lt;-\n  lmer(log(fp) ~ gramm*verb_t +\n         (1|sj) +\n         (1|item),\n       data = df_deic_verb)\n\nThis is now a model with by-participant and by-item random intercepts. Let‚Äôs inspect this model as we did with our model with by-participant random intercepts. This amounts to Equation \\(\\ref{eq-mixed_model_k}\\), where we have varying intercepts (\\(\\alpha\\)) for two grouping variables, \\(j\\) and \\(k\\).\n\\[\\begin{align}\nfp_i &= \\beta_0 + \\alpha_{j[i]} + \\alpha_{k[i]}+ \\beta_{verb\\_t}x + \\beta_{gramm}x + e_i \\label{eq-mixed_model_k}\n\\end{align}\\]\nIf we take \\(j\\) to represent participants and \\(k\\) to represent items, then the \\(j\\) in \\(\\alpha_{j[i]}\\) has 60 levels (1-60, because we have 60 participants), and \\(k\\) in \\(\\alpha_{j[i]}\\) has 96 levels (1-96, because we have 96 items). And \\(i\\) has 3795 levels, because there are 3795 observations in our model (which we will see in a moment)."
  },
  {
    "objectID": "08-mixed_models1.html#exploring-our-random-effects-estimates",
    "href": "08-mixed_models1.html#exploring-our-random-effects-estimates",
    "title": "9¬† Random intercepts",
    "section": "9.5 Exploring our random effects estimates",
    "text": "9.5 Exploring our random effects estimates\n\nwhat we saw in our model summary were the variance components\n\na description of the variance of our by-item and by-participant random intercepts\n\nour model also contains intercept estimates for each level of item and participant\n\nwe can extract the intercept estimates\nor we extract their deviance from the model intercept\n\n\n\n9.5.1 Extracting fixed effects\n\nwe‚Äôve already used coef() to extract fixed effect estimates from lm objects\n\n\ncoef(fit_lm_fp)\n\n   (Intercept)         gramm1        verb_t1 gramm1:verb_t1 \n   5.957251870    0.003101061    0.061204153   -0.015245374 \n\n\n\nto extract our fixed effect estimates from lmer objects we need fixef()\n\n\nfixef(fit_lmm_fp_sj_item)\n\n   (Intercept)         gramm1        verb_t1 gramm1:verb_t1 \n    5.95640363     0.00321152     0.06189237    -0.01431578 \n\n\n\nor we can append $coefficients to the model summary\n\n\nsummary(fit_lmm_fp_sj_item)$coefficients |&gt; \n  as_tibble()\n\n# A tibble: 4 √ó 3\n    Estimate `Std. Error` `t value`\n       &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1  5.9564        0.036790 161.90   \n2  0.0032115     0.013025   0.24657\n3  0.061892      0.013025   4.7517 \n4 -0.014316      0.026049  -0.54956\n\n\n\n9.5.1.1 Extract random intercept estimates\n\ncoef() behaves very differently with lmer objects, extracting the random effects estimates per level\n\n\ncoef(fit_lmm_fp_sj_item) |&gt; pluck(\"item\") |&gt; head()\n\n  (Intercept)     gramm1    verb_t1 gramm1:verb_t1\n1    6.022184 0.00321152 0.06189237    -0.01431578\n2    5.761268 0.00321152 0.06189237    -0.01431578\n3    5.854873 0.00321152 0.06189237    -0.01431578\n4    6.056862 0.00321152 0.06189237    -0.01431578\n5    6.138213 0.00321152 0.06189237    -0.01431578\n6    6.331058 0.00321152 0.06189237    -0.01431578\n\n\n\nwhich outputs a list object, with one data frame for item and one for sj\nwe can extract just one or the other (head() is for presentation purposes):\n\n\ncoef(fit_lmm_fp_sj_item) |&gt; pluck(\"item\") |&gt; \n  rownames_to_column(var = \"item\") |&gt; head()\n\n  item (Intercept)     gramm1    verb_t1 gramm1:verb_t1\n1    1    6.022184 0.00321152 0.06189237    -0.01431578\n2    2    5.761268 0.00321152 0.06189237    -0.01431578\n3    3    5.854873 0.00321152 0.06189237    -0.01431578\n4    4    6.056862 0.00321152 0.06189237    -0.01431578\n5    5    6.138213 0.00321152 0.06189237    -0.01431578\n6    6    6.331058 0.00321152 0.06189237    -0.01431578\n\n\n\ncoef(fit_lmm_fp_sj_item) |&gt; pluck(\"sj\") |&gt; \n  rownames_to_column(var = \"sj\") |&gt; head()\n\n  sj (Intercept)     gramm1    verb_t1 gramm1:verb_t1\n1 07    5.869627 0.00321152 0.06189237    -0.01431578\n2 09    5.782527 0.00321152 0.06189237    -0.01431578\n3  1    6.401777 0.00321152 0.06189237    -0.01431578\n4 10    6.621081 0.00321152 0.06189237    -0.01431578\n5 11    5.913712 0.00321152 0.06189237    -0.01431578\n6 12    6.153031 0.00321152 0.06189237    -0.01431578\n\n\n\nwhy do our intercepts vary by participant, but not verb_t1, gramm1, or verb_t1:gramm1?\n\n\n\n9.5.1.2 Extract deviations from the intercept\n\nthe ranef() function provides the deviance from the model intercept and each random intercept estimate\n\nthe output is a list with a one element per grouping factor\n\n\n\nranef(fit_lmm_fp_sj_item) |&gt;  pluck(\"item\") |&gt; \n  rownames_to_column(var = \"item\") |&gt; head(10)\n\n   item (Intercept)\n1     1  0.06578061\n2     2 -0.19513572\n3     3 -0.10153080\n4     4  0.10045812\n5     5  0.18180978\n6     6  0.37465425\n7     7  0.09281920\n8     8  0.13695475\n9     9  0.05810287\n10   10 -0.05426568\n\n\n\n\n\nranef()$grouping_factor or pluck(\"grouping_factor\") selects the relevant grouping factor\n\n\n\nranef(fit_lmm_fp_sj_item)$sj |&gt; \n  head()\n\n   (Intercept)\n07 -0.08677692\n09 -0.17387701\n1   0.44537367\n10  0.66467739\n11 -0.04269124\n12  0.19662767\n\n\n\n\nranef(fit_lmm_fp_sj_item) |&gt; \n  pluck(\"sj\") |&gt; head()\n\n   (Intercept)\n07 -0.08677692\n09 -0.17387701\n1   0.44537367\n10  0.66467739\n11 -0.04269124\n12  0.19662767\n\n\n\n\n\n\n9.5.1.3 Compare estimates and deviances\n\nthe values extracted by ranef() (sj_dev in Table¬†9.5) equal the difference (difference) between the model intercept (model_intercept) and the by-participant random intercept estimates (sj_est)\nso we can either look at each participant‚Äôs (or item‚Äôs) estimate, or look at how much it deviates from the model intercept\n\n\n\n\n\nTable¬†9.5: Random intercept estimates versus deviance\n\n\nsj\nsj_est\nsj_dev\nest_minus_dev\nmodel_intercept\nest_minus_intercept\n\n\n\n\n07\n5.870\n-0.087\n5.956\n5.956\n-0.087\n\n\n09\n5.783\n-0.174\n5.956\n5.956\n-0.174\n\n\n1\n6.402\n0.445\n5.956\n5.956\n0.445\n\n\n10\n6.621\n0.665\n5.956\n5.956\n0.665\n\n\n11\n5.914\n-0.043\n5.956\n5.956\n-0.043\n\n\n12\n6.153\n0.197\n5.956\n5.956\n0.197"
  },
  {
    "objectID": "08-mixed_models1.html#visualising-your-random-effects",
    "href": "08-mixed_models1.html#visualising-your-random-effects",
    "title": "9¬† Random intercepts",
    "section": "9.6 Visualising your random effects",
    "text": "9.6 Visualising your random effects\nThe simplest method to visualise your random effects is to use the dotplot() function from the lattice package. This prints out a caterpillar plot with a dot indicating the deviance of the intercept value per grouping factor level (here: per participant) from the model intercept, with 95% confidence intervals.\n\ndotplot(ranef(fit_lmm_fp_sj))\n\n$sj\n\n\n\n\n\nWe can also produce this plot ourselves by extracting our random effects per participant by using the broom.mixed::tidy() function with the argument effects = \"ran_vals and conf.int = TRUE. This will give us the intercept value, rather than the deviance from the model intercept. However, you can easily calculate the deviance by subtracting the model intercept value from each participant‚Äôs intercept value (Figure¬†9.6 A). If we want the actual by-partiipant intercept values, we can simply add the model intercept to get each by-participant estimate, i.e., the values we get with coef() (Figure¬†9.6 B). Notice that in comparison to Figure¬†9.6, nothing has changed except the values along the x-axis. This is because we‚Äôve performed a linear transformation: adding the model intercept value to the by-participant deviance. The x-axis ticks in Figure¬†9.6 B equal values as those in Figure¬†9.6 A (-0.5, 0, 0.5), but with the model intercept value (5.957102) added.\n\n\nCode for plots\nfig_res_dev &lt;-\n  broom.mixed::tidy(fit_lmm_fp_sj, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  filter(group == \"sj\") |&gt; \n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  labs(title = \"By-participant intercept deviance (log)\",\n       y = \"Participant ID\",\n       x = \"Deviance (log)\") +\n  geom_vline(xintercept = 0, colour = \"red\", linetype = \"dashed\") +\n  geom_point(colour = \"blue\") +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  scale_x_continuous(breaks = c(-0.5,0,0.5)) +\n  facet_grid(~term)\n\nfig_res_est &lt;-\n  broom.mixed::tidy(fit_lmm_fp_sj, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  filter(group == \"sj\") |&gt; \n  # back-transform to ms\n  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(fit_lmm_fp_sj)[1])) |&gt; \n  # mutate(across(c(estimate,conf.low,conf.high),exp)) |&gt; \n  # plot\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  labs(title = \"By-participant intercept estimates (ms)\",\n       y = \"Participant ID\",\n       x = \"Estimate (log)\") +\n  geom_vline(xintercept = fixef(fit_lmm_fp_sj)[1], colour = \"red\", linetype = \"dashed\") +\n  geom_point(colour = \"blue\") +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  scale_x_continuous(breaks = c(5.457102,5.957102 ,6.457102)) +\n  facet_grid(~term)\n\nfig_res_dev + fig_res_est + \n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure¬†9.6: By-participant intercept deviances (A) and estimates (B) in log scale\n\n\n\n\nIf we wanted to back-transform these values to milliseconds to facilitate interpretation, we simply exponentiate these estimates (Figure¬†9.7 A). Lastly, we can back-transform the deviances by subtracting the exponentiating model estimate from the back-transformed estimates (Figure¬†9.7 B).\n\n\nCode for plot\nfig_res_est_ms &lt;-\n  broom.mixed::tidy(fit_lmm_fp_sj, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  filter(group == \"sj\") |&gt; \n  # back-transform to ms\n  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(fit_lmm_fp_sj)[1])) |&gt; \n  mutate(across(c(estimate,conf.low,conf.high),exp)) |&gt;\n  # plot\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  labs(title = \"By-participant intercept estimates (ms)\",\n       y = \"Participant ID\",\n       x = \"Estimate (ms)\") +\n  geom_vline(xintercept = exp(fixef(fit_lmm_fp_sj)[1]), colour = \"red\", linetype = \"dashed\") +\n  geom_point(colour = \"blue\") +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  scale_x_continuous(breaks = c(186.4884,386.4884, 586.4884, 786.4884)) +\n  facet_grid(~term)\n\nfig_res_dev_ms &lt;-\nbroom.mixed::tidy(fit_lmm_fp_sj, effects = \"ran_vals\", conf.int = TRUE) |&gt; \n  filter(group == \"sj\") |&gt; \n  # back-transform to ms\n  mutate(across(c(estimate,conf.low,conf.high),~.+fixef(fit_lmm_fp_sj)[1])) |&gt; \n  mutate(across(c(estimate,conf.low,conf.high),exp)) |&gt;\n  mutate(across(c(estimate,conf.low,conf.high),~.-exp(fixef(fit_lmm_fp_sj)[1]))) |&gt;\n  # plot\n  ggplot() +\n  aes(x = estimate, y = reorder(level, estimate))  +\n  labs(title = \"By-participant intercept deviance (ms)\",\n       y = \"Participant ID\",\n       x = \"Deviance (ms)\") +\n  geom_vline(xintercept = 0, colour = \"red\", linetype = \"dashed\") +\n  geom_point(colour = \"blue\") +\n  geom_errorbar(\n    aes(xmin = conf.low,\n        xmax = conf.high)\n  ) +\n  # scale_x_continuous(breaks = c(-0.5,0,0.5)) +\n  facet_grid(~term)\n\nfig_res_est_ms + fig_res_dev_ms +\n  plot_annotation(tag_levels = \"A\")\n\n\n\n\n\nFigure¬†9.7: By-participant estimates back-transformed to milliseconds\n\n\n\n\nThese plots should seem somewhat familiar given our exploration of the by-participant variance in the dataset above. Now we see how this variance is modelled and including in our model, but what does including it actually change? Let‚Äôs take a look at the difference between our mixed model with by-participant varying intercepts and our fixed-effects only model."
  },
  {
    "objectID": "08-mixed_models1.html#reporting-your-model",
    "href": "08-mixed_models1.html#reporting-your-model",
    "title": "9¬† Random intercepts",
    "section": "9.7 Reporting your model",
    "text": "9.7 Reporting your model\nAccording to Sonderegger (2023) (p.¬†297), we should report:\n\nmodel definition (sometimes in ‚ÄòData Analysis‚Äô section)\nFixed effects\nRandom effects\nSample size (number of observations, number of levels for each grouping factor)\none or more quantitative summaries of the model, e.g., AIC, BIC, or logLik (although they‚Äôre only informative in comparison to another model fit to the same data)\n\n\n9.7.1 Model definition\nBelow is an example of a write-up of a model definition from Biondo et al. (2022) (p.¬†9). Note that I‚Äôve highlighted some aspects that we‚Äôve already covered, which you should remember to define in write ups.\n\nWe conducted the analysis by fitting linear mixed-effect models to our data, using the R package lme4 (Bates et al., 2014). We included Time Reference (past, future), and Verb Match (match, mismatch) as fixed-effect factors [‚Ä¶] by adopting sum contrast coding (Schad et al., 2020): past and match conditions were coded as ‚Äì.5. while future and mismatch conditions were coded as .5. [‚Ä¶] Moreover, we included crossed random intercepts and random slopes for all fixed-effect parameters for subject and item grouping factors (Barr et al., 2013) in all models. [‚Ä¶] Logit mixed-effect models were employed (Jaeger, 2008) for the analysis of the probability of regression measure. [‚Ä¶] P-values were derived by using the lmerTest package (Kuznetsova et al., 2017).\n\nBut this is missing the explicit mention of the method used to compute the p-values. For example, Troyer & Kutas (2020) (p.¬†9) included the following:\n\nP-values for individual predictors were computed using lmerTest, with the Satterthwaite option for denominator degrees of freedom for F statistics.\n\nBut here they don‚Äôt cite the package. So you see, there‚Äôs always something you miss! The aim is to be as descriptive as you can be. The aim in describing your model is to enable reproducibility. If you don‚Äôt fully describe your analysis steps it can be difficult (or impossible) to reproduce your analyses. Ideally, your analysis scripts should also be shared alongside your data (laurinavichyute_share_2022?), but your analysis steps should still be explicitly and unambiguously described to the best of your ability in your data analysis/results section.\n\n\n\n\n\n\nCiting packages\n\n\n\nTo get a package‚Äôs citation, run citation(\"package\") in the Console with the name of the relevant package in quotes. This will produce the APA-style formatted citation, as well as the BibTex citation (in case you‚Äôre writing using Quarto or LaTeX, for example).\n\n\n\n\n9.7.2 Results\nWhen reporting your results a combination of tables, figures, and in-text coefficient estimates is always key. In-line descriptions of your results should include the t- and p-values at minimum. The estimate and standard error (Est = ‚Ä¶, SE = ‚Ä¶,) could also be included in-line, but must at the very least be included in a table. Figures will typically only show the distribution of raw observations and model predictions for fixed effects, and so don‚Äôt differ much from what we saw in previous chapter.\n\n9.7.2.1 In-line text\nAn example of what we could write:\n\nA main effect of tense was found in first-pass reading times at the verb region (Est = 0.062, t = 4.8, p &lt; .001), with the future tense (M = 449ms, SD = 266ms) eliciting longer first-pass reading times than the past tense.\n\n\n\n9.7.2.2 Tables\nWe should include tables of all fixed effects, as we saw in previous chapters. In addition, a description of random effects is a good idea, but isn‚Äôt often done in practice.\n\n9.7.2.2.1 Fixed effects\n\n\nCode for table\ntidy(fit_lmm_fp_sj,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  select(-effect) |&gt; \n  mutate(p.value = format_pval(p.value),\n         across(c(estimate,std.error, statistic), round, 3),\n         df = round(df,1)) |&gt; \n  mutate(term = fct_recode(term,\n    \"Intercept\" = \"(Intercept)\",\n    \"Tense\" = \"verb_t1\",\n    \"Grammticality\" = \"gramm1\",\n    \"Tense x Gramm\" = \"verb_t1:gramm1\"\n  )) |&gt; \n  kable(\n        col.names = c(\"Coefficient\", \"$\\\\hat{\\\\beta}$\", \"SE\", \"t\", \"df\", \"p\")) |&gt; \n  kable_styling()\n\n\n\n\nTable¬†9.6: Table of fixed effects from fit_lmm_fp_sj\n\n\nCoefficient\n$\\hat{\\beta}$\nSE\nt\ndf\np\n\n\n\n\nIntercept\n5.957\n0.034\n176.199\n59.0\n&lt; .001\n\n\nGrammticality\n0.003\n0.014\n0.251\n3732.0\n0.802\n\n\nTense\n0.062\n0.014\n4.512\n3732.1\n&lt; .001\n\n\ngramm1:verb_t1\n-0.016\n0.028\n-0.571\n3732.0\n0.568\n\n\n\n\n\n\n\n\n\n\n9.7.2.2.2 Random effects\n\n\nCode for table\nas.data.frame(VarCorr(fit_lmm_fp_sj),comp=c(\"Variance\",\"Std.Dev.\")) |&gt; \n  as_tibble() |&gt; \n  select(-var2) |&gt; \n  # mutate(var1 = ifelse(var1 == \"NA\", \" \", var1)) |&gt;\n  kable(digits = 3,\n        col.names = c(\"Group\", \"Term\", \"Variance\", \"SD\")) |&gt; \n  kable_styling()\n\n\n\n\nTable¬†9.7: Table of random effects from fit_lmm_fp_sj\n\n\nGroup\nTerm\nVariance\nSD\n\n\n\n\nsj\n(Intercept)\n0.066\n0.256\n\n\nResidual\nNA\n0.180\n0.425\n\n\n\n\n\n\n\n\n\n\n\n9.7.2.3 Figures\nRandom effect visualisations aren‚Äôt typically included in publications, but these can be useful for model exploration and can be included in supplementary materials. When individual differences are of interest, these can also be useful. You can use either the lattice::dotplot() function or a combination of broom.mixed::tidy() and ggplot() that we saw above. I would suggest always starting with dotplot() though to make sure that the visualisations you produce have the same values."
  },
  {
    "objectID": "08-mixed_models1.html#summary",
    "href": "08-mixed_models1.html#summary",
    "title": "9¬† Random intercepts",
    "section": "9.8 Summary",
    "text": "9.8 Summary\n\nwe saw that the equation for a straight line boils down to its intercept and slope\nwe fit our first linear model with a categorical predictor"
  },
  {
    "objectID": "08-mixed_models1.html#learning-objectives-1",
    "href": "08-mixed_models1.html#learning-objectives-1",
    "title": "9¬† Random intercepts",
    "section": "Learning Objectives üèÅ",
    "text": "Learning Objectives üèÅ\nToday we learned‚Ä¶\n\nhow to model binomial data with logistic regression ‚úÖ\nhow to interpret log-odds and odds ratio ‚úÖ"
  },
  {
    "objectID": "08-mixed_models1.html#important-terms",
    "href": "08-mixed_models1.html#important-terms",
    "title": "9¬† Random intercepts",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    linear mixed (effects) model\nNA\nNA"
  },
  {
    "objectID": "08-mixed_models1.html#task",
    "href": "08-mixed_models1.html#task",
    "title": "9¬† Random intercepts",
    "section": "Task",
    "text": "Task\nRepeat the steps we took here, but on√ñ\n\nregression path duration at the verb region, and\nregressions in at the adverb region (roi == 2)\n\n\n9.8.1 Random-intercepts\nUsing the same dataset,\n\n\n9.8.2 Dutch verb regularity"
  },
  {
    "objectID": "08-mixed_models1.html#session-info",
    "href": "08-mixed_models1.html#session-info",
    "title": "9¬† Random intercepts",
    "section": "Session Info",
    "text": "Session Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.10.1           googlesheets4_1.1.1 kableExtra_1.4.0   \n [4] knitr_1.43          patchwork_1.2.0     lattice_0.22-6     \n [7] broom.mixed_0.2.9.5 lmerTest_3.1-3      lme4_1.1-35.3      \n[10] Matrix_1.7-0        sjPlot_2.8.15       ggeffects_1.5.2    \n[13] janitor_2.2.0       broom_1.0.5         here_1.0.1         \n[16] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[19] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[22] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[25] tidyverse_2.0.0     broman_0.80        \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.3         magrittr_2.0.3      snakecase_0.11.1   \n [4] furrr_0.3.1         compiler_4.4.0      mgcv_1.9-1         \n [7] systemfonts_1.0.6   vctrs_0.6.5         pkgconfig_2.0.3    \n[10] crayon_1.5.2        fastmap_1.1.1       backports_1.4.1    \n[13] labeling_0.4.3      utf8_1.2.4          rmarkdown_2.24     \n[16] tzdb_0.4.0          haven_2.5.4         nloptr_2.0.3       \n[19] bit_4.0.5           xfun_0.40           jsonlite_1.8.7     \n[22] highr_0.10          sjmisc_2.8.9        parallel_4.4.0     \n[25] R6_2.5.1            RColorBrewer_1.1-3  stringi_1.8.3      \n[28] parallelly_1.37.1   boot_1.3-30         cellranger_1.1.0   \n[31] numDeriv_2016.8-1.1 estimability_1.5    Rcpp_1.0.12        \n[34] modelr_0.1.11       pacman_0.5.1        splines_4.4.0      \n[37] timechange_0.3.0    tidyselect_1.2.1    rstudioapi_0.16.0  \n[40] yaml_2.3.7          codetools_0.2-20    sjlabelled_1.2.0   \n[43] curl_5.2.1          listenv_0.9.1       plyr_1.8.9         \n[46] withr_3.0.0         bayestestR_0.13.2   coda_0.19-4.1      \n[49] evaluate_0.21       future_1.33.2       xml2_1.3.6         \n[52] pillar_1.9.0        renv_1.0.7          insight_0.19.10    \n[55] generics_0.1.3      vroom_1.6.5         rprojroot_2.0.4    \n[58] hms_1.1.3           munsell_0.5.1       scales_1.3.0       \n[61] minqa_1.2.6         globals_0.16.3      xtable_1.8-4       \n[64] glue_1.7.0          emmeans_1.10.1      tools_4.4.0        \n[67] fs_1.6.3            mvtnorm_1.2-4       grid_4.4.0         \n[70] datawizard_0.10.0   colorspace_2.1-0    nlme_3.1-164       \n[73] Rmisc_1.5.1         performance_0.11.0  googledrive_2.1.1  \n[76] cli_3.6.2           fansi_1.0.6         gargle_1.5.2       \n[79] viridisLite_0.4.2   svglite_2.1.3       sjstats_0.18.2     \n[82] gtable_0.3.5        sass_0.4.7          digest_0.6.33      \n[85] farver_2.1.1        htmlwidgets_1.6.4   htmltools_0.5.8.1  \n[88] lifecycle_1.0.4     httr_1.4.7          bit64_4.0.5        \n[91] MASS_7.3-60.2      \n\n\n\n\n\n\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255‚Äì278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053\n\n\nKuznetsova, A., Brockhoff, P. B., & Christensen, R. H. B. (2017). lmerTest package: Tests in linear mixed effects models. Journal of Statistical Software, 82(13), 1‚Äì26. https://doi.org/10.18637/jss.v082.i13\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nTroyer, M., & Kutas, M. (2020). To catch a Snitch: Brain potentials reveal variability in the functional organization of (fictional) world knowledge during reading. Journal of Memory and Language, 113(August 2019), 104111. https://doi.org/10.1016/j.jml.2020.104111\n\n\nWinter, B. (2014). A very basic tutorial for performing linear mixed effects analyses (Tutorial 2).\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547\n\n\nWinter, B., & Grice, M. (2021). Independence and generalizability in linguistics. Linguistics, 59(5), 1251‚Äì1277. https://doi.org/10.1515/ling-2019-0049"
  },
  {
    "objectID": "09-mixed_models2.html#review-1",
    "href": "09-mixed_models2.html#review-1",
    "title": "10¬† Random slopes",
    "section": "10.1 Review",
    "text": "10.1 Review\n\n10.1.1 Fixed-effects only models\n\ndo not include any grouping factors\n\ncan be dangerously unconservative if violating independence assumption\n\n\n\n10.1.1.1 Fixed-effects only equation\n\\[\\begin{align}\nfp_i &= \\beta_0 + \\beta_{verb\\_t}x_i + \\beta_{gramm}x_i + e_i \\label{eq-fixed_effects}\n\\end{align}\\]\n\nEquation \\(\\ref{eq-fixed_effects}\\) shows the equation for such a model using first-pass reading times as a function of verb tense (verb_t) and grammaticality (gramm)\n\nwhere \\(i\\) represents an observation (\\(i\\) = 1:N)\n\\(\\beta_0\\) = intercept value\n\\(\\beta_{verb\\_t}x\\) = tense slope multiplied by the corresponding level (+/- 0.5)\n\\(\\beta_{gramm}x\\) = grammaticality slope multiplied by the corresponding level (+/- 0.5)\n\\(e_i\\) = residual error for this observation\n\n\n\n\n\n10.1.2 Random intercepts only models\n\nrandom intercepts: varying intercepts per e.g., participant\n\nintercept = mean when your predictor is centred (continuous) or sum contrast coded (categorical)\n\nexplains some additional variance (i.e., should reduce our residual error)\n\n\n10.1.2.1 Random intercepts model equation\n\nEquation \\(\\ref{eq-random_intercepts}\\) includes two additional terms:\n\n\\(\\alpha_{j[i]}\\) = random intercept (\\(\\alpha\\)) for some grouping factor \\(j\\)\n\ne.g., participants, where \\(i = 1:60\\)\n\n\\(\\alpha_{k[i]}\\) = random intercept (\\(\\alpha\\)) for some grouping factor \\(k\\)\n\ne.g., items, where \\(i = 1:96\\)\n\n\n\n\\[\\begin{align}\nfp_i &= \\beta_0 + \\alpha_{j[i]} + \\alpha_{k[i]}+ \\beta_{verb\\_t}x_i + \\beta_{gramm}x_i + e_i \\label{eq-random_intercepts}\n\\end{align}\\]\n\n\\(\\alpha_{j[16]}\\) = random intercept for participant 16\n\\(\\alpha_{j[7]}\\) = random intercept for item 1\n\n\n\n\n10.1.3 \n\n\n\n\n\n\nMissing values and subsetting conditions\n\n\n\nN.B., because we subsetted the data to include only adv_type == \"Deic\", each participant did not contribute 96 data points to our current dataset, but 64.\nSo, our overall N observations should be 64*60, minus however many missing observations we have + so \\(i\\) in \\(fp_i\\) has a value of 1:3840, minus missing values.\nWe can use the nobs() function to find out the number of observations in a model. For example our random-intercepts only model from the last class had 3795 observations, meaning we had 3840 - 3795 = 45 missing observations. This amounts to 1.17% or trials, which is fine (something around 5% of trials is not out of the ordinary).\n\nnobs(fit_fp_1)\n\n[1] 3795\n\n\nWhy do we have missing values? This can depend on a lot of things, such as incorrect attention-check responses (not relevant for this data), measurement error, or pre-processing steps (likely the cause for this data, which is eye-tracking during reading).\n\n\n\n10.1.3.1 Interpreting random effects\n\nhow can we interpret this output from a model, without knowing anything else about the model?\n\n\n\n Groups   Name        Std.Dev.\n Word     (Intercept)  38.201 \n Subject  (Intercept)  91.004 \n Residual             127.258 \n\n\n\n\n10.1.3.2 Formulating a model\n\ncan you formulate a model based on this output in the lmer() syntax?\n\nlet‚Äôs call the depedent variable rt for reaction time\n\n\n\n\n Groups   Name        Std.Dev.\n Word     (Intercept)  38.201 \n Subject  (Intercept)  91.004 \n Residual             127.258 \n\n\n                Estimate Std. Error   t value\n(Intercept)     619.6160  20.761039 29.845135\nNativeLanguage1 106.2954  40.622552  2.616659\nfreq_c          -29.4397   4.168753 -7.061990\n\n\n\n\n10.1.3.3 Interpreting random effects\n\n\n Groups   Name        Std.Dev.\n Word     (Intercept)  38.201 \n Subject  (Intercept)  91.004 \n Residual             127.258 \n\n\n\n\n\n\n\nFigure¬†10.5: lattice::dotplot(ranef(model))"
  },
  {
    "objectID": "09-mixed_models2.html#random-slopes-1",
    "href": "09-mixed_models2.html#random-slopes-1",
    "title": "10¬† Random slopes",
    "section": "10.2 Random slopes",
    "text": "10.2 Random slopes\n\n\n\nrandom slopes: varying slopes\n\nallows for different magnitude/sign of effects per e.g., participant\n\nrecall that our model still produces by-participant and -item slopes\n\nbut they don‚Äôt vary\n\n\n\n\nfixef(fit_fp_1)\n\n   (Intercept)        verb_t1         gramm1 verb_t1:gramm1 \n    5.95640363     0.06189237     0.00321152    -0.01431578 \n\n\n\ncoef(fit_fp_1)$item |&gt; \n  rownames_to_column(var = \"item\") |&gt; \n  head()\n\n  item (Intercept)    verb_t1     gramm1 verb_t1:gramm1\n1    1    6.022184 0.06189237 0.00321152    -0.01431578\n2    2    5.761268 0.06189237 0.00321152    -0.01431578\n3    3    5.854873 0.06189237 0.00321152    -0.01431578\n4    4    6.056862 0.06189237 0.00321152    -0.01431578\n5    5    6.138213 0.06189237 0.00321152    -0.01431578\n6    6    6.331058 0.06189237 0.00321152    -0.01431578\n\n\n\n\n\n10.2.1 A short history of varying slopes\n\nA lot of people construct random intercept-only models but conceptually, it makes hella sense to include random slopes most of the time. After all, you can almost always expect that people differ with how they react to an experimental manipulation!\n\n‚Äî Winter (2014), p.¬†17\n\nafter Baayen et al. (2008), linguists who adopted mixed models typically used random-intercepts only models\n\nbut these have been shown time and again to drastically inflate Type I error rate (false positive) (e.g., Barr et al., 2013)\nBarr et al. (2013) began the credo ‚Äúkeep it maximal‚Äù, meaning include all random slopes justified by your design and existing theories\nlet‚Äôs focus on adding just one varying slope for now\n\n\n\n\n10.2.2 Random intercepts and slopes equation\n\nEquation \\(\\ref{eq-random_slopes}\\) gives an example of a model with by-participant varying slopes for grammaticality\n\n\\[\\begin{align}\nfp_i &= \\beta_0 + \\alpha_{j[i]} + \\alpha_{k[i]} + \\beta_{verb\\_t}x_i + (\\beta_{gramm} + \\gamma_{j[i]})x_i + e_i \\label{eq-random_slopes}\n\\end{align}\\]\n\nwe‚Äôve changed \\(\\beta_{grammt}x_i\\) to \\((\\beta_{grammt} + \\gamma_j[i])x_i\\)\n\nwhere \\(\\gamma_{j[i]}\\) is our by-participant varying slope for gramm for participant \\(i\\)\n\nimagine observation 163 comes from participant (\\(j\\)) 6, item (\\(k\\)) 38, which is a Future-grammatical condition\n\nhow could we plug these into the equation?\n\n\n\n10.2.2.1 Visualising varying intercepts and slopes\n\n\n\n\n\nFigure¬†10.6: Mean effects by-participant overall (A) and per condition (B) with population-level effects in black, with the same plots by-item (C and D)\n\n\n\n\n\n\n\n10.2.3 Comparing participant and item effects\n\nwe‚Äôve already noted that there‚Äôs more variation between participants in the overall first-pass reading times\n\nsome tend to have higher, others lower, reading times\nthis is taken into consideration with the by-participant and -item varying intercepts\nand we saw in our random effects parameters that the standard deviation for participant intercepts was larger\n\ntoday we will focus on varying slopes\n\nthere seems to be comparable inter-group slope variation in both participants and items"
  },
  {
    "objectID": "09-mixed_models2.html#random-intercepts-and-slopes-model",
    "href": "09-mixed_models2.html#random-intercepts-and-slopes-model",
    "title": "10¬† Random slopes",
    "section": "10.3 Random intercepts and slopes model",
    "text": "10.3 Random intercepts and slopes model\n\nrandom slopes = taking group-level variance in effect direction/magnitude into account\n\ni.e., some participants might have a stronger effect, weaker effect, or effect in the opposite direction compared to the population-level\n\n\n\n10.3.1 Disclaimer!!!\n\n\n\n\n\n\nModel building\n\n\n\nToday we are exploring the random effects of our model by adding and subtracting random slopes to ‚Äòsee what happens‚Äô. You typically would NOT do this!\nGenerally, you would start with a pre-defined random effects structure justified by your experimental design and theory (your ‚Äúmaximal‚Äù model (Barr et al., 2013)). We will get into model selection in the next (and last) session. Today we will be adding and removing varying slopes willy-nilly, which can amount to p-hacking, data dredging, or HARKing (Hypohtesisng After the Results are Known).\n\n\n\n\n10.3.2 Random intercept-only model\n\nrecall our random intercept-only model\n\n\nfit_fp_1 &lt;-\n  lmer(log(fp) ~ verb_t*gramm + \n         (1 |sj) +\n         (1|item), \n       data = df_biondo, \n       subset = roi == 4) \n\n\nand inspect the random effects parameters\n\n\n# an alternative to VarCorr(fit_fp_1):\nsummary(fit_fp_1)$varcor \n\n Groups   Name        Std.Dev.\n item     (Intercept) 0.13929 \n sj       (Intercept) 0.25795 \n Residual             0.40111 \n\n\n\nwhat does this tell us?\n\n\n\n10.3.3 Adding a slope\n\nlet‚Äôs look at by-item varying slopes for tense to start\n\n\nfit_fp_item &lt;-\n  lmerTest::lmer(log(fp) ~ verb_t*gramm + \n         (1 |sj) +\n         (1 + verb_t|item), \n       data = df_biondo, \n       subset = roi == 4) \n\n\nwe‚Äôve just added + gramm to (1|sj)\n\nthis reads as ‚Äúfit varying intercepts (1) per participant (|sj)‚Ä¶‚Äù\n‚Äú‚Ä¶and by-item varying intercpets (1) and tense slopes (+ verb_t) per item (|item) ‚Äù\n\n\n\n\n10.3.4 summary()\n\nsummary(fit_fp_item)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(fp) ~ verb_t * gramm + (1 | sj) + (1 + verb_t | item)\n   Data: df_biondo\n Subset: roi == 4\n\nREML criterion at convergence: 4216.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1758 -0.6096 -0.0227  0.6060  4.0568 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n item     (Intercept) 0.019424 0.13937      \n          verb_t1     0.002513 0.05012  0.54\n sj       (Intercept) 0.066414 0.25771      \n Residual             0.160252 0.40032      \nNumber of obs: 3795, groups:  item, 96; sj, 60\n\nFixed effects:\n                  Estimate  Std. Error          df t value             Pr(&gt;|t|)\n(Intercept)       5.956384    0.036763   79.249350 162.023 &lt; 0.0000000000000002\nverb_t1           0.061733    0.013970   93.398429   4.419            0.0000267\ngramm1            0.003298    0.012999 3544.431928   0.254                 0.80\nverb_t1:gramm1   -0.014380    0.025998 3544.742546  -0.553                 0.58\n                  \n(Intercept)    ***\nverb_t1        ***\ngramm1            \nverb_t1:gramm1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) vrb_t1 gramm1\nverb_t1      0.077              \ngramm1       0.000 -0.002       \nvrb_t1:grm1  0.000  0.002  0.000\n\n\n\n\n10.3.5 Fixed effects\n\n\nRandom intercept only\n\nround(\n  summary(fit_fp_1)$coefficients,\n  5)\n\n               Estimate Std. Error         df   t value Pr(&gt;|t|)\n(Intercept)     5.95640    0.03679   79.20081 161.90252  0.00000\nverb_t1         0.06189    0.01303 3637.13315   4.75172  0.00000\ngramm1          0.00321    0.01302 3637.18338   0.24657  0.80526\nverb_t1:gramm1 -0.01432    0.02605 3637.10235  -0.54956  0.58265\n\n\n\nRandom intercept and slope\n\nround(\n  summary(fit_fp_item)$coefficients,\n  5)\n\n               Estimate Std. Error         df   t value Pr(&gt;|t|)\n(Intercept)     5.95638    0.03676   79.24935 162.02259  0.00000\nverb_t1         0.06173    0.01397   93.39843   4.41890  0.00003\ngramm1          0.00330    0.01300 3544.43193   0.25367  0.79977\nverb_t1:gramm1 -0.01438    0.02600 3544.74255  -0.55312  0.58021\n\n\n\n\n\nthe uncertainty around the effect of tense in fit_fp_item has changed\n\nslightly larger standard error\nmuch fewer degrees of freedom\nslightly smaller t-value value\nslightly larger larger p-value\n\n\n\n\n10.3.6 Random effects\n\n\n\nsummary(fit_fp_1)$varcor # or VarCorr(fit_fp_1)\n\n Groups   Name        Std.Dev.\n item     (Intercept) 0.13929 \n sj       (Intercept) 0.25795 \n Residual             0.40111 \n\n\n\n\nsummary(fit_fp_item)$varcor\n\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 0.139371      \n          verb_t1     0.050125 0.542\n sj       (Intercept) 0.257710      \n Residual             0.400315      \n\n\n\n\n\nvariance components are qualitatively unchanged\nresidual error is slightly lower\nbut we have a new row under the item group: verb_t1\n\nwe see the standard deviation of by-participant varying slopes by tense (0.05)\nand we see a new columns: Corr\n\n\n\n10.3.6.1 Correlation paramater\n\nthis is now what we call a variance-covariance matrix\n\nbut we only have one correlation term, that of by-item intercepts with by-item tense slopes\ntheir correlation is 0.54\n\nthis is a positive correlation, meaning the higher a participant‚Äôs intercept (overall first-pass reading times), the stronger the effect of tense\n\n\n\n\n\n10.3.6.2 Plotting\n\nto make life simple, let‚Äôs use lattice::dotplot(): what do these plots tell us?\n\n\n\nCode\nfig_item &lt;- lattice::dotplot(ranef(fit_fp_item))$item\nfig_sj &lt;- lattice::dotplot(ranef(fit_fp_item))$sj\n\ncowplot::plot_grid(fig_item, fig_sj, rel_widths = c(2,1), labels = c(\"A\", \"B\"))\n\n\n\n\n\nFigure¬†10.7: By-item varying intercepts and slopes (A), by-participant varying intercepts (B)\n\n\n\n\n\n\n\n10.3.7 Correlation parameter\n\nwe can plot this relationship by extracting the intercept and slope values with coef()\n\nor ranef to get their deviances from the population-level intercept/slope\n\n\n\n\nCode\ncoef(fit_fp_item)$item |&gt; \n  rownames_to_column(var = \"item\") |&gt; \n  rename(intercept = `(Intercept)`) |&gt; \n  # head()\n  ggplot() +\n  aes(x = verb_t1, y = intercept) +\n  geom_point() +\n  labs(\n    title = \"Correlation of slopes and intercepts\"\n  )\n\n\n\n\n\nFigure¬†10.8: Correlation of by-participant gramm1 slopes (x-axis) and intercepts (y-axis)\n\n\n\n\n\nparticipants with higher intercepts had a stronger effect of grammaticality\n\nwith most participants estimated to have a positive effect\n\n\n\n\n10.3.8 Model comparison\n\ndoes including by-participant slopes for adverb type improve our model fit?\n\n\nanova(fit_fp_1, fit_fp_item)\n\nData: df_biondo\nSubset: roi == 4\nModels:\nfit_fp_1: log(fp) ~ verb_t * gramm + (1 | sj) + (1 | item)\nfit_fp_item: log(fp) ~ verb_t * gramm + (1 | sj) + (1 + verb_t | item)\n            npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nfit_fp_1       7 4210.3 4254.0 -2098.2   4196.3                     \nfit_fp_item    9 4210.4 4266.5 -2096.2   4192.4 3.9796  2     0.1367\n\n\n\nnot really\n\nlog likelihood is slightly higher (‚Äúsmaller‚Äù negative number) for fit_fp_item\nbut p &gt; 0.05\n\nrecall from our plots that there seemed to be by-participant variance in the slopes\n\nwhat if we add by-participant slopes?"
  },
  {
    "objectID": "09-mixed_models2.html#adding-another-slope",
    "href": "09-mixed_models2.html#adding-another-slope",
    "title": "10¬† Random slopes",
    "section": "10.4 Adding another slope",
    "text": "10.4 Adding another slope\n\nhere we‚Äôve added + verb_t to (1|sj)\n\n\nfit_fp_sj_item &lt;-\n  lmerTest::lmer(log(fp) ~ verb_t*gramm + \n         (1 + verb_t|sj) +\n         (1 + verb_t|item), \n       data = df_biondo, \n       subset = roi == 4) \n\nboundary (singular) fit: see help('isSingular')\n\n\n\nand we get a message about singular fit\n\n\n10.4.1 Singular fit\n\nboundary (singular) fit: see help('isSingular')\n\nfollow this advice: run help('isSingular') in the Console and see what you find\n\nyou should never ignore such messages, nor report models with singular fit or convergence warnings!\n\nlet‚Äôs explore the model to see what went wrong\n\n\n\n\n10.4.2 Fixed effects\n\n\n\nround(\n  summary(fit_fp_item)$coefficients,\n  5)\n\n               Estimate Std. Error         df   t value Pr(&gt;|t|)\n(Intercept)     5.95638    0.03676   79.24935 162.02259  0.00000\nverb_t1         0.06173    0.01397   93.39843   4.41890  0.00003\ngramm1          0.00330    0.01300 3544.43193   0.25367  0.79977\nverb_t1:gramm1 -0.01438    0.02600 3544.74255  -0.55312  0.58021\n\n\n\n\nround(\n  summary(fit_fp_sj_item)$coefficients,\n  5)\n\n               Estimate Std. Error         df   t value Pr(&gt;|t|)\n(Intercept)     5.95641    0.03676   79.17933 162.05485  0.00000\nverb_t1         0.06173    0.01415   91.56777   4.36365  0.00003\ngramm1          0.00329    0.01300 3544.45971   0.25349  0.79990\nverb_t1:gramm1 -0.01434    0.02599 3544.77121  -0.55150  0.58133\n\n\n\n\n\nwe see again that the effect of tense is slightly changed, with an increase in the uncertainty around the effect\n\n\n\n10.4.3 Random effects\n\n\n\nsummary(fit_fp_item)$varcor # or VarCorr(fit_fp_1)\n\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 0.139371      \n          verb_t1     0.050125 0.542\n sj       (Intercept) 0.257710      \n Residual             0.400315      \n\n\n\n\nsummary(fit_fp_sj_item)$varcor\n\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 0.139147      \n          verb_t1     0.050010 0.549\n sj       (Intercept) 0.257726      \n          verb_t1     0.017519 1.000\n Residual             0.400240      \n\n\n\n\n\nwe see that by-participant tense has a comparatively smaller variance than the other terms\n\nand the correlation with by-item intercepts is 1\nthis is a red flat: 1 or -1 correlation terms are an indication of convergence failure\n\n\n\n10.4.3.1 Plotting\n\nin Figure¬†10.9 we see by-participant varying tense slopes\n\nbut the confidence intervals are tiny and hard to see\nand they constantly increase\nthis is because of the erroneous perfect correlation between them an the intercepts\n\n\n\n\nCode\nfig_item &lt;- lattice::dotplot(ranef(fit_fp_sj_item))$item\nfig_sj &lt;- lattice::dotplot(ranef(fit_fp_sj_item))$sj\n\ncowplot::plot_grid(fig_item, fig_sj, labels = c(\"A\", \"B\"))\n\n\n\n\n\nFigure¬†10.9: By-item (A) and by-participant (B) varying intercepts and slopes\n\n\n\n\n\n\n\n10.4.4 Convergence warnings\n\nconvergence warnings should not be ignored\n\nthey are a sign that a reliable line fit could not be found (this is an oversimplification‚Ä¶)\nthere can be many reasons for this:\n\nimpossible random effects structure (e.g., adding slopes that don‚Äôt make sense)\nsparse data\noverfitting\n\n\nthese are topics that we can address next week when discussion model selection\n\n\n\n10.4.5 Dealing with convergence issues\n\ngetting a convergence warning is an invitation to explore your random effects\n\na first step is to remove terms that are giving you Correlation terms +/-1\n\nso for now we would stick with fit_fp_item\n\n\n# extract formula\nformula(fit_fp_item)\n\nlog(fp) ~ verb_t * gramm + (1 | sj) + (1 + verb_t | item)"
  },
  {
    "objectID": "09-mixed_models2.html#reporting-your-model",
    "href": "09-mixed_models2.html#reporting-your-model",
    "title": "10¬† Random slopes",
    "section": "10.5 Reporting your model",
    "text": "10.5 Reporting your model\n\nan example for this particular model:\n\n\nA linear-mixed model was fit to log-transformed first-pass reading times at the verb region with grammaticality, tense, and their interaction as fixed effects, and by-participant intercepts and by-item varying intercepts and tense slopes. Tense and grammaticality were sum contrast coded (past and grammatical = -0.5, future and ungrammatical = 0.5).\n\n\nhowever, we‚Äôve made a grave misstep in coming to our final model\n\nwe did not start with a ‚Äúmaximal‚Äù model\n\nwe‚Äôll talk about model selection and reduction next"
  },
  {
    "objectID": "09-mixed_models2.html#learning-objectives-1",
    "href": "09-mixed_models2.html#learning-objectives-1",
    "title": "10¬† Random slopes",
    "section": "Learning objectives üèÅ",
    "text": "Learning objectives üèÅ\nToday we learned‚Ä¶\n\nhow to fit a random-intercepts and slopes model ‚úÖ\nhow to inspect and interpret random slopes ‚úÖ ## Important terms {.unnumbered .smaller}\n\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    linear mixed (effects) model\nNA\nNA\n  \n  \n  \n\n\n\n\n\nTask\nRepeat the steps we took here, but on√ñ\n\nregression path duration at the verb region, and\nregressions in at the adverb region (roi == 2)\n\n\n10.5.0.1 Random-intercepts\nUsing the same dataset,\n\n\n10.5.0.2 Dutch verb regularity\n\n\n\nSession Info\nDeveloped with Quarto using R version 4.4.0 (2024-04-24) (Puppy Cup) and RStudio version 2023.9.0.463 (Desert Sunflower), and the following packages:\n\nsessionInfo()\n\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n [1] gt_0.10.1           googlesheets4_1.1.1 kableExtra_1.4.0   \n [4] knitr_1.43          patchwork_1.2.0     lattice_0.22-6     \n [7] broom.mixed_0.2.9.5 lmerTest_3.1-3      lme4_1.1-35.3      \n[10] Matrix_1.7-0        sjPlot_2.8.15       ggeffects_1.5.2    \n[13] janitor_2.2.0       broom_1.0.5         here_1.0.1         \n[16] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[19] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[22] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[25] tidyverse_2.0.0     broman_0.80        \n\nloaded via a namespace (and not attached):\n [1] rlang_1.1.3         magrittr_2.0.3      snakecase_0.11.1   \n [4] furrr_0.3.1         compiler_4.4.0      mgcv_1.9-1         \n [7] systemfonts_1.0.6   vctrs_0.6.5         pkgconfig_2.0.3    \n[10] crayon_1.5.2        fastmap_1.1.1       backports_1.4.1    \n[13] labeling_0.4.3      utf8_1.2.4          rmarkdown_2.24     \n[16] tzdb_0.4.0          nloptr_2.0.3        bit_4.0.5          \n[19] xfun_0.40           jsonlite_1.8.7      sjmisc_2.8.9       \n[22] parallel_4.4.0      R6_2.5.1            stringi_1.8.3      \n[25] parallelly_1.37.1   boot_1.3-30         cellranger_1.1.0   \n[28] numDeriv_2016.8-1.1 estimability_1.5    Rcpp_1.0.12        \n[31] modelr_0.1.11       pacman_0.5.1        splines_4.4.0      \n[34] timechange_0.3.0    tidyselect_1.2.1    rstudioapi_0.16.0  \n[37] yaml_2.3.7          codetools_0.2-20    sjlabelled_1.2.0   \n[40] curl_5.2.1          listenv_0.9.1       plyr_1.8.9         \n[43] withr_3.0.0         bayestestR_0.13.2   coda_0.19-4.1      \n[46] evaluate_0.21       future_1.33.2       xml2_1.3.6         \n[49] pillar_1.9.0        renv_1.0.7          insight_0.19.10    \n[52] generics_0.1.3      vroom_1.6.5         rprojroot_2.0.4    \n[55] hms_1.1.3           munsell_0.5.1       scales_1.3.0       \n[58] minqa_1.2.6         globals_0.16.3      xtable_1.8-4       \n[61] languageR_1.5.0     glue_1.7.0          emmeans_1.10.1     \n[64] tools_4.4.0         fs_1.6.3            mvtnorm_1.2-4      \n[67] cowplot_1.1.3       grid_4.4.0          colorspace_2.1-0   \n[70] nlme_3.1-164        Rmisc_1.5.1         performance_0.11.0 \n[73] googledrive_2.1.1   cli_3.6.2           fansi_1.0.6        \n[76] gargle_1.5.2        viridisLite_0.4.2   svglite_2.1.3      \n[79] sjstats_0.18.2      gtable_0.3.5        sass_0.4.7         \n[82] digest_0.6.33       farver_2.1.1        htmlwidgets_1.6.4  \n[85] htmltools_0.5.8.1   lifecycle_1.0.4     httr_1.4.7         \n[88] bit64_4.0.5         MASS_7.3-60.2      \n\n\n\n\n\n\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390‚Äì412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255‚Äì278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2014). A very basic tutorial for performing linear mixed effects analyses (Tutorial 2).\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "10-shrinkage.html",
    "href": "10-shrinkage.html",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "",
    "text": "Learning Objectives\nToday we will learn‚Ä¶\n# suppress scientific notation\noptions(scipen=999)\nCode\nsum_shrinkage &lt;- df_biondo |&gt; \n  filter(roi == 4) |&gt; \n  summarise(mean = mean(log(fp), na.rm = T),\n            .by = \"sj\") |&gt; \n  mutate(population_mean = mean(mean, na.rm = T)) |&gt; \n  left_join(coef(fit_fp_1)$sj[\"(Intercept)\"] |&gt; rownames_to_column(var = \"sj\")) |&gt; \n  rename(intercept_1 = `(Intercept)`) |&gt; \n  left_join(coef(fit_fp_item)$sj[\"(Intercept)\"] |&gt; rownames_to_column(var = \"sj\")) |&gt; \n  rename(intercept_item = `(Intercept)`) \n\nsum_shrinkage |&gt; \n  head() \n\n\n# A tibble: 6 √ó 5\n  sj     mean population_mean intercept_1 intercept_item\n  &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 1      6.42            5.96        6.40           6.40\n2 2      5.79            5.96        5.79           5.80\n3 07     5.87            5.96        5.87           5.87\n4 09     5.78            5.96        5.78           5.78\n5 10     6.67            5.96        6.62           6.62\n6 11     5.91            5.96        5.91           5.92\nFigure¬†13.1: Elaine Benes learns about shrinkage of random effect estimates towards the population-level estimates\nToday we learned‚Ä¶\nTerm\n      Definition\n      Equation/Code\n    \n  \n  \n    linear mixed (effects) model\nNA\nNA"
  },
  {
    "objectID": "10-shrinkage.html#load-packages",
    "href": "10-shrinkage.html#load-packages",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "Load packages",
    "text": "Load packages\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               janitor,\n               here,\n               lmerTest)\n\n\nlmer &lt;- lmerTest::lmer"
  },
  {
    "objectID": "10-shrinkage.html#load-data",
    "href": "10-shrinkage.html#load-data",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "Load data",
    "text": "Load data\n\ndata from Biondo et al. (2022)\n\n\ndf_biondo &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ## for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  mutate_if(is.character,as_factor) |&gt; # all character variables as factors\n  droplevels() |&gt; \n  filter(adv_type == \"Deic\")"
  },
  {
    "objectID": "10-shrinkage.html#set-contrasts",
    "href": "10-shrinkage.html#set-contrasts",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "11.1 Set contrasts",
    "text": "11.1 Set contrasts\n\ncontrasts(df_biondo$verb_t) &lt;- c(-0.5,+0.5)\ncontrasts(df_biondo$gramm) &lt;- c(-0.5,+0.5)\ncontrasts(df_biondo$adv_type) &lt;- c(-0.5,+0.5)\n\n\ncontrasts(df_biondo$verb_t)\n\n       [,1]\nPast   -0.5\nFuture  0.5\n\n\n\ncontrasts(df_biondo$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5\n\n\n\ncontrasts(df_biondo$adv_type)\n\n         [,1]\nDeic     -0.5\nNon-deic  0.5"
  },
  {
    "objectID": "10-shrinkage.html#run-models",
    "href": "10-shrinkage.html#run-models",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "11.2 Run models",
    "text": "11.2 Run models\n\nrandom-intercepts only\n\n\nfit_fp_1 &lt;-\n  lmer(log(fp) ~ verb_t*gramm + \n         (1 |sj) +\n         (1|item), \n       data = df_biondo, \n       subset = roi == 4) \n\n\nby-item varying tense slopes\n\n\nfit_fp_item &lt;-\n  lmerTest::lmer(log(fp) ~ verb_t*gramm + \n         (1 |sj) +\n         (1 + verb_t|item), \n       data = df_biondo, \n       subset = roi == 4)"
  },
  {
    "objectID": "10-shrinkage.html#no-pooling",
    "href": "10-shrinkage.html#no-pooling",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "12.1 No pooling",
    "text": "12.1 No pooling\n\n\n\nno pooling refers to separate regression lines fit e.g., per participant\n\neach regression line is fit ignoring the population-level information\nthe intercepts are the true mean from each participant\n\n\n\n\nhead(df_no_pooling)\n\n       model sj intercept    verb_t1      gramm1 verb_t1:gramm1\n1 No pooling  1  6.422811 0.16094962  0.07844247     0.12950513\n2 No pooling  2  5.792669 0.10115512 -0.10571656    -0.23199316\n3 No pooling 07  5.870556 0.15344172 -0.25264603    -0.29866189\n4 No pooling 09  5.780839 0.16938275  0.14074977    -0.07324559\n5 No pooling 10  6.664530 0.04786447 -0.13824470     0.21824110\n6 No pooling 11  5.912309 0.07573670 -0.06469794     0.35318406\n\n\n\n\nsum_shrinkage |&gt; head(6)\n\n# A tibble: 6 √ó 5\n  sj     mean population_mean intercept_1 intercept_item\n  &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 1      6.42            5.96        6.40           6.40\n2 2      5.79            5.96        5.79           5.80\n3 07     5.87            5.96        5.87           5.87\n4 09     5.78            5.96        5.78           5.78\n5 10     6.67            5.96        6.62           6.62\n6 11     5.91            5.96        5.91           5.92"
  },
  {
    "objectID": "10-shrinkage.html#complete-pooling",
    "href": "10-shrinkage.html#complete-pooling",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "12.2 Complete pooling",
    "text": "12.2 Complete pooling\n\n\n\ncomplete pooling refers to ignoring grouping factors\n\ni.e., fixed-effects only models (e.g., with lm() or glm())\none regression line fit ignoring the individual-level information\nthe intercepts are the same as the population-level mean\n\n\n\n\nhead(df_pooled)\n\n# A tibble: 6 √ó 6\n  model            sj    intercept verb_t1  gramm1 `verb_t1:gramm1`\n  &lt;chr&gt;            &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n1 Complete pooling 1          5.96  0.0612 0.00310          -0.0152\n2 Complete pooling 2          5.96  0.0612 0.00310          -0.0152\n3 Complete pooling 07         5.96  0.0612 0.00310          -0.0152\n4 Complete pooling 09         5.96  0.0612 0.00310          -0.0152\n5 Complete pooling 10         5.96  0.0612 0.00310          -0.0152\n6 Complete pooling 11         5.96  0.0612 0.00310          -0.0152\n\n\n\n\nsum_shrinkage |&gt; head(6)\n\n# A tibble: 6 √ó 5\n  sj     mean population_mean intercept_1 intercept_item\n  &lt;chr&gt; &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;\n1 1      6.42            5.96        6.40           6.40\n2 2      5.79            5.96        5.79           5.80\n3 07     5.87            5.96        5.87           5.87\n4 09     5.78            5.96        5.78           5.78\n5 10     6.67            5.96        6.62           6.62\n6 11     5.91            5.96        5.91           5.92"
  },
  {
    "objectID": "10-shrinkage.html#complete-vs.-no-pooling",
    "href": "10-shrinkage.html#complete-vs.-no-pooling",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "12.3 Complete vs.¬†no pooling",
    "text": "12.3 Complete vs.¬†no pooling\n\n\n\ncomplete pooling (green solid line) and no pooling (orange dotted line) of grammaticality effects for 10 participants\n\ndescribe what you see in terms of intercept and slopes across the participants\n\n\n\n\n\n\n\n\nFigure¬†12.1: Observations (black dots) with complete pooling regression line (solid green) and no pooling line (dotted orange) per 10 participants"
  },
  {
    "objectID": "10-shrinkage.html#partial-pooling-mixed-models",
    "href": "10-shrinkage.html#partial-pooling-mixed-models",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "12.4 Partial pooling: mixed models",
    "text": "12.4 Partial pooling: mixed models"
  },
  {
    "objectID": "10-shrinkage.html#shrinkage-1",
    "href": "10-shrinkage.html#shrinkage-1",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "13.1 Shrinkage",
    "text": "13.1 Shrinkage\n\n\n\n\n\nFigure¬†13.2: Shrinkage of 10 participants"
  },
  {
    "objectID": "10-shrinkage.html#centre-of-gravity",
    "href": "10-shrinkage.html#centre-of-gravity",
    "title": "11¬† Shrinkage and Partial Pooling",
    "section": "13.2 Centre of gravity",
    "text": "13.2 Centre of gravity\n\nwhy are some points not being pulled directly to the ‚Äòcentre of gravity‚Äô?\n\nthey‚Äôre being pulled to a higher confidence region\n\n\n\n\n\n\n\nFigure¬†13.3: Shrinkage for all participants: each ellipsis represents a confidence level (really, a quantile: q1, q3, q5, q7, and q9); The inner ellipsis contains the centre 10% of the data, the outer ellipsis 90%"
  },
  {
    "objectID": "11-model_selection.html#learning-objectives",
    "href": "11-model_selection.html#learning-objectives",
    "title": "12¬† Model selection",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will learn about‚Ä¶\n\nthe history of mixed models (again)\nstrategies for model selection\nvariability in model selection\n\nand how to‚Ä¶\n\napply remedies for nonconvergence\nreduce our RES with a data-driven approach\ncompare a parsimonious model to maximal and intercept-only models"
  },
  {
    "objectID": "11-model_selection.html#resources",
    "href": "11-model_selection.html#resources",
    "title": "12¬† Model selection",
    "section": "Resources",
    "text": "Resources\n\nrelevant papers for this topic\n\nBarr et al. (2013)\nBates et al. (2015)\nMatuschek et al. (2017)\nBrauer & Curtin (2018)\nMeteyard & Davies (2020)\n\npractical resources covered in the practical application:\n\nSections 10.3-5 in Sonderegger (2023)\nSection 15.7.3 ‚ÄòConvergence Issues‚Äô in Winter (2019)\nBrauer & Curtin (2018)\nMeteyard & Davies (2020)\n\nwe will continue using the data from Biondo et al. (2022)"
  },
  {
    "objectID": "11-model_selection.html#review-random-intercepts-and-slopes",
    "href": "11-model_selection.html#review-random-intercepts-and-slopes",
    "title": "12¬† Model selection",
    "section": "12.1 Review: random intercepts and slopes",
    "text": "12.1 Review: random intercepts and slopes\n\nviolation of independence assumption ‚Äì&gt; increased Type I error (false positive)\n\ninclude random effects per plausible grouping factor (herein ‚Äúunit,‚Äù √† la Barr et al., 2013)\n\nrandom-intercepts only models ‚Äì&gt; increased Type I error (false positive)\n\nadd random slopes per within-unit manipulation, i.e., ‚Äúmaximal‚Äù models (Barr et al., 2013)\n\nbut such models often fail to ‚Äúconverge‚Äù\n\nand have been shown to increase Type II error (false negative) (Bates et al., 2015; Matuschek et al., 2017)\n\nsome questions remain:\n\nhow to define our maximal model\nhow to handle convergence issues"
  },
  {
    "objectID": "11-model_selection.html#history-of-lmms-revisited",
    "href": "11-model_selection.html#history-of-lmms-revisited",
    "title": "12¬† Model selection",
    "section": "12.2 History of LMMs revisited",
    "text": "12.2 History of LMMs revisited\n\nrecall Clark (1973)‚Äôs language-as-fixed-effect fallacy and the issue of generalisability (see also Winter & Grice, 2021; Yarkoni, 2022)\nBaayen et al. (2008) motivated LMM‚Äôs for linguistic data in the JML special issue\n\neffect: everybody adopted random-intercepts only models\n\nBarr et al. (2013): random-intercepts only models are overconfident, ‚Äúkeep it maximal!‚Äù\n\neffect: everybody adopted maximal models\n\nMatuschek et al. (2017) and Bates et al. (2015): maximal models are underconfident and lower statistical power! Use data-driven model selection to find a ‚Äúparsimonious‚Äù model!\n\neffect: some people adopt this method, but many psycholinguists just want a ‚Äúrecipe‚Äù to follow\n\n\n\n12.2.1 2013: Keep it maximal\n\nA maximal model should optimize generalization of the findings to new subjects and new items.\n‚Äì Barr et al. (2013), p.¬†261\n\n\nrandom-intercepts-only models tend to be underpowered\nfor this reason, Barr et al. (2013) suggested using a maximal random effects structure justified by the experimental design\n\n\n\n12.2.2 2015 & 2017: Parsimonious models\n\n[W]hile the maximal model indeed performs well as far as Type I error rates were concerned, power decreases substantially with model complexity.\n‚Äî Matuschek et al. (2017), p.¬†310-311\n\n\nthere is a trade-off between Type I (overconfidence) and Type II error (underconfidence)\ni.e., maximal models can lead to over-fitting\n\nlowers statistical power, which increases Type II error (false rejection)\n\nbut we should strive for the most parsimonious model\n\nparsimonous models are a compromise between the maximal model justified by your design and theory, and a given data set\n\nbest way to maintain low Type I and II error: collect lots of data"
  },
  {
    "objectID": "11-model_selection.html#model-building",
    "href": "11-model_selection.html#model-building",
    "title": "12¬† Model selection",
    "section": "12.3 Model building",
    "text": "12.3 Model building\n\nEvery statistical model is a description of some real or hypothetical state of affairs in the world.\n‚Äì Yarkoni (2022), p.¬†2\n\n\nour models reflect not only our hypotheses or effects of interest, but any other plausible or known co-variates\nour predictors should reflect our research questions or theories tested\n\nplus any plausible or previously motivated co-variates (e.g., trial order)\nplus known sources of nonindependence, i.e., our random effects\n\n\n\n12.3.1 Choosing predictors\n\nyour model should be defined a priori\n\ni.e., you should define what predictors you will include and any covariates\ne.g., if you have a prediction about the effect of phonological neighbours on vowel duration\n\ndefine what phonological characteristics you will include (e.g., place of articulation? manner?)\nthese should be related to specific hypotheses/research questions\n\n\n\n\n\n12.3.2 Choosing a maximal random effects structure (RES)\n\nhow do we define our maximal model? Some tips from Barr et al. (2013)\n\nbetween-unit factor (e.g., age): include random intercept only\nwithin-unit factor with multiple observations per unit-level (e.g., age in longitudinal data): include random slopes\n\nall factors in an interaction are within-unit: include by-unit random slopes for interaction terms\n\n\n12.3.2.1 Example: Biondo et al. (2022)\n\nBiondo et al. (2022): 2x2 design\n\nverb-tense (past, future) and grammaticality (grammatical, ungrammatical)\nrepeated-measures: within-participant and -item design\n\nso we should have by-participant and -item random intercepts (multiple observations per unit level)\n\neach participant and item contributed multiple data points per condition (i.e., tense and grammaticality were manipulated within each unit level)\n\nso we should have varying tense and grammaticality slopes by- item and -participant\n\n\n\n\n\nCode\nfit_verb_fp_mm &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t*gramm|sj) +\n                      (1 + verb_t*gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4)\n\n\n\n\n\n12.3.3 Observations per cell\n\nif there is only a single observation per cell, e.g., you collected one observation from each participant per condition, then you can‚Äôt fit random intercepts or slopes\nideally you would have at least 5 observations per cell (per unit level per condition, e.g., each participant has at least 5 observations per condition)\nthis is also a question of statistical power\n\n\n# obvz per sj per condition\ndf_biondo |&gt; \n  filter(roi == 4) |&gt; \n  count(sj, verb_t, gramm) |&gt; \n  count(n)\n\n# A tibble: 1 √ó 2\n      n    nn\n  &lt;int&gt; &lt;int&gt;\n1    16   240\n\n\n\n# obvz per item per condition\ndf_biondo |&gt; \n  filter(roi == 4) |&gt; \n  count(item, verb_t, gramm) |&gt; \n  arrange(desc(n)) |&gt; \n  count(n)\n\n# A tibble: 1 √ó 2\n      n    nn\n  &lt;int&gt; &lt;int&gt;\n1    10   384\n\n\n\n\n12.3.4 Data structure\n\nrandom effects must be factors/categorical\nsingle observation per row\n\ngenerally speaking, there should be n(participants) * n(items) rows\nevery fixed or random effect in your model should correspond to a column in your dataset"
  },
  {
    "objectID": "11-model_selection.html#variability-in-methods",
    "href": "11-model_selection.html#variability-in-methods",
    "title": "12¬† Model selection",
    "section": "12.4 Variability in methods",
    "text": "12.4 Variability in methods\n\nMeteyard & Davies (2020)\n\nsurvey of (psychology) researchers\nreview of papers using LMMs\n\ninsecurity in researchers re: choosing models\ngreat variation in papers in how models are built and reported\n\n\n12.4.1 Researcher degrees of freedom\n\nWhat we hope to make clear is that there is no single correct way in which LMM analyses should be conducted, and this has important implications for how the reporting of LMMs should be approached.\n‚Äî Meteyard & Davies (2020), p.¬†9\n\n\nthe problem:\n\n‚Äòresearcher degrees of freedom‚Äô (Simmons et al., 2011), or ‚Äòthe garden of forking paths‚Äô (Gelman & Loken, 2013)\nthe same data can be analysed in a variety of ways\n\nthis leads to insecurity for many researchers\n\n\n\n12.4.2 Justify and document\n\nReplicability and reproducibility are critical for scientific progress, so the way in which researchers have implemented LMM analysis must be entirely transparent. We also hope that the sharing of analysis code and data becomes widespread, enabling the periodic re-analysis of raw data over multiple experiments as studies accumulate over time.\n‚Äî Meteyard & Davies (2020), p.¬†9\n\n\nthe (partial) solution:\n\nmake model building/selection decisions a priori\nbe transparent\nshare your data and code"
  },
  {
    "objectID": "11-model_selection.html#a-practical-example",
    "href": "11-model_selection.html#a-practical-example",
    "title": "12¬† Model selection",
    "section": "12.5 A practical example",
    "text": "12.5 A practical example\nWe‚Äôll now look at an example of a model that encounters convergence issues, and take some steps to reach convergence. We are considering a case where your predictors and covariates are already selected, and will focus on selecting model options and random effects structure that achieve model convergence.\n\n12.5.1 Set-up\nWe first need to set up our environment.\n\n## suppress scientific notation\noptions(scipen=999)\n\n\nLoad packages\n\n## load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               janitor,\n               ## new packages for mixed models:\n               lme4,\n               lmerTest,\n               broom.mixed,\n               lattice)\n\n\nlmer &lt;- lmerTest::lmer\n\n\n\nLoad data\n\ndata from Biondo et al. (2022)\n\n\ndf_biondo &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ### for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  mutate_if(is.character,as_factor) |&gt; ## all character variables as factors\n  droplevels() |&gt; \n  filter(adv_type == \"Deic\")\n\n\n\n12.5.1.1 Set contrasts\n\ncontrasts(df_biondo$verb_t) &lt;- c(-0.5,+0.5)\ncontrasts(df_biondo$gramm) &lt;- c(-0.5,+0.5)\n\n\ncontrasts(df_biondo$verb_t)\n\n       [,1]\nPast   -0.5\nFuture  0.5\n\n\n\ncontrasts(df_biondo$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5\n\n\n\n\n\n12.5.2 Start maximal\n\nmodel structure should be decided a priori\n\nincluded fixed (predictors and covariates) and random effects\n\n\n\n12.5.2.1 Maximal model\n\nstarting point: most maximal model structure justified by your design\n\nif this converges, great!\nif it doesn‚Äôt, what does this mean and what should we do?\n\n\n\nfit_verb_fp_mm &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t*gramm|sj) +\n                      (1 + verb_t*gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4)\n\n\nwe get a warning of singular fit\n\n\n\n\n12.5.3 Convergence issues\n\n‚ÄúConvergence is not a metric of model quality‚Äù (Sonderegger, 2023, p. 365, Box 10.2)\n\nconvergence does not always indicate ‚Äúoverfitting‚Äù or ‚Äúoverparameterisation‚Äù\ncan also be due to optimizer choice\n\nsince default optimizer was changed to nloptwrap from bobyqa, there seem to be more ‚Äòfalse positive‚Äô convergence warnings\n\n\nfalse-positive convergence: you get a convergence warning, but changing the optimizer and/or iteration count does not produce a warning\nfalse-negative convergence: you do not get a warning, but your variance-covariance matrix might indicate overfitting\n\n\n12.5.3.1 Nonconvergence remedies\n\nunfortunately there is no one ‚Äúright‚Äù way to deal with convergence issues\n\nimportant is to transparently report and justify your method\n\nTable 17 in Brauer & Curtin (2018) (p.¬†404) suggests 20 remedies, whittled down to 10 suggestions in Sonderegger (2023)\n\n\n\n\n\n\nFigure¬†12.1: From Sonderegger (2023), p.¬†366\n\n\n\n\n\n\n12.5.3.2 Intrusive vs.¬†Non-intrusive remedies\n\nnon-intrusive remedies amount to checking/adjusting data and model specifications\nintrusive remedies involve reducing random effects structure\n\nthere are different schools of thought\n\nrandom-intercepts only: increased Type I error rate = overconfident estimates\nmaximal-but-singular-fit model (Barr et al., 2013): reduces power = underconfident estimates\ndata-driven approach (Bates et al., 2015): can lose the forest for the trees, e.g., removing random slopes for predictors of interest\n\n\neach strategy has its drawback\n\nimportant is to choose your strategy a priori and transparently report and justify your strategy\neven better: share/publish your data and code, which should be reproducible\n\n\n\n\n12.5.3.3 ?convergence\n\ntype ?convergence in the Console and read the vignette\n\nwhat suggestions does it make?\n\ncompare this to ?isSingular\n\n\n\n\n12.5.4 Non-intrusive methods\n\ncheck your data structure/variables\n\ncheck model assumptions (e.g., normality, missing transformations of variables)\ncheck your RES is justified by your experimental design/data structure\ncentre your predictors (e.g., sum contrasts, or centring/standardizing) to reduce multicollinearity; reduces collinearity in the random effects (a possible source of nonconvergence)\ncheck observations per cell (e.g., is there a participant very few observations, or few observations per one condition? Should be at least &gt;5 per cell)\n\nalter model controls:\n\nincrease iterations\ncheck optimizer\n\n\n\n12.5.4.1 Check optimzer\n\noptimizer\n\nlme4::allFit(model) (can take a while to run)\n\n\n\nall_fit_verb_fp_mm &lt;- allFit(fit_verb_fp_mm)\n## bobyqa : boundary (singular) fit: see help('isSingular')\n## [OK]\n## Nelder_Mead : [OK]\n## nlminbwrap : boundary (singular) fit: see help('isSingular')\n## [OK]\n## nmkbw : [OK]\n## optimx.L-BFGS-B : boundary (singular) fit: see help('isSingular')\n## [OK]\n## nloptwrap.NLOPT_LN_NELDERMEAD : boundary (singular) fit: see help('isSingular')\n## [OK]\n## nloptwrap.NLOPT_LN_BOBYQA : boundary (singular) fit: see help('isSingular')\n## [OK]\n## There were 11 warnings (use warnings() to see them)\n\n\n\n12.5.4.2 Optimizers\n\ndefault optimizer for lmer() is nloptwrap, formerly bobyqa (Bound Optimization by Quaradric Approximiation)\n\nusually changing to bobyqa helps\n\nsee ?lmerControl for more info\nif fits are very similar (or all optimizeres except the default), the nonconvergent fit was a false positive\n\nit‚Äôs safe to use the new optimizer\n\n\n\nsummary(all_fit_verb_fp_mm)$llik\n\n                       bobyqa                   Nelder_Mead \n                    -2105.109                     -2179.479 \n                   nlminbwrap                         nmkbw \n                    -2105.106                     -2105.109 \n              optimx.L-BFGS-B nloptwrap.NLOPT_LN_NELDERMEAD \n                    -2105.106                     -2105.106 \n    nloptwrap.NLOPT_LN_BOBYQA \n                    -2105.106 \n\n\n\nsummary(all_fit_verb_fp_mm)$fixef\n\n                              (Intercept)    verb_t1      gramm1 verb_t1:gramm1\nbobyqa                           5.956403 0.06170602 0.003369634    -0.01418865\nNelder_Mead                      5.956350 0.06188102 0.003488675    -0.01397531\nnlminbwrap                       5.956403 0.06170726 0.003369637    -0.01419047\nnmkbw                            5.956404 0.06170653 0.003369153    -0.01419036\noptimx.L-BFGS-B                  5.956403 0.06170717 0.003369787    -0.01419044\nnloptwrap.NLOPT_LN_NELDERMEAD    5.956403 0.06170725 0.003369649    -0.01419046\nnloptwrap.NLOPT_LN_BOBYQA        5.956403 0.06170771 0.003369203    -0.01419184\n\n\n\n\n12.5.4.3 Increase iterations\n\nand/or increase number of iterations\n\ndefault is 10 000 (1e5 in scientific notation)\nyou can try 20 000, 100 000, etc.\nthis usually helps with larger data or models with complex RES\n\n\n\n## check n of iterations\nfit_verb_fp_mm@optinfo$feval\n\n[1] 2318\n\n\n\n\n12.5.4.4 lmerControl()\n\nfit_verb_fp_mm &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t*gramm|sj) +\n                      (1 + verb_t*gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\n\nor you can just ‚Äòupdate‚Äô the model to save some syntax\n\n\nfit_verb_fp_mm &lt;- update(fit_verb_fp_mm,\n                         control = lmerControl(optimizer = \"bobyqa\", \n                                                optCtrl = list(maxfun = 2e5)))\n\nboundary (singular) fit: see help('isSingular')\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -5.3e-01\n\n\n\n\n12.5.4.5 Removing parameters\n\nstill won‚Äôt converge?\n\nit‚Äôs time to consider intrusive remedies: removing random effects parameters\n\n\n\n\n\n12.5.5 Intrusive methods\n\nnonconvergence in maximal models is often due to overfitting\n\ni.e., the model is overly complex given your data\nthis is typically due to an overly complex random effects structure\n\nif the non-intrusive methods don‚Äôt lead to convergence, the problem is likely overfitting\n\n\n12.5.5.1 Parsimonious vs.¬†maximal\n\nthere are different camps on how to deal with this issue\nI personally follow the suggestions in Bates et al. (2015) (for now)\n\nrun random effects Principal Components Analysis (summary(rePCA(model)), lme4 package)\n\ninforms by how many parameters our model is overfit\n\ncheck variance-covariance matrix (VarCorr(model))\nremove parameters with very high or low Correlation terms and/or much lower variance compared to other terms\nfit simplified model\nwash, rinse, repeat\n\nwe‚Äôll practice this method today, but keep in mind that it‚Äôs up to you to decide and justify which method you use\n\n\n\n12.5.5.2 Random effects Principal Components Analysis\n\ngives us a ranking of all parameters (‚Äòcomponents‚Äô) in our RES per unit\n\n\nsummary(rePCA(fit_verb_fp_mm))\n\n$item\nImportance of components:\n                         [,1]   [,2]    [,3]                    [,4]\nStandard deviation     0.3638 0.2493 0.08366 0.000000000000000001309\nProportion of Variance 0.6567 0.3085 0.03474 0.000000000000000000000\nCumulative Proportion  0.6567 0.9653 1.00000 1.000000000000000000000\n\n$sj\nImportance of components:\n                         [,1]    [,2]        [,3]          [,4]\nStandard deviation     0.6490 0.01470 0.000001281 0.00000001467\nProportion of Variance 0.9995 0.00051 0.000000000 0.00000000000\nCumulative Proportion  0.9995 1.00000 1.000000000 1.00000000000\n\n\n\n\n12.5.5.3 \n\nimportant is the Cumulative Proportion\n\nhow much of the cumulative variance explained by all the by-unit parameters does this one parameter contribute?\nwe see for item, the first component accounts for 66% of the variance explained, and the next contributes an additional 31%, and the next 3%\nso two components account for roughly 97% of variance explained by our RES\nin other words, we can remove one component for sure, and possibly another\nwe could potentially remove 3 components from participant\n\n\n\n\n12.5.5.4 Variance-covariance matrix\n\nso we can remove 2 parameters from item and participant\n\nso either the varying intercept, or slope for tense, grammaticality, or their interaction\n\nwe can check this with VarCorr(fit_verb_fp_mm)\n\n\nVarCorr(fit_verb_fp_mm)\n\n Groups   Name           Std.Dev. Corr                \n item     (Intercept)    0.139189                     \n          verb_t1        0.055890  0.488              \n          gramm1         0.022569 -0.109 -0.921       \n          verb_t1:gramm1 0.095314 -0.283  0.456 -0.646\n sj       (Intercept)    0.257535                     \n          verb_t1        0.018296 0.974               \n          gramm1         0.012055 0.960  0.872        \n          verb_t1:gramm1 0.017731 0.991  0.934  0.990 \n Residual                0.399095                     \n\n\n\nfor item I would remove gramm because it has the lowest variance, and has a pretty high correlation with verb_t (which is unlikely to be true)\nI would also remove gramm for participant for the same reason, as well as its high correlation with the intercept and verb_t\n\n\n12.5.5.4.1 Alternate model 1\n\nfor now let‚Äôs just remove the interaction term\n\nfor reproducibility reasons, do not delete the code for a model that did not converge\nrather, write a comment on what decision was made (and why) for the new model\n\n\n\nfit_verb_fp_m1 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t+gramm|sj) +\n                      (1 + verb_t+gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n12.5.5.4.1.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m1))\n\n$item\nImportance of components:\n                         [,1]   [,2]          [,3]\nStandard deviation     0.3559 0.1291 0.00000007181\nProportion of Variance 0.8837 0.1163 0.00000000000\nCumulative Proportion  0.8837 1.0000 1.00000000000\n\n$sj\nImportance of components:\n                         [,1]         [,2] [,3]\nStandard deviation     0.6465 0.0000006824    0\nProportion of Variance 1.0000 0.0000000000    0\nCumulative Proportion  1.0000 1.0000000000    1\n\n\n\n\n12.5.5.4.1.2 VarCorr()\n\nVarCorr(fit_verb_fp_m1)\n\n Groups   Name        Std.Dev. Corr         \n item     (Intercept) 0.139274              \n          verb_t1     0.055550  0.489       \n          gramm1      0.020747 -0.117 -0.924\n sj       (Intercept) 0.257657              \n          verb_t1     0.017584 1.000        \n          gramm1      0.011554 1.000  1.000 \n Residual             0.399869              \n\n\n\nwhen we see Corr +/-1, this tells us there was an error computing correlations between parameters\n\nit is an invitation to explore\n\nthis is not plausible, and indicates overfitting in our model\n\nwe can remove all slopes from sj\n\n\n\n\n12.5.5.4.1.3 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp_m1))$item\n\n\n\n\n\n\n12.5.5.4.1.4 by-participant random effects (with +1 correlations)\n\nlattice::dotplot(ranef(fit_verb_fp_m1))$sj\n\n\n\n\n\n\n\n12.5.5.4.2 Alternate model 2\n\nfit_verb_fp_m2 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + verb_t+gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n12.5.5.4.2.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m2))\n\n$item\nImportance of components:\n                         [,1]   [,2]          [,3]\nStandard deviation     0.3559 0.1297 0.00000001647\nProportion of Variance 0.8827 0.1173 0.00000000000\nCumulative Proportion  0.8827 1.0000 1.00000000000\n\n$sj\nImportance of components:\n                         [,1]\nStandard deviation     0.6441\nProportion of Variance 1.0000\nCumulative Proportion  1.0000\n\n\n\n\n12.5.5.4.2.2 VarCorr()\n\nVarCorr(fit_verb_fp_m2)\n\n Groups   Name        Std.Dev. Corr         \n item     (Intercept) 0.139364              \n          verb_t1     0.055805  0.485       \n          gramm1      0.020546 -0.097 -0.917\n sj       (Intercept) 0.257648              \n Residual             0.399995              \n\n\n\nby-item slopes for gramm and verb_t are highly correlated\ngramm has least variance, so let‚Äôs remove it\n\n\n\n12.5.5.4.2.3 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp_m2))$item\n\n\n\n\n\n\n\n12.5.5.4.3 Alternate model 3\n\nfit_verb_fp_m3 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + verb_t|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\n\nconverged!\n\n\n12.5.5.4.3.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m3))\n\n$item\nImportance of components:\n                         [,1]    [,2]\nStandard deviation     0.3553 0.10311\nProportion of Variance 0.9223 0.07768\nCumulative Proportion  0.9223 1.00000\n\n$sj\nImportance of components:\n                         [,1]\nStandard deviation     0.6438\nProportion of Variance 1.0000\nCumulative Proportion  1.0000\n\n\n\n\n12.5.5.4.3.2 VarCorr()\n\nVarCorr(fit_verb_fp_m3)\n\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 0.139365      \n          verb_t1     0.050134 0.542\n sj       (Intercept) 0.257714      \n Residual             0.400315      \n\n\n\n\n\n12.5.5.4.4 Alternate model 4\n\nbut we might‚Äôve also decided to remove verb_t, so let‚Äôs run that model\n\n\nfit_verb_fp_m4 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\ndoes not converge, so we‚Äôre justified in keeping by-item verb_t slopes\n\n\n\n\n12.5.5.5 Final model\n\nthe final model name should be some sort of convention to make your life easier\n\nso remove model index\n\n\n\nfit_verb_fp &lt;- fit_verb_fp_m3\n\n\n12.5.5.5.0.1 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp))$sj\n\n\n\n\n\n\n12.5.5.5.0.2 by-participant random effects\n\nlattice::dotplot(ranef(fit_verb_fp))$item\n\n\n\n\n\n\n12.5.5.5.0.3 summary()\n\nsummary(fit_verb_fp)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(fp) ~ verb_t * gramm + (1 | sj) + (1 + verb_t | item)\n   Data: df_biondo\nControl: lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 200000))\n Subset: roi == 4\n\nREML criterion at convergence: 4216.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1758 -0.6096 -0.0227  0.6060  4.0568 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n item     (Intercept) 0.019423 0.13936      \n          verb_t1     0.002513 0.05013  0.54\n sj       (Intercept) 0.066417 0.25771      \n Residual             0.160252 0.40032      \nNumber of obs: 3795, groups:  item, 96; sj, 60\n\nFixed effects:\n                  Estimate  Std. Error          df t value             Pr(&gt;|t|)\n(Intercept)       5.956384    0.036763   79.243172 162.021 &lt; 0.0000000000000002\nverb_t1           0.061733    0.013971   93.410519   4.419            0.0000267\ngramm1            0.003298    0.012999 3544.451690   0.254                 0.80\nverb_t1:gramm1   -0.014380    0.025998 3544.762213  -0.553                 0.58\n                  \n(Intercept)    ***\nverb_t1        ***\ngramm1            \nverb_t1:gramm1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) vrb_t1 gramm1\nverb_t1      0.077              \ngramm1       0.000 -0.002       \nvrb_t1:grm1  0.000  0.002  0.000\n\n\n\nIMPORTANTLY, only look at the fixed effects after you‚Äôve got your final model!!!!\n\ni.e., run model -&gt; convergence error -&gt; rePCA() + VarCorr() -&gt; run model -&gt; ‚Ä¶ -&gt; converges -&gt; only NOW run summary(model)\n\n\n\n\n\n\n12.5.6 Comparing to ‚Äòbad‚Äô models\n\nlet‚Äôs compare our final model to our ‚Äòbad‚Äô models\n\nrandom intercepts-only model (overconfident)\nmaximal model (underconfident)\n\n\n\n12.5.6.1 Random-intercepts only\n\nfit_verb_fp_intercepts &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 |item),\n                    data = df_biondo,\n                    subset = roi == 4\n)\n\n\nconverges\n\n\n\nCode\nsum_fit_verb_fp &lt;-\n  tidy(fit_verb_fp,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = p.value,\n         model = \"parsimonious\") \n\nsum_fit_verb_fp_mm &lt;-\n  tidy(fit_verb_fp_mm,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = p.value,\n         model = \"maximal\") \n\nsum_fit_verb_fp_intercepts &lt;-\n  tidy(fit_verb_fp_intercepts,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = p.value,\n         model = \"intercepts\")\n\n\n\n\n12.5.6.2 coefficient estimates\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, estimate, model) |&gt;\n  mutate(estimate = round(estimate,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = estimate\n  ) |&gt; \n  mutate(measure = \"estimate\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†12.1: Coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n5.9564\n5.9564\n5.9564\nestimate\n\n\nverb_t1\n0.0617\n0.0619\n0.0617\nestimate\n\n\ngramm1\n0.0033\n0.0032\n0.0034\nestimate\n\n\nverb_t1:gramm1\n-0.0144\n-0.0143\n-0.0142\nestimate\n\n\n\n\n\n\n\n\n\n\n12.5.6.3 standard error\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, std.error, model) |&gt;\n  mutate(std.error = round(std.error,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = std.error\n  ) |&gt; \n  mutate(measure = \"std.error\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†12.2: Standard error of coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n0.0368\n0.0368\n0.0367\nstd.error\n\n\nverb_t1\n0.0140\n0.0130\n0.0144\nstd.error\n\n\ngramm1\n0.0130\n0.0130\n0.0133\nstd.error\n\n\nverb_t1:gramm1\n0.0260\n0.0260\n0.0278\nstd.error\n\n\n\n\n\n\n\n\n\nstandard error (\\\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\\\\\)) is a measure of uncertainty\n\nlarger values reflect greater uncertainty\nbecause \\(n\\) is in the denominator, SE gets smaller with more observations\n\ncompared to our parsimonious model with by-item varying verb_t slopes:\n\nsmaller SE for our overconfident (intercepts) model\nlarger SE for our underconfident (maximal) model\nbut only for the estimate also included in the random effects\n\n\n\n\n12.5.6.4 t-values\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, statistic, model) |&gt;\n  mutate(statistic = round(statistic,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = statistic\n  ) |&gt; \n  mutate(measure = \"statistic\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†12.3: t-values of each estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n162.0213\n161.9025\n162.1605\nstatistic\n\n\nverb_t1\n4.4188\n4.7517\n4.2982\nstatistic\n\n\ngramm1\n0.2537\n0.2466\n0.2542\nstatistic\n\n\nverb_t1:gramm1\n-0.5531\n-0.5496\n-0.5108\nstatistic\n\n\n\n\n\n\n\n\n\nt-value (\\\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE}\\\\\\)) is a measure of uncertainty\n\nlarger values reflect greater effect\nmore \\(n\\) increases \\(t\\)\n\nagain, verb_t: \\(t_{max}\\) &lt; \\(t_{pars}\\) &lt; \\(t_{int}\\)\n\n\n\n12.5.6.5 degrees of freedom\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, df, model) |&gt;\n  mutate(df = round(df,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = df\n  ) |&gt; \n  mutate(measure = \"df\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†12.4: Degrees of freedom of each estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n79.2432\n79.2008\n79.1789\ndf\n\n\nverb_t1\n93.4105\n3637.1332\n71.4491\ndf\n\n\ngramm1\n3544.4517\n3637.1834\n179.9254\ndf\n\n\nverb_t1:gramm1\n3544.7622\n3637.1023\n91.8597\ndf\n\n\n\n\n\n\n\n\n\ndegrees of freedom: not trivially defined in mixed models; we‚Äôre using Satterthwaite approximiation (default in lmerTest::lmer())\n\nlarger degrees of freedom corresponds to larger \\(n\\)\nincluding more random effects reduces our \\(n\\) and therefore reduces \\(df\\)\n\nagain, verb_t: \\(df_{max}\\) &lt; \\(df_{pars}\\) &lt; \\(df_{int}\\)\n\nand large differences between our maximal model and the other two for other terms\n\n\n\n\n12.5.6.6 p-values\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, p.value, model) |&gt;\n  mutate(p.value = round(p.value, 8)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = p.value\n  ) |&gt; \n  mutate(measure = \"p.value\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†12.5: p-values of coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n0.0000000\n0.0000000\n0.0000000\np.value\n\n\nverb_t1\n0.0000267\n0.0000021\n0.0000535\np.value\n\n\ngramm1\n0.7997645\n0.8052568\n0.7996181\np.value\n\n\nverb_t1:gramm1\n0.5802114\n0.5826522\n0.6107496\np.value\n\n\n\n\n\n\n\n\n\np-values: inversely related to t-values (larger t-values = smaller p-values)\nagain, verb_t: \\(p_{max}\\) &lt; \\(p_{pars}\\) &lt; \\(p_{int}\\)\n\nthis would be important for ‚Äòsignicance‚Äô if the values were closer to the convential alpha-levels (p &lt; .05, p &lt; .01, p &lt; .001)\nbut here the different random effects structures don‚Äôt qualitatively change (all are &lt; .001)\n\nthis is not always the case, however!\n\nthis is why we do not peek at the fixed effects until we have our final model\nwe don‚Äôt want to be influenced (consciously or not) by seeing small p-values in one model but not another\n\n\n\n\n\n12.5.7 Reporting\n\nin Data Analysis section, e.g.,\n\n\nWe included Time Reference (past, future), and Verb Match (match, mismatch) as fixed-effect factors in the models used to investigate the processing of past‚Äìfuture violations (Q1), by adopting sum contrast coding (Schad et al., 2020): past and match conditions were coded as ‚Äì.5. while future and mismatch conditions were coded as .5. [‚Ä¶] Moreover, we included crossed random intercepts and random slopes for all fixed-effect parameters for subject and item grouping factors (Barr et al., 2013) in all models.\nWe reduced the complexity of the random effect structure of the maximal model by performing a principal component analysis so as to identify the most parsimonious model properly supported by the data (Bates et al., 2015). [‚Ä¶] all reading time data were log transformed before performing the analyses.\n‚Äî Biondo et al. (2022), p.¬†9\n\n\n12.5.7.1 Formatted p-values\nWe can create our own function, which we will call format_pval(), to produce formatted p-values. Here we use the case_when() function to print &lt; .05, &lt; .01 or &lt; .001 when the p-value is smaller than these values (which is a convention). Otherwise (TRUE), round the p-value to 3 decimal points.\n\n## function to format p-values to APA7 guidelines\nformat_pval &lt;- function(pval){\n    dplyr::case_when(\n        pval &lt; .001 ~ \"&lt; .001\",\n        TRUE ~ str_remove(round(pval, 3), \"^0+\")\n    )\n}\n\nWe can now use our format_pval() function to format our p-values and print them in a table, as in Table¬†13.6.\n\n\nCode\n  tidy(fit_verb_fp,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = format_pval(p.value)) |&gt; \n  select(-p.value) |&gt; \n  knitr::kable() |&gt; \n  kableExtra::kable_styling()\n\n\n\n\nTable¬†12.6: Table with formatted p-values from format_pval()\n\n\neffect\nterm\nestimate\nstd.error\nstatistic\ndf\np_value\n\n\n\n\nfixed\n(Intercept)\n5.9563839\n0.0367630\n162.0213327\n79.24317\n&lt; .001\n\n\nfixed\nverb_t1\n0.0617330\n0.0139706\n4.4187865\n93.41052\n&lt; .001\n\n\nfixed\ngramm1\n0.0032976\n0.0129994\n0.2536709\n3544.45169\n.8\n\n\nfixed\nverb_t1:gramm1\n-0.0143804\n0.0259984\n-0.5531269\n3544.76221\n.58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA 7 guidelines: formatting p-values\n\n\n\nThe guidelines from the 7th edition from the American Psychological Association pertaining to numbers and statistics define how we should be formatting p-values, summarised in the Guide for Numbers and Statistics (American Psychological Association, 2022). A summary:\n\nDon‚Äôt use leading 0‚Äôs (i.e., zero before a decimal) when the statistic cannot be greater than 1 (proportion, correlation, level of statistical significance).\nReport exact p-values to two or three decimals (e.g., p = .006, p = .03). i.e., no trailing 0‚Äôs\nReport p-values less than .001 as ‚Äúp &lt; .001‚Äù.\nDo not repeat statistics in both the text and a table or figure\n\nWe used the round() function from base R to round p-values to 3 decimal points. This function drops trailing 0‚Äôs, so 0.030 will become 0.03. We also use the str_remove() function from the stringr package (Tidyverse), which takes the results from round() and removes any leading 0‚Äôs (i.e., those before the decimal) as defined by the regular expression (regex) ^0+. This will take 0.03 and print .03.\nHere is some example output using round():\n\nround(0.020001,3)\n\n[1] 0.02\n\n\nAnd removing the leading 0 using str_remove():\n\nstr_remove(round(0.020001,3), \"^0+\")\n\n[1] \".02\"\n\n\nIf our p-value is smaller than .001, then it will be written as &lt; .001:\n\nformat_pval(c(0.2, 0.02, 0.002, 0.0002))\n\n[1] \".2\"     \".02\"    \".002\"   \"&lt; .001\"\n\n\nSo now our p-values are formatted according to APA 7 guidelines, as long as we pass them through format_pval()."
  },
  {
    "objectID": "11-model_selection.html#alternatives-to-lme4",
    "href": "11-model_selection.html#alternatives-to-lme4",
    "title": "12¬† Model selection",
    "section": "12.6 Alternatives to lme4",
    "text": "12.6 Alternatives to lme4\n\nother alternatives that have fewer convergence issues:\n\nJulia\n\ne.g., in VS Code IDE\n\nBayesian framework (e.g., brms R package)\n\nalso run (G)LMMs, but abandons arbitrary p-values\ninstead quantifies uncertainty\n\n\nboth are more more computationally powerful\n\nthe are not (yet) as widely used in the field"
  },
  {
    "objectID": "11-model_selection.html#learning-objectives-1",
    "href": "11-model_selection.html#learning-objectives-1",
    "title": "12¬† Model selection",
    "section": "Learning objectives üèÅ",
    "text": "Learning objectives üèÅ\nToday we learned‚Ä¶\n\nthe history of mixed models (again) ‚úÖ\nstrategies for model selection ‚úÖ\nvariability in model selection ‚úÖ\n\nand how to‚Ä¶\n\napply remedies for nonconvergence ‚úÖ\nreduce our RES with a data-driven approach ‚úÖ\ncompare a parsimonious model to maximal and intercept-only models ‚úÖ"
  },
  {
    "objectID": "11-model_selection.html#important-terms",
    "href": "11-model_selection.html#important-terms",
    "title": "12¬† Model selection",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    linear mixed (effects) model\nNA\nNA"
  },
  {
    "objectID": "11-model_selection.html#references",
    "href": "11-model_selection.html#references",
    "title": "12¬† Model selection",
    "section": "References",
    "text": "References\n\n\nAmerican Psychological Association. (2022). APA Style numbers and statistics guide. American Psychological Association.\n\n\nBaayen, R. H., Davidson, D. J., & Bates, D. M. (2008). Mixed-effects modeling with crossed random effects for subjects and items. Journal of Memory and Language, 59(4), 390‚Äì412. https://doi.org/10.1016/j.jml.2007.12.005\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255‚Äì278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious Mixed Models. arXiv Preprint, 1‚Äì27. https://doi.org/10.48550/arXiv.1506.04967\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053\n\n\nBrauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389‚Äì411. https://doi.org/10.1037/met0000159\n\n\nClark, H. H. (1973). The language-as-fixed-effect fallacy: A critique of language statistics in psychological research. Journal of Verbal Learning and Verbal Behavior, 12(4), 335‚Äì359. https://doi.org/10.1016/S0022-5371(73)80014-3\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no ‚Äúfishing expedition‚Äù or ‚Äúp-hacking‚Äù and the research hypothesis was posited ahead of time.\n\n\nMatuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type I error and power in linear mixed models. Journal of Memory and Language, 94, 305‚Äì315. https://doi.org/10.1016/j.jml.2017.01.001\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359‚Äì1366. https://doi.org/10.1177/0956797611417632\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547\n\n\nWinter, B., & Grice, M. (2021). Independence and generalizability in linguistics. Linguistics, 59(5), 1251‚Äì1277. https://doi.org/10.1515/ling-2019-0049\n\n\nYarkoni, T. (2022). The generalizability crisis. Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685"
  },
  {
    "objectID": "12-model_selection_example.html#learning-objectives",
    "href": "12-model_selection_example.html#learning-objectives",
    "title": "13¬† Model selection: Example",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nToday we will‚Ä¶\n\napply remedies for nonconvergence\nreduce our RES with a data-driven approach\ncompare a parsimonious model to maximal and intercept-only models"
  },
  {
    "objectID": "12-model_selection_example.html#resources",
    "href": "12-model_selection_example.html#resources",
    "title": "13¬† Model selection: Example",
    "section": "Resources",
    "text": "Resources\n\nthis lecture covers\n\nSections 10.3-5 in Sonderegger (2023)\nSection 15.7.3 ‚ÄòConvergence Issues‚Äô in Winter (2019)\nBrauer & Curtin (2018)\nMeteyard & Davies (2020)\n\nwe will continue using the data from Biondo et al. (2022)"
  },
  {
    "objectID": "12-model_selection_example.html#set-up",
    "href": "12-model_selection_example.html#set-up",
    "title": "13¬† Model selection: Example",
    "section": "Set-up",
    "text": "Set-up\n\n# suppress scientific notation\noptions(scipen=999)\n\n\n\nCode for a function to format p-values\nlibrary(broman)\n# function to format p-values\nformat_pval &lt;- function(pval){\n    dplyr::case_when(\n        pval &lt; .001 ~ \"&lt; .001\",\n        pval &lt; .01 ~ \"&lt; .01\",\n        pval &lt; .05 ~ \"&lt; .05\",\n        TRUE ~ broman::myround(pval, 3)\n    )\n}\n\n\n\nLoad packages\n\n# load libraries\npacman::p_load(\n               tidyverse,\n               here,\n               janitor,\n               # new packages for mixed models:\n               lme4,\n               lmerTest,\n               broom.mixed,\n               lattice)\n\n\nlmer &lt;- lmerTest::lmer\n\n\n\nLoad data\n\ndata from Biondo et al. (2022)\n\n\ndf_biondo &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ## for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  mutate_if(is.character,as_factor) |&gt; # all character variables as factors\n  droplevels() |&gt; \n  filter(adv_type == \"Deic\")\n\n\n\n13.0.1 Set contrasts\n\ncontrasts(df_biondo$verb_t) &lt;- c(-0.5,+0.5)\ncontrasts(df_biondo$gramm) &lt;- c(-0.5,+0.5)\n\n\ncontrasts(df_biondo$verb_t)\n\n       [,1]\nPast   -0.5\nFuture  0.5\n\n\n\ncontrasts(df_biondo$gramm)\n\n        [,1]\ngramm   -0.5\nungramm  0.5"
  },
  {
    "objectID": "12-model_selection_example.html#start-maximal",
    "href": "12-model_selection_example.html#start-maximal",
    "title": "13¬† Model selection: Example",
    "section": "13.1 Start maximal",
    "text": "13.1 Start maximal\n\nmodel structure should be decided a priori\n\nincluded fixed (predictors and covariates) and random effects\n\n\n\n13.1.1 Maximal model\n\nstarting point: most maximal model structure justified by your design\n\nif this converges, great!\nif it doesn‚Äôt, what does this mean and what should we do?\n\n\n\nfit_verb_fp_mm &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t*gramm|sj) +\n                      (1 + verb_t*gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4)\n\n\nwe get a warning of singular fit"
  },
  {
    "objectID": "12-model_selection_example.html#convergence-issues",
    "href": "12-model_selection_example.html#convergence-issues",
    "title": "13¬† Model selection: Example",
    "section": "13.2 Convergence issues",
    "text": "13.2 Convergence issues\n\n‚ÄúConvergence is not a metric of model quality‚Äù (Sonderegger, 2023, p. 365, Box 10.2)\n\nconvergence does not always indicate ‚Äúoverfitting‚Äù or ‚Äúoverparameterisation‚Äù\ncan also be due to optimizer choice\n\nsince default optimizer was changed to nloptwrap from bobyqa, there seem to be more ‚Äòfalse positive‚Äô convergence warnings\n\n\nfalse-positive convergence: you get a convergence warning, but changing the optimizer and/or iteration count does not produce a warning\nfalse-negative convergence: you do not get a warning, but your variance-covariance matrix might indicate overfitting\n\n\n13.2.1 Nonconvergence remedies\n\nunfortunately there is no one ‚Äúright‚Äù way to deal with convergence issues\n\nimportant is to transparently report and justify your method\n\nTable 17 in Brauer & Curtin (2018) (p.¬†404) suggests 20 remedies, whittled down to 10 suggestions in Sonderegger (2023)\n\n\n\n\n\n\nFigure¬†13.1: From Sonderegger (2023), p.¬†366\n\n\n\n\n\n\n13.2.2 Intrusive vs.¬†Non-intrusive remedies\n\nnon-intrusive remedies amount to checking/adjusting data and model specifications\nintrusive remedies involve reducing random effects structure\n\nthere are different schools of thought\n\nrandom-intercepts only: increased Type I error rate = overconfident estimates\nmaximal-but-singular-fit model (Barr et al., 2013): reduces power = underconfident estimates\ndata-driven approach (Bates et al., 2015): can lose the forest for the trees, e.g., removing random slopes for predictors of interest\n\n\neach strategy has its drawback\n\nimportant is to choose your strategy a priori and transparently report and justify your strategy\neven better: share/publish your data and code, which should be reproducible\n\n\n\n\n13.2.3 ?convergence\n\ntype ?convergence in the Console and read the vignette\n\nwhat suggestions does it make?\n\ncompare this to ?isSingular"
  },
  {
    "objectID": "12-model_selection_example.html#non-intrusive-methods",
    "href": "12-model_selection_example.html#non-intrusive-methods",
    "title": "13¬† Model selection: Example",
    "section": "13.3 Non-intrusive methods",
    "text": "13.3 Non-intrusive methods\n\ncheck your data structure/variables\n\ncheck model assumptions (e.g., normality, missing transformations of variables)\ncheck your RES is justified by your experimental design/data structure\ncentre your predictors (e.g., sum contrasts, or centring/standardizing) to reduce multicollinearity; reduces collinearity in the random effects (a possible source of nonconvergence)\ncheck observations per cell (e.g., is there a participant very few observations, or few observations per one condition? Should be at least &gt;5 per cell)\n\nalter model controls:\n\nincrease iterations\ncheck optimizer\n\n\n\n13.3.1 Check optimzer\n\noptimizer\n\nlme4::allFit(model) (can take a while to run)\n\n\n\nall_fit_verb_fp_mm &lt;- allFit(fit_verb_fp_mm)\n# bobyqa : boundary (singular) fit: see help('isSingular')\n# [OK]\n# Nelder_Mead : [OK]\n# nlminbwrap : boundary (singular) fit: see help('isSingular')\n# [OK]\n# nmkbw : [OK]\n# optimx.L-BFGS-B : boundary (singular) fit: see help('isSingular')\n# [OK]\n# nloptwrap.NLOPT_LN_NELDERMEAD : boundary (singular) fit: see help('isSingular')\n# [OK]\n# nloptwrap.NLOPT_LN_BOBYQA : boundary (singular) fit: see help('isSingular')\n# [OK]\n# There were 11 warnings (use warnings() to see them)\n\n\n\n13.3.2 Optimizers\n\ndefault optimizer for lmer() is nloptwrap, formerly bobyqa (Bound Optimization by Quaradric Approximiation)\n\nusually changing to bobyqa helps\n\nsee ?lmerControl for more info\nif fits are very similar (or all optimizeres except the default), the nonconvergent fit was a false positive\n\nit‚Äôs safe to use the new optimizer\n\n\n\nsummary(all_fit_verb_fp_mm)$llik\n\n                       bobyqa                   Nelder_Mead \n                    -2105.109                     -2179.479 \n                   nlminbwrap                         nmkbw \n                    -2105.106                     -2105.109 \n              optimx.L-BFGS-B nloptwrap.NLOPT_LN_NELDERMEAD \n                    -2105.106                     -2105.106 \n    nloptwrap.NLOPT_LN_BOBYQA \n                    -2105.106 \n\n\n\nsummary(all_fit_verb_fp_mm)$fixef\n\n                              (Intercept)    verb_t1      gramm1 verb_t1:gramm1\nbobyqa                           5.956403 0.06170602 0.003369634    -0.01418865\nNelder_Mead                      5.956350 0.06188102 0.003488675    -0.01397531\nnlminbwrap                       5.956403 0.06170726 0.003369637    -0.01419047\nnmkbw                            5.956404 0.06170653 0.003369153    -0.01419036\noptimx.L-BFGS-B                  5.956403 0.06170717 0.003369787    -0.01419044\nnloptwrap.NLOPT_LN_NELDERMEAD    5.956403 0.06170725 0.003369649    -0.01419046\nnloptwrap.NLOPT_LN_BOBYQA        5.956403 0.06170771 0.003369203    -0.01419184\n\n\n\n\n13.3.3 Increase iterations\n\nand/or increase number of iterations\n\ndefault is 10 000 (1e5 in scientific notation)\nyou can try 20 000, 100 000, etc.\nthis usually helps with larger data or models with complex RES\n\n\n\n# check n of iterations\nfit_verb_fp_mm@optinfo$feval\n\n[1] 2318\n\n\n\n\n13.3.4 lmerControl()\n\nfit_verb_fp_mm &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t*gramm|sj) +\n                      (1 + verb_t*gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\n\nor you can just ‚Äòupdate‚Äô the model to save some syntax\n\n\nfit_verb_fp_mm &lt;- update(fit_verb_fp_mm,\n                         control = lmerControl(optimizer = \"bobyqa\", \n                                                optCtrl = list(maxfun = 2e5)))\n\nboundary (singular) fit: see help('isSingular')\n\n\nWarning: Model failed to converge with 1 negative eigenvalue: -5.3e-01\n\n\n\n\n13.3.5 Removing parameters\n\nstill won‚Äôt converge?\n\nit‚Äôs time to consider intrusive remedies: removing random effects parameters"
  },
  {
    "objectID": "12-model_selection_example.html#intrusive-methods",
    "href": "12-model_selection_example.html#intrusive-methods",
    "title": "13¬† Model selection: Example",
    "section": "13.4 Intrusive methods",
    "text": "13.4 Intrusive methods\n\nnonconvergence in maximal models is often due to overfitting\n\ni.e., the model is overly complex given your data\nthis is typically due to an overly complex random effects structure\n\nif the non-intrusive methods don‚Äôt lead to convergence, the problem is likely overfitting\n\n\n13.4.1 Parsimonious vs.¬†maximal\n\nthere are different camps on how to deal with this issue\nI personally follow the suggestions in Bates et al. (2015) (for now)\n\nrun random effects Principal Components Analysis (summary(rePCA(model)), lme4 package)\n\ninforms by how many parameters our model is overfit\n\ncheck variance-covariance matrix (VarCorr(model))\nremove parameters with very high or low Correlation terms and/or much lower variance compared to other terms\nfit simplified model\nwash, rinse, repeat\n\nwe‚Äôll practice this method today, but keep in mind that it‚Äôs up to you to decide and justify which method you use\n\n\n\n13.4.2 Random effects Principal Components Analysis\n\ngives us a ranking of all parameters (‚Äòcomponents‚Äô) in our RES per unit\n\n\nsummary(rePCA(fit_verb_fp_mm))\n\n$item\nImportance of components:\n                         [,1]   [,2]    [,3]                    [,4]\nStandard deviation     0.3638 0.2493 0.08366 0.000000000000000001309\nProportion of Variance 0.6567 0.3085 0.03474 0.000000000000000000000\nCumulative Proportion  0.6567 0.9653 1.00000 1.000000000000000000000\n\n$sj\nImportance of components:\n                         [,1]    [,2]        [,3]          [,4]\nStandard deviation     0.6490 0.01470 0.000001281 0.00000001467\nProportion of Variance 0.9995 0.00051 0.000000000 0.00000000000\nCumulative Proportion  0.9995 1.00000 1.000000000 1.00000000000\n\n\n\n13.4.2.1 \n\nimportant is the Cumulative Proportion\n\nhow much of the cumulative variance explained by all the by-unit parameters does this one parameter contribute?\nwe see for item, the first component accounts for 66% of the variance explained, and the next contributes an additional 31%, and the next 3%\nso two components account for roughly 97% of variance explained by our RES\nin other words, we can remove one component for sure, and possibly another\nwe could potentially remove 3 components from participant\n\n\n\n\n\n13.4.3 Variance-covariance matrix\n\nso we can remove 2 parameters from item and participant\n\nso either the varying intercept, or slope for tense, grammaticality, or their interaction\n\nwe can check this with VarCorr(fit_verb_fp_mm)\n\n\nVarCorr(fit_verb_fp_mm)\n\n Groups   Name           Std.Dev. Corr                \n item     (Intercept)    0.139189                     \n          verb_t1        0.055890  0.488              \n          gramm1         0.022569 -0.109 -0.921       \n          verb_t1:gramm1 0.095314 -0.283  0.456 -0.646\n sj       (Intercept)    0.257535                     \n          verb_t1        0.018296 0.974               \n          gramm1         0.012055 0.960  0.872        \n          verb_t1:gramm1 0.017731 0.991  0.934  0.990 \n Residual                0.399095                     \n\n\n\nfor item I would remove gramm because it has the lowest variance, and has a pretty high correlation with verb_t (which is unlikely to be true)\nI would also remove gramm for participant for the same reason, as well as its high correlation with the intercept and verb_t\n\n\n13.4.3.1 Alternate model 1\n\nfor now let‚Äôs just remove the interaction term\n\nfor reproducibility reasons, do not delete the code for a model that did not converge\nrather, write a comment on what decision was made (and why) for the new model\n\n\n\nfit_verb_fp_m1 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 + verb_t+gramm|sj) +\n                      (1 + verb_t+gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n13.4.3.1.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m1))\n\n$item\nImportance of components:\n                         [,1]   [,2]          [,3]\nStandard deviation     0.3559 0.1291 0.00000007181\nProportion of Variance 0.8837 0.1163 0.00000000000\nCumulative Proportion  0.8837 1.0000 1.00000000000\n\n$sj\nImportance of components:\n                         [,1]         [,2] [,3]\nStandard deviation     0.6465 0.0000006824    0\nProportion of Variance 1.0000 0.0000000000    0\nCumulative Proportion  1.0000 1.0000000000    1\n\n\n\n\n13.4.3.1.2 VarCorr()\n\nVarCorr(fit_verb_fp_m1)\n\n Groups   Name        Std.Dev. Corr         \n item     (Intercept) 0.139274              \n          verb_t1     0.055550  0.489       \n          gramm1      0.020747 -0.117 -0.924\n sj       (Intercept) 0.257657              \n          verb_t1     0.017584 1.000        \n          gramm1      0.011554 1.000  1.000 \n Residual             0.399869              \n\n\n\nwhen we see Corr +/-1, this tells us there was an error computing correlations between parameters\n\nit is an invitation to explore\n\nthis is not plausible, and indicates overfitting in our model\n\nwe can remove all slopes from sj\n\n\n\n\n13.4.3.1.3 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp_m1))$item\n\n\n\n\n\n\n13.4.3.1.4 by-participant random effects (with +1 correlations)\n\nlattice::dotplot(ranef(fit_verb_fp_m1))$sj\n\n\n\n\n\n\n\n13.4.3.2 Alternate model 2\n\nfit_verb_fp_m2 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + verb_t+gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n13.4.3.2.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m2))\n\n$item\nImportance of components:\n                         [,1]   [,2]          [,3]\nStandard deviation     0.3559 0.1297 0.00000001647\nProportion of Variance 0.8827 0.1173 0.00000000000\nCumulative Proportion  0.8827 1.0000 1.00000000000\n\n$sj\nImportance of components:\n                         [,1]\nStandard deviation     0.6441\nProportion of Variance 1.0000\nCumulative Proportion  1.0000\n\n\n\n\n13.4.3.2.2 VarCorr()\n\nVarCorr(fit_verb_fp_m2)\n\n Groups   Name        Std.Dev. Corr         \n item     (Intercept) 0.139364              \n          verb_t1     0.055805  0.485       \n          gramm1      0.020546 -0.097 -0.917\n sj       (Intercept) 0.257648              \n Residual             0.399995              \n\n\n\nby-item slopes for gramm and verb_t are highly correlated\ngramm has least variance, so let‚Äôs remove it\n\n\n\n13.4.3.2.3 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp_m2))$item\n\n\n\n\n\n\n\n13.4.3.3 Alternate model 3\n\nfit_verb_fp_m3 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + verb_t|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\n\nconverged!\n\n\n13.4.3.3.1 rePCA()\n\nsummary(rePCA(fit_verb_fp_m3))\n\n$item\nImportance of components:\n                         [,1]    [,2]\nStandard deviation     0.3553 0.10311\nProportion of Variance 0.9223 0.07768\nCumulative Proportion  0.9223 1.00000\n\n$sj\nImportance of components:\n                         [,1]\nStandard deviation     0.6438\nProportion of Variance 1.0000\nCumulative Proportion  1.0000\n\n\n\n\n13.4.3.3.2 VarCorr()\n\nVarCorr(fit_verb_fp_m3)\n\n Groups   Name        Std.Dev. Corr \n item     (Intercept) 0.139365      \n          verb_t1     0.050134 0.542\n sj       (Intercept) 0.257714      \n Residual             0.400315      \n\n\n\n\n\n13.4.3.4 Alternate model 4\n\nbut we might‚Äôve also decided to remove verb_t, so let‚Äôs run that model\n\n\nfit_verb_fp_m4 &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 + gramm|item),\n                    data = df_biondo,\n                    subset = roi == 4,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                          optCtrl = list(maxfun = 2e5))\n)\n\nboundary (singular) fit: see help('isSingular')\n\n\n\ndoes not converge, so we‚Äôre justified in keeping by-item verb_t slopes\n\n\n\n\n13.4.4 Final model\n\nthe final model name should be some sort of convention to make your life easier\n\nso remove model index\n\n\n\nfit_verb_fp &lt;- fit_verb_fp_m3\n\n\n13.4.4.0.1 by-item random effects\n\nlattice::dotplot(ranef(fit_verb_fp))$sj\n\n\n\n\n\n\n13.4.4.0.2 by-participant random effects\n\nlattice::dotplot(ranef(fit_verb_fp))$item\n\n\n\n\n\n\n13.4.4.0.3 summary()\n\nsummary(fit_verb_fp)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(fp) ~ verb_t * gramm + (1 | sj) + (1 + verb_t | item)\n   Data: df_biondo\nControl: lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 200000))\n Subset: roi == 4\n\nREML criterion at convergence: 4216.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.1758 -0.6096 -0.0227  0.6060  4.0568 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n item     (Intercept) 0.019423 0.13936      \n          verb_t1     0.002513 0.05013  0.54\n sj       (Intercept) 0.066417 0.25771      \n Residual             0.160252 0.40032      \nNumber of obs: 3795, groups:  item, 96; sj, 60\n\nFixed effects:\n                  Estimate  Std. Error          df t value             Pr(&gt;|t|)\n(Intercept)       5.956384    0.036763   79.243172 162.021 &lt; 0.0000000000000002\nverb_t1           0.061733    0.013971   93.410519   4.419            0.0000267\ngramm1            0.003298    0.012999 3544.451690   0.254                 0.80\nverb_t1:gramm1   -0.014380    0.025998 3544.762213  -0.553                 0.58\n                  \n(Intercept)    ***\nverb_t1        ***\ngramm1            \nverb_t1:gramm1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) vrb_t1 gramm1\nverb_t1      0.077              \ngramm1       0.000 -0.002       \nvrb_t1:grm1  0.000  0.002  0.000\n\n\n\nIMPORTANTLY, only look at the fixed effects after you‚Äôve got your final model!!!!\n\ni.e., run model -&gt; convergence error -&gt; rePCA() + VarCorr() -&gt; run model -&gt; ‚Ä¶ -&gt; converges -&gt; only NOW run summary(model)"
  },
  {
    "objectID": "12-model_selection_example.html#comparing-to-bad-models",
    "href": "12-model_selection_example.html#comparing-to-bad-models",
    "title": "13¬† Model selection: Example",
    "section": "13.5 Comparing to ‚Äòbad‚Äô models",
    "text": "13.5 Comparing to ‚Äòbad‚Äô models\n\nlet‚Äôs compare our final model to our ‚Äòbad‚Äô models\n\nrandom intercepts-only model (overconfident)\nmaximal model (underconfident)\n\n\n\n13.5.1 Random-intercepts only\n\nfit_verb_fp_intercepts &lt;- lmer(log(fp) ~ verb_t*gramm + \n                      (1 |sj) +\n                      (1 |item),\n                    data = df_biondo,\n                    subset = roi == 4\n)\n\n\nconverges\n\n\n\nCode\nsum_fit_verb_fp &lt;-\n  tidy(fit_verb_fp,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = format_pval(p.value),\n         model = \"parsimonious\") \n\nsum_fit_verb_fp_mm &lt;-\n  tidy(fit_verb_fp_mm,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = format_pval(p.value),\n         model = \"maximal\") \n\nsum_fit_verb_fp_intercepts &lt;-\n  tidy(fit_verb_fp_intercepts,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = format_pval(p.value),\n         model = \"intercepts\")\n\n\n\n\n13.5.2 coefficient estimates\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, estimate, model) |&gt;\n  mutate(estimate = round(estimate,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = estimate\n  ) |&gt; \n  mutate(measure = \"estimate\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†13.1: Coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n5.9564\n5.9564\n5.9564\nestimate\n\n\nverb_t1\n0.0617\n0.0619\n0.0617\nestimate\n\n\ngramm1\n0.0033\n0.0032\n0.0034\nestimate\n\n\nverb_t1:gramm1\n-0.0144\n-0.0143\n-0.0142\nestimate\n\n\n\n\n\n\n\n\n\n\n13.5.3 standard error\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, std.error, model) |&gt;\n  mutate(std.error = round(std.error,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = std.error\n  ) |&gt; \n  mutate(measure = \"std.error\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†13.2: Standard error of coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n0.0368\n0.0368\n0.0367\nstd.error\n\n\nverb_t1\n0.0140\n0.0130\n0.0144\nstd.error\n\n\ngramm1\n0.0130\n0.0130\n0.0133\nstd.error\n\n\nverb_t1:gramm1\n0.0260\n0.0260\n0.0278\nstd.error\n\n\n\n\n\n\n\n\n\nstandard error (\\\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\\\\\)) is a measure of uncertainty\n\nlarger values reflect greater uncertainty\nbecause \\(n\\) is in the denominator, SE gets smaller with more observations\n\ncompared to our parsimonious model with by-item varying verb_t slopes:\n\nsmaller SE for our overconfident (intercepts) model\nlarger SE for our underconfident (maximal) model\nbut only for the estimate also included in the random effects\n\n\n\n\n13.5.4 t-values\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, statistic, model) |&gt;\n  mutate(statistic = round(statistic,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = statistic\n  ) |&gt; \n  mutate(measure = \"statistic\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†13.3: t-values of each estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n162.0213\n161.9025\n162.1605\nstatistic\n\n\nverb_t1\n4.4188\n4.7517\n4.2982\nstatistic\n\n\ngramm1\n0.2537\n0.2466\n0.2542\nstatistic\n\n\nverb_t1:gramm1\n-0.5531\n-0.5496\n-0.5108\nstatistic\n\n\n\n\n\n\n\n\n\nt-value (\\\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE}\\\\\\)) is a measure of uncertainty\n\nlarger values reflect greater effect\nmore \\(n\\) increases \\(t\\)\n\nagain, verb_t: \\(t_{max}\\) &lt; \\(t_{pars}\\) &lt; \\(t_{int}\\)\n\n\n\n13.5.5 degrees of freedom\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, df, model) |&gt;\n  mutate(df = round(df,4)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = df\n  ) |&gt; \n  mutate(measure = \"df\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†13.4: Degrees of freedom of each estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n79.2432\n79.2008\n79.1789\ndf\n\n\nverb_t1\n93.4105\n3637.1332\n71.4491\ndf\n\n\ngramm1\n3544.4517\n3637.1834\n179.9254\ndf\n\n\nverb_t1:gramm1\n3544.7622\n3637.1023\n91.8597\ndf\n\n\n\n\n\n\n\n\n\ndegrees of freedom: not trivially defined in mixed models; we‚Äôre using Satterthwaite approximiation (default in lmerTest::lmer())\n\nlarger degrees of freedom corresponds to larger \\(n\\)\nincluding more random effects reduces our \\(n\\) and therefore reduces \\(df\\)\n\nagain, verb_t: \\(df_{max}\\) &lt; \\(df_{pars}\\) &lt; \\(df_{int}\\)\n\nand large differences between our maximal model and the other two for other terms\n\n\n\n\n13.5.6 p-values\n\n\nCode\nrbind(sum_fit_verb_fp, sum_fit_verb_fp_intercepts, sum_fit_verb_fp_mm) |&gt; \n  select(term, p.value, model) |&gt;\n  mutate(p.value = round(p.value, 8)) |&gt; \n  pivot_wider(\n    id_cols = c(term),\n    names_from = model,\n    values_from = p.value\n  ) |&gt; \n  mutate(measure = \"p.value\") |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\n\nTable¬†13.5: p-values of coefficient estimates for our parsimonious model, a random-intercepts only model, and a maximal model\n\n\nterm\nparsimonious\nintercepts\nmaximal\nmeasure\n\n\n\n\n(Intercept)\n0.0000000\n0.0000000\n0.0000000\np.value\n\n\nverb_t1\n0.0000267\n0.0000021\n0.0000535\np.value\n\n\ngramm1\n0.7997645\n0.8052568\n0.7996181\np.value\n\n\nverb_t1:gramm1\n0.5802114\n0.5826522\n0.6107496\np.value\n\n\n\n\n\n\n\n\n\np-values: inversely related to t-values (larger t-values = smaller p-values)\nagain, verb_t: \\(p_{max}\\) &lt; \\(p_{pars}\\) &lt; \\(p_{int}\\)\n\nthis would be important for ‚Äòsignicance‚Äô if the values were closer to the convential alpha-levels (p &lt; .05, p &lt; .01, p &lt; .001)\nbut here the different random effects structures don‚Äôt qualitatively change (all are &lt; .001)\n\nthis is not always the case, however!\n\nthis is why we do not peek at the fixed effects until we have our final model\nwe don‚Äôt want to be influenced (consciously or not) by seeing small p-values in one model but not another"
  },
  {
    "objectID": "12-model_selection_example.html#reporting",
    "href": "12-model_selection_example.html#reporting",
    "title": "13¬† Model selection: Example",
    "section": "13.6 Reporting",
    "text": "13.6 Reporting\n\nin Data Analysis section, e.g.,\n\n\nWe included Time Reference (past, future), and Verb Match (match, mismatch) as fixed-effect factors in the models used to investigate the processing of past‚Äìfuture violations (Q1), by adopting sum contrast coding (Schad et al., 2020): past and match conditions were coded as ‚Äì.5. while future and mismatch conditions were coded as .5. [‚Ä¶] Moreover, we included crossed random intercepts and random slopes for all fixed-effect parameters for subject and item grouping factors (Barr et al., 2013) in all models.\nWe reduced the complexity of the random effect structure of the maximal model by performing a principal component analysis so as to identify the most parsimonious model properly supported by the data (Bates et al., 2015). [‚Ä¶] all reading time data were log transformed before performing the analyses.\n‚Äî Biondo et al. (2022), p.¬†9\n\n\n13.6.1 Formatted p-values\n\nwe can use the format_pval() function defined earlier to produce formatted p-values\n\n\n  tidy(fit_verb_fp,\n     effects = \"fixed\") |&gt; \n  as_tibble() |&gt; \n  mutate(p_value = format_pval(p.value)) |&gt; \n  select(-p.value) |&gt; \n  kable() |&gt; \n  kable_styling()\n\n\n\nTable¬†13.6: Table with formatted p-values from format_pval()\n\n\neffect\nterm\nestimate\nstd.error\nstatistic\ndf\np_value\n\n\n\n\nfixed\n(Intercept)\n5.9563839\n0.0367630\n162.0213327\n79.24317\n&lt; .001\n\n\nfixed\nverb_t1\n0.0617330\n0.0139706\n4.4187865\n93.41052\n&lt; .001\n\n\nfixed\ngramm1\n0.0032976\n0.0129994\n0.2536709\n3544.45169\n0.800\n\n\nfixed\nverb_t1:gramm1\n-0.0143804\n0.0259984\n-0.5531269\n3544.76221\n0.580"
  },
  {
    "objectID": "12-model_selection_example.html#learning-objectives-1",
    "href": "12-model_selection_example.html#learning-objectives-1",
    "title": "13¬† Model selection: Example",
    "section": "Learning objectives üèÅ",
    "text": "Learning objectives üèÅ\nToday we‚Ä¶\n\napplied remedies for nonconvergence ‚úÖ\nreduced our RES with a data-driven approach ‚úÖ\ncompared a parsimonious model to maximal and intercept-only models ‚úÖ"
  },
  {
    "objectID": "12-model_selection_example.html#important-terms",
    "href": "12-model_selection_example.html#important-terms",
    "title": "13¬† Model selection: Example",
    "section": "Important terms",
    "text": "Important terms\n\n\n\n\n\n\n  \n    \n      Term\n      Definition\n      Equation/Code\n    \n  \n  \n    linear mixed (effects) model\nNA\nNA"
  },
  {
    "objectID": "12-model_selection_example.html#references",
    "href": "12-model_selection_example.html#references",
    "title": "13¬† Model selection: Example",
    "section": "References",
    "text": "References\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255‚Äì278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious Mixed Models. arXiv Preprint, 1‚Äì27. https://doi.org/10.48550/arXiv.1506.04967\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053\n\n\nBrauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. Psychological Methods, 23(3), 389‚Äì411. https://doi.org/10.1037/met0000159\n\n\nMeteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. Journal of Memory and Language, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092\n\n\nSonderegger, M. (2023). Regression Modeling for Linguistic Data.\n\n\nWinter, B. (2019). Statistics for Linguists: An Introduction Using R. In Statistics for Linguists: An Introduction Using R. Routledge. https://doi.org/10.4324/9781315165547"
  },
  {
    "objectID": "reports/report 2/report2.html#dataset",
    "href": "reports/report 2/report2.html#dataset",
    "title": "14¬† Report 2",
    "section": "14.1 Dataset",
    "text": "14.1 Dataset\nFor this report you will continue using the data from Biondo et al. (2022), an eye-tracking reading study on adverb-tense congruence effects on reading time measures. Participants‚Äô eye movements were recorded as they read Spanish sentences where temporal adverbs and verb tense were either congruent or incongruent. For both sentence regions, the time reference was either past (e.g., yesterday, bought) or future (e.g., tomorrow, will buy). Example stimuli from this experiment are given in Table¬†14.1.\n\n\n\n\nTable¬†14.1: Example stimuli\n\n\nsentence\nadverb\nverb\ngramm\n\n\n\n\nA la salida del trabajo, **ayer** las chicas **compraron** pan en la tienda.&lt;br&gt; *After leaving work* **yesterday** *the girls* **bought** *bread at the shop*\npast\npast\ngramm\n\n\nA la salida del trabajo, **ayer** las chicas **\\*comprar√°n** pan en la tienda.&lt;br&gt; *After leaving work* **yesterday** *the girls* **\\*will buy** *bread at the shop*\npast\nfuture\nungramm\n\n\nA la salida del trabajo, **ma√±ana** las chicas **comprar√°n** pan en la tienda.&lt;br&gt; *After leaving work* **tomorrow** *the girls* **will buy** *bread at the shop*\nfuture\nfuture\ngramm\n\n\nA la salida del trabajo, **ma√±ana** las chicas **\\*compraron** pan en la tienda.&lt;br&gt; *After leaving work* **tomorrow** *the girls* **\\*bought** *bread at the shop*\nfuture\npast\nungramm\n\n\n\n\n\n\n\n\nYou will be fitting models to different eye-tracking reading measures from this experiment, with the predictors adverb time and grammaticality."
  },
  {
    "objectID": "reports/report 2/report2.html#set-up",
    "href": "reports/report 2/report2.html#set-up",
    "title": "14¬† Report 2",
    "section": "14.2 Set-up",
    "text": "14.2 Set-up\nMake sure you begin with a clear working environment. To achieve this, you can go to Session &gt; Restart R. Your Environment should have no objects in it, and you should not have any packages loaded.\n\n14.2.1 Quarto YAML\nMake sure your YAML looks something like this:\n\n---\ntitle: \"Report 2\"\nname: \"My Name\"\nformat:\n  html: default\n  pdf: default\ntoc: true\nnumber-sections: true\n---\n\n\n\n\n\n\n\nRender often\n\n\n\nI suggest you render your document frequently, e.g., after every substantial code chunk/task achieved. This will ensure earlier detection of broken code and makes it easier to fix problems. Do this for both HTML and PDF.\n\n\n\n\n14.2.2 Packages\nLoad the following packages, however you prefer (i.e., you don‚Äôt have to use pacman::p_load()):\n\ntidyverse\njanitor\nhere\nbroom.mixed\nlattice\nlme4\nlmerTest\n\nDescribe what each of the following packages is used for (in our experience, they have many more useful functions than we‚Äôve tried).\n\nbroom.mixed:\nlattice:\nlme4:\nlmerTest:\n\n\n\n14.2.3 Data\nLoad in the Biondo et al. (2022) data by running the following code chunk.\n\ndf_biondo &lt;-\n  read_csv(here(\"data\", \"Biondo.Soilemezidi.Mancini_dataset_ET.csv\"),\n           locale = locale(encoding = \"Latin1\") ## for special characters in Spanish\n           ) |&gt; \n  clean_names() |&gt; \n  mutate(gramm = ifelse(gramm == \"0\", \"ungramm\", \"gramm\")) |&gt; \n  mutate_if(is.character,as_factor) |&gt; # all character variables as factors\n  filter(adv_type == \"Deic\") |&gt; \n  droplevels() |&gt; \n  mutate(\n    roi_length = str_length(label)\n  ) |&gt; \n  relocate(roi_length, .after = label)\n\nThe last few lines add a new variable (roi_length) that contains region length (in letters). We will use this as a covariate in one of our models."
  },
  {
    "objectID": "reports/report 2/report2.html#model-set-up",
    "href": "reports/report 2/report2.html#model-set-up",
    "title": "14¬† Report 2",
    "section": "14.3 Model set up",
    "text": "14.3 Model set up\nYou will be asked to run two models, one linear mixed model (lmer() from the lme4 or lmerTest package) and one genearlised (logistic) linear mixed model (glmer(family = \"binomial\") from the lme4 package).\n\n14.3.1 Variable transformations\nFor each model, consider whether you need to implement the following steps:\n\ncentre (sum contrast code) categorical predictors\nstandardize continuous predictors (e.g., using the scale() function)\nlog-transform continuous dependent variables if skewed\nmodel selection: begin with a maximal model\n\nsimplify in case of nonconvergence or singular fit\n\n\n\n\n14.3.2 Model selection\nFor each model, start with a ‚Äúmaximal‚Äù model justified by the design. If you encounter convergence issues, begin by first implementing ‚Äúunintrusive‚Äù remedies. If you still have convergence issues (as indicated by warning messages and/or e.g., inspecting the variance-covariance matrix), reduce the random effects structure as you see fit. Be sure to document and justify your decisions step-by-step. N.B., the equivalent of lmerControl argument (for lmer() models) is glmerControl for glmer() models.\nIf you choose to use the lme4::allFit() function, beware that it can take a long time to run, especially on ‚Äòmaximal‚Äô models. I suggest you (i) save the output as an object (e.g., allFit_model1 &lt;- allFit(model1)) and (ii) plan another task that doesn‚Äôt involve running code when you run this function.\nI am not expecting any particular model/random effects structure that is correct, but am looking for explanations on how you made decisions regarding what to remove or keep in your model."
  },
  {
    "objectID": "reports/report 2/report2.html#linear-mixed-model",
    "href": "reports/report 2/report2.html#linear-mixed-model",
    "title": "14¬† Report 2",
    "section": "14.4 Linear mixed model",
    "text": "14.4 Linear mixed model\nFit a linear mixed model to total reading times (tt) at the adverb region (roi == 2). Your fixed effects are adverb time reference (adv_t), grammaticality (gramm), their interaction, and (standardized) region length in characters as a covariate without any interaction. Include by-participant and -item random effects.\n\n14.4.1 Fit a model\nStart by defining your most maximal model justified by your design, and simplify accordingly. Remember to not delete the code for nonconverging models, instead set the code chunk to not run when you render your document, as in the code chunk below (#| eval: false).\n\n```{r}\n#| eval: false\n\nfit_some_maximal_model &lt;- \n  lmer(dependent_variable ~ predictor1*predictor2 + covariate +\n         (1 + predictor1*predictor2|participant) +\n         data = my_data,\n       subset = some_factor == \"some_level\")\n# informative comment, e.g., \"didn't converge\"\n```\n\n\n\n14.4.2 Report results\nOnce you‚Äôve landed on a final model that converges, inspect the fixed and random effects (some useful functions we‚Äôve already seen: summary(), broom.mixed::tidy(), fixef(), ranef(), coef(), lattice::dotplot())."
  },
  {
    "objectID": "reports/report 2/report2.html#generalised-linear-mixed-model",
    "href": "reports/report 2/report2.html#generalised-linear-mixed-model",
    "title": "14¬† Report 2",
    "section": "14.5 Generalised linear mixed model",
    "text": "14.5 Generalised linear mixed model\nWe didn‚Äôt cover how to implement logistic mixed regression, however the relationship between lm() and glm() is the same in mixed models (lmer() and glmer()).\n\n14.5.1 Fit a model\nFit a generalised linear mixed model (glmer() from the lme4 package, lmerTest does not have this function) to the regressions in (ri) to the adverb region (roi == 2). Your fixed effects are adverb time reference (adv_t), grammaticality (gramm), and their interaction. Remember to use eval: false in your code chunk options to stop Quarto from running all your non-final models when rendering.\n\n\n14.5.2 Report results\nOnce you‚Äôve landed on a final model that converges, inspect the fixed and random effects (some useful functions we‚Äôve already seen: summary(), broom.mixed::tidy(), fixef(), ranef(), coef(), lattice::dotplot())\nRecall that our coefficient estimates are in log odds. The interpretation of your coefficient estimates (fixed effects) is identical to that in genearlised linear models (i.e., without random effects)."
  },
  {
    "objectID": "reports/report 2/report2.html#interpretation",
    "href": "reports/report 2/report2.html#interpretation",
    "title": "14¬† Report 2",
    "section": "14.6 Interpretation",
    "text": "14.6 Interpretation\nWrite a short report of the findings from the two models. Produce a table and plot like in the example above to supplement your report."
  },
  {
    "objectID": "reports/report 2/report2.html#render",
    "href": "reports/report 2/report2.html#render",
    "title": "14¬† Report 2",
    "section": "14.7 Render",
    "text": "14.7 Render\nRender your Quarto finished script. Upload the .qmd, .pdf, and .html files to Moodle. N.B., you need to have tinytex installed to be able to render PDFs.\n\n\n\n\nBiondo, N., Soilemezidi, M., & Mancini, S. (2022). Yesterday is history, tomorrow is a mystery: An eye-tracking investigation of the processing of past and future time reference during sentence reading. Journal of Experimental Psychology: Learning, Memory, and Cognition, 48(7), 1001‚Äì1018. https://doi.org/10.1037/xlm0001053"
  }
]